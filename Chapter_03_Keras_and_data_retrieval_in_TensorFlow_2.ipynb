{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Keras and Data Retrieval in TensorFlow 2\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we dealt with the low-level building blocks of TensorFlow: Tensors, Variables, and Operations. While powerful, building complex deep learning models using only these primitives is time-consuming and error-prone. \n",
    "\n",
    "This chapter introduces **Keras**, the high-level API built into TensorFlow 2, which streamlines model development. We will explore the three specific ways to build models in Keras, ranging from simple to highly complex architectures. Furthermore, a model is useless without data. We will also master the art of building efficient, scalable data pipelines using `tf.data` and other utilities to feed our models.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **Model Abstractions:** Sequential vs. Functional vs. Subclassing paradigms.\n",
    "* **Data Pipelines:** The ETL (Extract, Transform, Load) process in Deep Learning.\n",
    "* **Input Optimization:** Prefetching, caching, and batching.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Building models using the **Sequential**, **Functional**, and **Subclassing** APIs.\n",
    "* Implementing custom Keras layers.\n",
    "* creating robust data pipelines using the **tf.data API**.\n",
    "* Using **Keras DataGenerators** for legacy support and **TensorFlow Datasets (TFDS)** for benchmark datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Three Flavors of Keras Model Building\n",
    "\n",
    "Keras offers three distinct ways to define a neural network, offering a trade-off between simplicity and flexibility.\n",
    "\n",
    "#### 1. The Sequential API\n",
    "* **Definition:** A linear stack of layers. Each layer has exactly one input tensor and one output tensor.\n",
    "* **Intuition:** Think of it as a simple assembly line. Data goes in one end, passes through stations A, B, and C in order, and comes out the other end.\n",
    "* **Use Case:** Perfect for 90% of standard deep learning models (e.g., simple CNNs, MLPs).\n",
    "* **Limitation:** Cannot handle models with multiple inputs (e.g., image + metadata), multiple outputs, or non-linear topology (e.g., Residual connections).\n",
    "\n",
    "#### 2. The Functional API\n",
    "* **Definition:** A graph-based approach where you treat layers as functions that take tensors as inputs and return tensors as outputs.\n",
    "* **Intuition:** Think of it as a complex plumbing system. You can split pipes, merge them, have multiple inlets, and multiple outlets.\n",
    "* **Use Case:** Essential for complex architectures like ResNet (skip connections), InceptionNet (branching), or multi-modal models.\n",
    "\n",
    "#### 3. The Subclassing API\n",
    "* **Definition:** A fully object-oriented approach where you define a class inheriting from `tf.keras.Model` and define the forward pass logic in the `call()` method.\n",
    "* **Intuition:** This gives you full control over the \"forward pass.\" You can use Python control flow (`if`, `for`) inside the model execution.\n",
    "* **Use Case:** Research on exotic architectures, dynamic networks, or when you need total control over the training loop.\n",
    "\n",
    "### 2.2 Data Retrieval Strategies\n",
    "\n",
    "Deep learning models are data-hungry. If your GPU has to wait for the CPU to load and process images, your training will be slow. TensorFlow provides tools to optimize this.\n",
    "\n",
    "1.  **`tf.data` API:** The standard, most performant way to build pipelines. It treats data as a stream that can be mapped, filtered, batched, and prefetched asynchronously.\n",
    "2.  **Keras DataGenerators:** Older utilities (like `ImageDataGenerator`) specifically designed for image augmentation. Easier to use for simple image tasks but less performant than `tf.data`.\n",
    "3.  **TensorFlow Datasets (TFDS):** A library of ready-to-use datasets (like MNIST, CIFAR-10) managed by Google, handling download and caching automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Part 1: Keras Model-Building APIs\n",
    "\n",
    "We will implement three models to solve a classification problem on the **Iris Dataset**. \n",
    "1.  **Model A (Sequential):** A standard baseline.\n",
    "2.  **Model B (Functional):** A model that takes two separate inputs (Raw features + PCA features).\n",
    "3.  **Model C (Subclassing):** A model utilizing a custom layer with a multiplicative bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Ensure reproducibility\n",
    "def fix_random_seed(seed):\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy not imported\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow not imported\")\n",
    "\n",
    "fix_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preparation (Iris Dataset)\n",
    "We will download the Iris dataset, clean it, and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download Data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "file_path = \"iris.data\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    r = requests.get(url)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# 2. Load Data with Pandas\n",
    "iris_df = pd.read_csv(file_path, header=None)\n",
    "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_width', 'petal_length', 'label']\n",
    "\n",
    "# 3. Preprocessing\n",
    "# Map string labels to integers\n",
    "iris_df[\"label\"] = iris_df[\"label\"].map(\n",
    "    {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    ")\n",
    "\n",
    "# Shuffle the data\n",
    "iris_df = iris_df.sample(frac=1.0, random_state=42)\n",
    "\n",
    "# Separate Features (x) and Labels (y)\n",
    "x = iris_df[['sepal_length', 'sepal_width', 'petal_width', 'petal_length']]\n",
    "y = iris_df['label']\n",
    "\n",
    "# Normalize features (Center around 0)\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "# Convert to One-Hot Encoding for the output\n",
    "y = tf.one_hot(y, depth=3)\n",
    "\n",
    "# Convert to numpy arrays for Keras\n",
    "x = x.values\n",
    "y = y.numpy()\n",
    "\n",
    "print(\"Data Shape:\", x.shape)\n",
    "print(\"Labels Shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model A: The Sequential API\n",
    "This is the simplest approach. We stack layers linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Define the model using Sequential\n",
    "model_seq = Sequential([\n",
    "    # Input shape must be defined in the first layer\n",
    "    Dense(32, activation='relu', input_shape=(4,)), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3, activation='softmax') # Output layer: 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_seq.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model_seq.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\n--- Training Sequential Model ---\")\n",
    "model_seq.fit(x, y, batch_size=64, epochs=10, verbose=0)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model B: The Functional API\n",
    "We will create a model that takes **two inputs**:\n",
    "1. The original raw features (4 dimensions).\n",
    "2. PCA-reduced features (2 dimensions).\n",
    "\n",
    "The model will process them in parallel branches and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Prepare PCA data (2nd Input source)\n",
    "pca_model = PCA(n_components=2, random_state=42)\n",
    "x_pca = pca_model.fit_transform(x)\n",
    "\n",
    "# --- Defining the Functional Graph ---\n",
    "\n",
    "# 1. Define Inputs explicitly\n",
    "input_raw = Input(shape=(4,), name='input_raw')\n",
    "input_pca = Input(shape=(2,), name='input_pca')\n",
    "\n",
    "# 2. Branch 1: Process Raw Data\n",
    "x1 = Dense(16, activation='relu')(input_raw)\n",
    "\n",
    "# 3. Branch 2: Process PCA Data\n",
    "x2 = Dense(16, activation='relu')(input_pca)\n",
    "\n",
    "# 4. Merge Branches\n",
    "concat = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "# 5. Post-Merge Processing\n",
    "h = Dense(16, activation='relu')(concat)\n",
    "output = Dense(3, activation='softmax')(h)\n",
    "\n",
    "# 6. Instantiate Model\n",
    "model_func = Model(inputs=[input_raw, input_pca], outputs=output)\n",
    "\n",
    "model_func.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_func.summary()\n",
    "\n",
    "# Train (Pass inputs as a list)\n",
    "print(\"\\n--- Training Functional Model ---\")\n",
    "model_func.fit([x, x_pca], y, batch_size=64, epochs=10, verbose=0)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model C: The Subclassing API (Custom Layers)\n",
    "Here we will define a custom layer `MulBiasDense`. Unlike standard layers that calculate $y = \\sigma(Wx + b)$, this layer will calculate:\n",
    "$$ y = \\sigma((Wx + b) \\times b_{mul}) $$\n",
    "where $b_{mul}$ is a learned multiplicative bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "class MulBiasDense(Layer):\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        super(MulBiasDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create Weights (w)\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='kernel'\n",
    "        )\n",
    "        # Create Additive Bias (b)\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='bias'\n",
    "        )\n",
    "        # Create Multiplicative Bias (b_mul)\n",
    "        self.b_mul = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='mul_bias'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The computation logic\n",
    "        out = (tf.matmul(inputs, self.w) + self.b) * self.b_mul\n",
    "        return self.activation(out)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Using the Custom Layer in a Functional Model\n",
    "inp = Input(shape=(4,))\n",
    "out = MulBiasDense(units=32, activation='relu')(inp)\n",
    "out = Dense(3, activation='softmax')(out)\n",
    "\n",
    "model_sub = Model(inputs=inp, outputs=out)\n",
    "model_sub.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Train\n",
    "print(\"\\n--- Training Custom Layer Model ---\")\n",
    "model_sub.fit(x, y, batch_size=64, epochs=10, verbose=0)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Part 2: Retrieving Data for TensorFlow\n",
    "\n",
    "In this section, we will build a production-grade data pipeline for image data. We will simulate a scenario where we have images on disk and labels in a CSV file.\n",
    "\n",
    "### 4.1 Setup: Downloading Dummy Image Data\n",
    "Since we don't have the local files mentioned in the book, we will download the specific 'flower_photos' dataset often used in TF tutorials to simulate the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Flower Dataset\n",
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Create a CSV file simulating a real-world scenario (filename, label)\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# We will only use 'roses' and 'daisy' for this small example to keep it fast\n",
    "image_paths = list(data_dir.glob('roses/*')) + list(data_dir.glob('daisy/*'))\n",
    "image_paths = [str(path) for path in image_paths]\n",
    "labels = [0] * len(list(data_dir.glob('roses/*'))) + [1] * len(list(data_dir.glob('daisy/*')))\n",
    "\n",
    "# Shuffle\n",
    "rng = np.random.default_rng(42)\n",
    "combined = list(zip(image_paths, labels))\n",
    "rng.shuffle(combined)\n",
    "image_paths, labels = zip(*combined)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = 'flower_labels.csv'\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['filename', 'label'])\n",
    "    for img, lbl in zip(image_paths, labels):\n",
    "        writer.writerow([img, lbl])\n",
    "\n",
    "print(f\"Created CSV with {len(image_paths)} records.\")\n",
    "print(f\"Example path: {image_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The `tf.data` API Pipeline\n",
    "\n",
    "We will build a pipeline that:\n",
    "1. Reads the CSV file.\n",
    "2. Parses filenames and labels.\n",
    "3. Loads the actual image from the disk.\n",
    "4. Resizes and normalizes the image.\n",
    "5. Batches and prefetches the data.\n",
    "\n",
    "This is the most efficient way to feed data in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Dataset from the CSV file\n",
    "# We skip the header line\n",
    "csv_ds = tf.data.experimental.make_csv_dataset(\n",
    "    csv_file,\n",
    "    batch_size=1, # Read one by one initially\n",
    "    header=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 2. Transformation Functions\n",
    "def process_csv_row(row):\n",
    "    # Extract filename and label from the dictionary row returned by make_csv_dataset\n",
    "    return row['filename'], row['label']\n",
    "\n",
    "def load_and_preprocess_image(filename, label):\n",
    "    # Read file from disk\n",
    "    img_raw = tf.io.read_file(filename)\n",
    "    # Decode image (detects format automatically)\n",
    "    img = tf.image.decode_image(img_raw, channels=3)\n",
    "    # Resize to fixed size (e.g., 64x64)\n",
    "    img = tf.image.resize(img, [64, 64])\n",
    "    # Normalize to [0, 1]\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "# 3. Construct the Pipeline\n",
    "# Unbatch first because make_csv_dataset returns batches\n",
    "train_ds = csv_ds.unbatch().map(process_csv_row)\n",
    "\n",
    "# Map the image loading function\n",
    "# num_parallel_calls=AUTOTUNE allows TF to load images in parallel using multiple CPU cores\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Optimization: Shuffle, Batch, and Prefetch\n",
    "BATCH_SIZE = 32\n",
    "train_ds = train_ds.shuffle(buffer_size=100)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# 5. Test the pipeline\n",
    "print(\"\\n--- Testing tf.data Pipeline ---\")\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Batch of Images Shape:\", images.shape)\n",
    "    print(\"Batch of Labels Shape:\", labels.shape)\n",
    "    print(\"Sample Label:\", labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation of the Pipeline\n",
    "1.  **`make_csv_dataset`**: Reads the text file efficiently.\n",
    "2.  **`map`**: Applies transformations. `load_and_preprocess_image` contains the critical logic: `tf.io.read_file` brings bytes into memory, and `tf.image.decode_image` turns bytes into pixel tensors.\n",
    "3.  **`num_parallel_calls=AUTOTUNE`**: This is crucial. It tells TensorFlow to use available CPU cores to load/process images *while* the GPU is busy training on the previous batch.\n",
    "4.  **`prefetch`**: This ensures there is always a batch of data ready in memory when the GPU finishes the current step, eliminating I/O bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Keras DataGenerators\n",
    "Before `tf.data`, `ImageDataGenerator` was the standard. It is still useful for quick prototypes involving image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 1. Define Generator with Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,         # Normalize\n",
    "    rotation_range=20,      # Random rotation\n",
    "    width_shift_range=0.2,  # Random shift\n",
    "    horizontal_flip=True    # Random flip\n",
    ")\n",
    "\n",
    "# 2. Flow from DataFrame\n",
    "# We reuse the DataFrame concept (although we wrote to CSV, we can load it back to DF)\n",
    "df_flow = pd.read_csv(csv_file)\n",
    "# Provide string labels for categorical mode\n",
    "df_flow['label'] = df_flow['label'].astype(str) \n",
    "\n",
    "generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df_flow,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Testing Keras ImageDataGenerator ---\")\n",
    "batch_x, batch_y = next(generator)\n",
    "print(\"Batch Shape:\", batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 TensorFlow Datasets (TFDS)\n",
    "Finally, the easiest way to access standard datasets. We will load **CIFAR-10** as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load CIFAR-10\n",
    "# with_info=True returns metadata about the dataset\n",
    "data, info = tfds.load(\"cifar10\", with_info=True)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']\n",
    "\n",
    "print(\"\\n--- TFDS Info ---\")\n",
    "print(\"Dataset Size:\", info.splits['train'].num_examples)\n",
    "print(\"Features:\", info.features)\n",
    "\n",
    "# Pipeline for TFDS\n",
    "def format_data(data):\n",
    "    image = tf.cast(data['image'], tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    return image, data['label']\n",
    "\n",
    "train_data = train_data.map(format_data).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\n--- Testing TFDS Pipeline ---\")\n",
    "for img, label in train_data.take(1):\n",
    "    print(\"Image Batch Shape:\", img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Chapter Summary\n",
    "\n",
    "In this chapter, we moved from basic TensorFlow primitives to professional-grade model building and data handling.\n",
    "\n",
    "* **Keras APIs:**\n",
    "    * Use **Sequential** for simple stacks of layers.\n",
    "    * Use **Functional** for complex topologies (multi-input/output, shared layers).\n",
    "    * Use **Subclassing** for custom training loops and dynamic behaviors.\n",
    "* **Data Pipelines:**\n",
    "    * **`tf.data`** is the gold standard. It creates highly optimized, asynchronous pipelines that prevent GPU starvation.\n",
    "    * **`ImageDataGenerator`** is convenient for quick augmentation but less scalable.\n",
    "    * **`TFDS`** provides instant access to standard academic datasets.\n",
    "\n",
    "In the next chapter, we will combine these skills to dip our toes into deep learning by building Fully Connected Networks, CNNs, and RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
