{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: TensorBoard: Big Brother of TensorFlow\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "Deep learning models are often referred to as \"Black Boxes\" because their internal decision-making processes are complex and opaque. Furthermore, training these models is time-consuming and prone to silent failures (e.g., dead neurons, vanishing gradients).\n",
    "\n",
    "This chapter introduces **TensorBoard**, TensorFlow's built-in visualization toolkit. TensorBoard acts as a window into the black box, allowing us to visualize datasets, track training metrics (loss/accuracy) in real-time, inspect internal weights (histograms), profile performance bottlenecks, and visualize high-dimensional embeddings.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **Monitoring:** Tracking scalar metrics (Loss, Accuracy) over time to detect overfitting/underfitting.\n",
    "* **Histograms:** Visualizing the distribution of weights and biases to detect saturation or dead neurons.\n",
    "* **Dimensionality Reduction:** Using PCA/t-SNE to visualize high-dimensional word vectors in 3D space.\n",
    "* **Profiling:** Identifying whether your model is CPU-bound or GPU-bound.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Using `tf.keras.callbacks.TensorBoard` for automatic logging.\n",
    "* Using `tf.summary` for custom logging in custom training loops.\n",
    "* Visualizing Image Data on the TensorBoard dashboard.\n",
    "* Setting up the **Embedding Projector** to visualize Word2Vec/GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 How TensorBoard Works\n",
    "TensorBoard does not read data directly from your program's memory. Instead, it relies on **Event Files** (logs).\n",
    "\n",
    "1.  **Summary Writer:** In your TensorFlow code, you define a `SummaryWriter` that points to a specific directory (e.g., `logs/run1`).\n",
    "2.  **Writing Events:** During training, you push data (scalars, images, histograms) to this writer.\n",
    "3.  **TensorBoard Server:** You launch a separate process (the TensorBoard server) that watches the log directory. It parses the event files and renders them as interactive web pages.\n",
    "\n",
    "### 2.2 Key Visualizations\n",
    "\n",
    "1.  **Scalars:** Line charts tracking values that change over time (Epochs/Steps). Crucial for Loss and Accuracy.\n",
    "2.  **Images:** Allows viewing the actual image data feeding into the model. Useful to verify data augmentation (e.g., did the rotation flip the image correctly?).\n",
    "3.  **Histograms:** 3D plots showing the distribution of tensor values (weights/biases) over time. Helps identifying:\n",
    "    * *Vanishing Gradients:* Values concentrate at 0.\n",
    "    * *Dead Relu:* Values stay negative.\n",
    "4.  **Embeddings (Projector):** Projects high-dimensional vectors (e.g., 128D word vectors) into 3D space using algorithms like PCA or t-SNE.\n",
    "\n",
    "### 2.3 Profiling\n",
    "Deep learning training pipelines are complex. Bottlenecks can occur in:\n",
    "    * **Data Pipeline:** The GPU is starving because the CPU takes too long to load/augment images.\n",
    "    * **Kernel Launch:** The CPU is slow to tell the GPU what to do.\n",
    "    * **Device Compute:** The model is simply too big.\n",
    "TensorBoard Profiler breaks down the execution time into these components, helping you optimize the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Imports\n",
    "\n",
    "We need to load the TensorBoard extension to view it inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure clean log directory\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Section 1: Visualizing Data with TensorBoard\n",
    "\n",
    "Before training, we should inspect our data. We will load the **Fashion MNIST** dataset and log a batch of images to TensorBoard.\n",
    "\n",
    "### 4.1 Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST\n",
    "dataset, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True)\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']\n",
    "\n",
    "# Map Class IDs to Names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "train_ds = train_ds.map(normalize_img).shuffle(1000).batch(32)\n",
    "test_ds = test_ds.map(normalize_img).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logging Images\n",
    "We use `tf.summary.image` to write image data to the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log directory with timestamp\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join('logs', 'image_viz', current_time)\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Get a single batch of images\n",
    "images, labels = next(iter(train_ds))\n",
    "\n",
    "# Reshape for visualization (Batch, Height, Width, Channels)\n",
    "# Fashion MNIST is (32, 28, 28, 1)\n",
    "\n",
    "with file_writer.as_default():\n",
    "    # Log the first 5 images\n",
    "    # step=0 indicates this is the initial state\n",
    "    tf.summary.image(\"Training data\", images, max_outputs=5, step=0)\n",
    "\n",
    "print(f\"Images logged to {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Section 2: Monitoring Model Training\n",
    "\n",
    "We will build a simple CNN and use `tf.keras.callbacks.TensorBoard` to automatically log metrics (Loss, Accuracy) and weights (Histograms).\n",
    "\n",
    "**Key Argument:** `histogram_freq=1` tells Keras to compute histograms of weights every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define TensorBoard Callback\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1 # Log weight histograms every epoch\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(train_ds, \n",
    "          epochs=3, \n",
    "          validation_data=test_ds, \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Viewing TensorBoard\n",
    "To view the dashboard, you would typically run the following command in a cell. \n",
    "\n",
    "**Note:** In some environments (like standard Jupyter), this opens an interactive window. In others, you might need to run `tensorboard --logdir logs` from your terminal.\n",
    "\n",
    "```python\n",
    "%tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Section 3: Custom Logging with `tf.summary`\n",
    "\n",
    "Sometimes the Keras callback isn't enough. You might want to log weird custom metrics (e.g., the mean value of gradients, or the learning rate schedule) inside a custom training loop.\n",
    "\n",
    "Here, we simulate a custom loop and log the **mean weight** of the first layer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a separate writer for custom metrics\n",
    "custom_log_dir = os.path.join(\"logs\", \"custom\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "summary_writer = tf.summary.create_file_writer(custom_log_dir)\n",
    "\n",
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # --- Custom Logging ---\n",
    "        # Log every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            with summary_writer.as_default():\n",
    "                # 1. Log scalar Loss\n",
    "                tf.summary.scalar('custom_loss', loss_value, step=optimizer.iterations)\n",
    "                \n",
    "                # 2. Log mean weight of first layer\n",
    "                # (To check if weights are exploding or vanishing)\n",
    "                w = model.layers[0].weights[0]\n",
    "                mean_w = tf.reduce_mean(w)\n",
    "                tf.summary.scalar('weight_mean_l0', mean_w, step=optimizer.iterations)\n",
    "                \n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Section 4: Profiling Performance\n",
    "\n",
    "The TensorBoard **Profiler** helps identify if your input pipeline is slow (CPU bound) or if your model operations are slow (GPU bound).\n",
    "\n",
    "To use it, we simply add the `profile_batch` argument to the callback. It defines which batches to monitor (e.g., batches 500 to 520).\n",
    "\n",
    "*Note: Profiling often requires specific GPU drivers and the CUPTI library installed on the host machine.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\", \"profile\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    profile_batch='50,60' # Profile batches 50 to 60\n",
    ")\n",
    "\n",
    "# We would then fit the model as usual:\n",
    "# model.fit(train_ds, epochs=1, callbacks=[tensorboard_callback])\n",
    "print(\"Profiler configured. Check the 'Profile' tab in TensorBoard after running fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Section 5: Visualizing Embeddings (Projector)\n",
    "\n",
    "The Embedding Projector allows us to verify if our model has learned semantic relationships between words. We will download pretrained **GloVe** vectors and visualize them.\n",
    "\n",
    "**Logic:**\n",
    "1. Save the weights of the embedding layer to a checkpoint file.\n",
    "2. Save the vocabulary (metadata) to a TSV file.\n",
    "3. Configure a `projector_config.pbtxt` linking the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "# 1. Create dummy embeddings (Simulating GloVe for demonstration)\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "dummy_weights = tf.Variable(tf.random.normal([vocab_size, embedding_dim]))\n",
    "dummy_vocab = [f\"word_{i}\" for i in range(vocab_size)]\n",
    "\n",
    "# 2. Setup Log Directory\n",
    "log_dir = os.path.join('logs', 'embeddings')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# 3. Save Weights (Checkpoint)\n",
    "checkpoint = tf.train.Checkpoint(embedding=dummy_weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# 4. Save Metadata (TSV)\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n",
    "    for word in dummy_vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "# 5. Configure Projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "projector.visualize_embeddings(log_dir, config)\n",
    "\n",
    "print(f\"Embeddings ready. Run TensorBoard pointing to {log_dir} and check 'Projector' tab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Chapter Summary\n",
    "\n",
    "* **TensorBoard** is indispensable for debugging deep learning models.\n",
    "* **Scalars Tab:** Use it to track Overfitting (when Validation Loss diverges from Training Loss).\n",
    "* **Images Tab:** Use it to sanity check your data pipeline inputs.\n",
    "* **Histograms Tab:** Use it to monitor weight health (check for bell curves; avoid spikes at 0 or -1).\n",
    "* **Profile Tab:** Use it to identify if you need to optimize your `tf.data` pipeline (prefetching/caching) or your model ops.\n",
    "* **Projector Tab:** Use it to visualize high-dimensional embeddings in 3D space using PCA/t-SNE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
