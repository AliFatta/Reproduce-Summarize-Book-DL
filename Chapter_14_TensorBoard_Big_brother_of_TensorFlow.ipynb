{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: TensorBoard — Big Brother of TensorFlow\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "Deep learning models are often described as **black boxes** due to their high-dimensional parameter spaces and non-linear decision boundaries, which make internal behavior difficult to interpret. In addition, training deep neural networks is computationally expensive and susceptible to silent failures such as vanishing gradients, dead neurons, or inefficient data pipelines.\n",
    "\n",
    "This chapter introduces **TensorBoard**, TensorFlow’s built-in visualization and diagnostics toolkit. TensorBoard provides a systematic way to observe, debug, and optimize deep learning models by exposing internal states and training dynamics that would otherwise remain hidden.\n",
    "\n",
    "Through TensorBoard, practitioners can monitor training metrics in real time, inspect parameter distributions, visualize learned embeddings, and profile system-level performance bottlenecks. As a result, TensorBoard plays a crucial role in both model development and experimental reproducibility.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 How TensorBoard Works\n",
    "\n",
    "TensorBoard operates by reading **event log files** generated during model execution rather than directly inspecting a running program. These logs capture time-stamped summaries of tensors, metrics, and metadata.\n",
    "\n",
    "The workflow consists of three main components:\n",
    "1. **Summary Writer:** A TensorFlow object that writes events (scalars, histograms, images, embeddings) to disk.\n",
    "2. **Event Files:** Binary log files that store serialized summary data produced during training or evaluation.\n",
    "3. **TensorBoard Server:** A separate process that monitors the log directory, parses event files, and renders interactive visualizations in a web interface.\n",
    "\n",
    "This decoupled design allows TensorBoard to scale to long-running experiments and large models without interfering with training execution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Monitoring Training with Scalars\n",
    "\n",
    "The most common TensorBoard visualization is the **Scalar Dashboard**, which tracks scalar values as functions of training steps or epochs.\n",
    "\n",
    "Typical scalar metrics include:\n",
    "* Training and validation loss\n",
    "* Training and validation accuracy\n",
    "* Learning rate schedules\n",
    "\n",
    "Formally, a scalar metric can be expressed as a function:\n",
    "\n",
    "$$ s(t) = f(\\theta_t, D) $$\n",
    "\n",
    "where $t$ denotes training time (step or epoch), $\\theta_t$ represents model parameters at time $t$, and $D$ is the dataset. Visualizing these curves enables early detection of **overfitting**, **underfitting**, and unstable optimization behavior.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Histograms and Weight Distributions\n",
    "\n",
    "Histogram visualizations display the distribution of tensor values (such as weights, biases, or gradients) over time. These plots are essential for diagnosing numerical pathologies in deep networks.\n",
    "\n",
    "Key failure modes detectable via histograms include:\n",
    "* **Vanishing gradients:** Parameter updates concentrate near zero.\n",
    "* **Exploding gradients:** Parameter magnitudes grow uncontrollably.\n",
    "* **Dead ReLU units:** Activations remain zero across training iterations.\n",
    "\n",
    "By observing how distributions evolve, practitioners can adjust initialization schemes, activation functions, or optimization hyperparameters to stabilize training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualizing Data and Model Inputs\n",
    "\n",
    "TensorBoard supports direct visualization of input data such as images and audio. This capability is particularly valuable for verifying data preprocessing and augmentation pipelines.\n",
    "\n",
    "For example, when applying random rotations, crops, or color jitter to images, visual inspection ensures that augmentations preserve semantic meaning and do not introduce unintended artifacts.\n",
    "\n",
    "Data visualization serves as a sanity check that helps prevent training on corrupted or mislabeled data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Embedding Projector and Dimensionality Reduction\n",
    "\n",
    "Modern neural networks frequently learn high-dimensional embeddings, such as word vectors or latent feature representations. TensorBoard’s **Embedding Projector** enables visualization of these embeddings by projecting them into two or three dimensions.\n",
    "\n",
    "Common dimensionality reduction techniques include:\n",
    "* **Principal Component Analysis (PCA):** Linear projection maximizing variance.\n",
    "* **t-SNE:** Nonlinear projection preserving local neighborhood structure.\n",
    "\n",
    "Although projections lose information, they provide qualitative insight into semantic relationships, such as word similarity or class clustering in latent space.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Performance Profiling\n",
    "\n",
    "Training deep learning models involves a complex interaction between data loading, CPU execution, and accelerator (GPU/TPU) computation. Performance bottlenecks may arise at different stages of the pipeline.\n",
    "\n",
    "TensorBoard’s **Profiler** decomposes execution time into components such as:\n",
    "* Input pipeline latency\n",
    "* Kernel launch overhead\n",
    "* Device computation time\n",
    "\n",
    "By analyzing these breakdowns, practitioners can identify whether training is limited by data throughput, model complexity, or hardware utilization, enabling targeted optimization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Practical Importance of TensorBoard\n",
    "\n",
    "TensorBoard serves as both a debugging tool and an experimental analysis platform. It enables reproducibility by preserving detailed training logs and supports systematic comparison between experimental runs.\n",
    "\n",
    "In large-scale deep learning workflows, TensorBoard is essential for:\n",
    "* Monitoring long-running experiments\n",
    "* Diagnosing training instabilities\n",
    "* Communicating model behavior to collaborators\n",
    "\n",
    "As model complexity grows, visualization and profiling tools become indispensable components of the deep learning development lifecycle.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Imports\n",
    "\n",
    "We need to load the TensorBoard extension to view it inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure clean log directory\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Section 1: Visualizing Data with TensorBoard\n",
    "\n",
    "Before training, we should inspect our data. We will load the **Fashion MNIST** dataset and log a batch of images to TensorBoard.\n",
    "\n",
    "### 4.1 Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST\n",
    "dataset, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True)\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']\n",
    "\n",
    "# Map Class IDs to Names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "train_ds = train_ds.map(normalize_img).shuffle(1000).batch(32)\n",
    "test_ds = test_ds.map(normalize_img).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logging Images\n",
    "We use `tf.summary.image` to write image data to the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log directory with timestamp\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join('logs', 'image_viz', current_time)\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Get a single batch of images\n",
    "images, labels = next(iter(train_ds))\n",
    "\n",
    "# Reshape for visualization (Batch, Height, Width, Channels)\n",
    "# Fashion MNIST is (32, 28, 28, 1)\n",
    "\n",
    "with file_writer.as_default():\n",
    "    # Log the first 5 images\n",
    "    # step=0 indicates this is the initial state\n",
    "    tf.summary.image(\"Training data\", images, max_outputs=5, step=0)\n",
    "\n",
    "print(f\"Images logged to {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Section 2: Monitoring Model Training\n",
    "\n",
    "We will build a simple CNN and use `tf.keras.callbacks.TensorBoard` to automatically log metrics (Loss, Accuracy) and weights (Histograms).\n",
    "\n",
    "**Key Argument:** `histogram_freq=1` tells Keras to compute histograms of weights every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define TensorBoard Callback\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1 # Log weight histograms every epoch\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(train_ds, \n",
    "          epochs=3, \n",
    "          validation_data=test_ds, \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Viewing TensorBoard\n",
    "To view the dashboard, you would typically run the following command in a cell. \n",
    "\n",
    "**Note:** In some environments (like standard Jupyter), this opens an interactive window. In others, you might need to run `tensorboard --logdir logs` from your terminal.\n",
    "\n",
    "```python\n",
    "%tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Section 3: Custom Logging with `tf.summary`\n",
    "\n",
    "Sometimes the Keras callback isn't enough. You might want to log weird custom metrics (e.g., the mean value of gradients, or the learning rate schedule) inside a custom training loop.\n",
    "\n",
    "Here, we simulate a custom loop and log the **mean weight** of the first layer manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a separate writer for custom metrics\n",
    "custom_log_dir = os.path.join(\"logs\", \"custom\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "summary_writer = tf.summary.create_file_writer(custom_log_dir)\n",
    "\n",
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # --- Custom Logging ---\n",
    "        # Log every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            with summary_writer.as_default():\n",
    "                # 1. Log scalar Loss\n",
    "                tf.summary.scalar('custom_loss', loss_value, step=optimizer.iterations)\n",
    "                \n",
    "                # 2. Log mean weight of first layer\n",
    "                # (To check if weights are exploding or vanishing)\n",
    "                w = model.layers[0].weights[0]\n",
    "                mean_w = tf.reduce_mean(w)\n",
    "                tf.summary.scalar('weight_mean_l0', mean_w, step=optimizer.iterations)\n",
    "                \n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Section 4: Profiling Performance\n",
    "\n",
    "The TensorBoard **Profiler** helps identify if your input pipeline is slow (CPU bound) or if your model operations are slow (GPU bound).\n",
    "\n",
    "To use it, we simply add the `profile_batch` argument to the callback. It defines which batches to monitor (e.g., batches 500 to 520).\n",
    "\n",
    "*Note: Profiling often requires specific GPU drivers and the CUPTI library installed on the host machine.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\", \"profile\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    profile_batch='50,60' # Profile batches 50 to 60\n",
    ")\n",
    "\n",
    "# We would then fit the model as usual:\n",
    "# model.fit(train_ds, epochs=1, callbacks=[tensorboard_callback])\n",
    "print(\"Profiler configured. Check the 'Profile' tab in TensorBoard after running fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Section 5: Visualizing Embeddings (Projector)\n",
    "\n",
    "The Embedding Projector allows us to verify if our model has learned semantic relationships between words. We will download pretrained **GloVe** vectors and visualize them.\n",
    "\n",
    "**Logic:**\n",
    "1. Save the weights of the embedding layer to a checkpoint file.\n",
    "2. Save the vocabulary (metadata) to a TSV file.\n",
    "3. Configure a `projector_config.pbtxt` linking the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "# 1. Create dummy embeddings (Simulating GloVe for demonstration)\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "dummy_weights = tf.Variable(tf.random.normal([vocab_size, embedding_dim]))\n",
    "dummy_vocab = [f\"word_{i}\" for i in range(vocab_size)]\n",
    "\n",
    "# 2. Setup Log Directory\n",
    "log_dir = os.path.join('logs', 'embeddings')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# 3. Save Weights (Checkpoint)\n",
    "checkpoint = tf.train.Checkpoint(embedding=dummy_weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# 4. Save Metadata (TSV)\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n",
    "    for word in dummy_vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "# 5. Configure Projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "projector.visualize_embeddings(log_dir, config)\n",
    "\n",
    "print(f\"Embeddings ready. Run TensorBoard pointing to {log_dir} and check 'Projector' tab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Chapter Summary\n",
    "\n",
    "* **TensorBoard** is indispensable for debugging deep learning models.\n",
    "* **Scalars Tab:** Use it to track Overfitting (when Validation Loss diverges from Training Loss).\n",
    "* **Images Tab:** Use it to sanity check your data pipeline inputs.\n",
    "* **Histograms Tab:** Use it to monitor weight health (check for bell curves; avoid spikes at 0 or -1).\n",
    "* **Profile Tab:** Use it to identify if you need to optimize your `tf.data` pipeline (prefetching/caching) or your model ops.\n",
    "* **Projector Tab:** Use it to visualize high-dimensional embeddings in 3D space using PCA/t-SNE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
