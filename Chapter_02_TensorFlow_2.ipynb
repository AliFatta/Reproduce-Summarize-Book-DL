{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: TensorFlow 2\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "This chapter dives into the core of TensorFlow 2. We move away from the high-level abstract discussions of Chapter 1 into the nuts and bolts of the framework. We will explore the fundamental data structures that make up every TensorFlow program and understand how the framework executes operations under the hood.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **The Computational Graph:** Understanding imperative (eager) vs. declarative (graph) execution.\n",
    "* **Tensors:** The fundamental mathematical object in Deep Learning.\n",
    "* **Variables:** Handling state and mutable parameters (weights/biases) in a network.\n",
    "* **Core Operations:** Matrix multiplication, Convolution, and Pooling.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing a simple Neural Network (MLP) from scratch using low-level TensorFlow.\n",
    "* Manipulating `tf.Tensor` and `tf.Variable` objects.\n",
    "* Using the `@tf.function` decorator to compile Python code into efficient graphs.\n",
    "* performing image processing tasks like edge detection using raw convolution operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Big Shift: TensorFlow 1.x vs. TensorFlow 2.x\n",
    "\n",
    "To understand TensorFlow 2, it helps to know what came before.\n",
    "\n",
    "* **TensorFlow 1.x (Declarative/Graph-based):** You had to define a \"graph\" of computations first (like a blueprint), and then \"run\" it in a Session. It was powerful but difficult to debug because Python code didn't execute immediately.\n",
    "* **TensorFlow 2.x (Imperative/Eager):** Operations are executed immediately as they are called from Python. This makes it intuitive, easy to debug, and very \"Pythonic\".\n",
    "\n",
    "However, we still need the performance of graphs. TF2 bridges this gap with **AutoGraph** and the `@tf.function` decorator, which traces your Python code and converts it into a highly optimized graph automatically.\n",
    "\n",
    "### 2.2 The Building Blocks\n",
    "\n",
    "Every model in TensorFlow is built using three atomic elements:\n",
    "\n",
    "1.  **`tf.Tensor` (Immutable Data):** \n",
    "    * *Definition:* A multidimensional array. Once created, its values cannot be changed.\n",
    "    * *Role:* Holds input data, intermediate layer outputs, and final predictions.\n",
    "    * *Analogy:* Read-only memory or constants.\n",
    "\n",
    "2.  **`tf.Variable` (Mutable State):**\n",
    "    * *Definition:* A wrapper around a tensor that allows its values to be modified (mutated).\n",
    "    * *Role:* Holds the **weights** and **biases** of a neural network. These need to change during training as the model learns.\n",
    "\n",
    "3.  **`tf.Operation` (Transformations):**\n",
    "    * *Definition:* Mathematical computations that consume and produce tensors.\n",
    "    * *Examples:* Matrix multiplication (`tf.matmul`), Addition (`tf.add`), Convolution (`tf.nn.conv2d`).\n",
    "\n",
    "### 2.3 Neural Network Computations\n",
    "\n",
    "Deep learning is essentially a composition of specific mathematical operations:\n",
    "\n",
    "* **Matrix Multiplication:** The engine of Fully Connected (Dense) layers. It transforms input vectors by multiplying them with a weight matrix.\n",
    "* **Convolution:** The engine of Computer Vision. It slides a small window (kernel) over an image to detect features like edges, textures, or shapes.\n",
    "* **Pooling:** A down-sampling operation that reduces the size of data (e.g., taking the maximum value in a small window), making the model robust to small translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Code Reproduction: First Steps with TensorFlow 2\n",
    "\n",
    "We will start by implementing a simple **Multilayer Perceptron (MLP)**. An MLP is a basic neural network with an input layer, hidden layers, and an output layer.\n",
    "\n",
    "We will manually define the weights (`W`) and biases (`b`) as `tf.Variable`s and define the forward pass computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Define Data and Variables\n",
    "# Input data: A single sample with 4 features (Shape: 1x4)\n",
    "x = np.random.normal(size=[1, 4]).astype('float32')\n",
    "\n",
    "# Initialize weights and biases randomly\n",
    "init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "# Layer 1: 4 inputs -> 3 hidden units\n",
    "w1 = tf.Variable(init(shape=[4, 3]))\n",
    "b1 = tf.Variable(init(shape=[1, 3]))\n",
    "\n",
    "# Layer 2: 3 hidden units -> 2 output units\n",
    "w2 = tf.Variable(init(shape=[3, 2]))\n",
    "b2 = tf.Variable(init(shape=[1, 2]))\n",
    "\n",
    "# 2. Define the Forward Pass Function\n",
    "# We use @tf.function to compile this into a graph for performance\n",
    "@tf.function\n",
    "def forward(x, W, b, act):\n",
    "    return act(tf.matmul(x, W) + b)\n",
    "\n",
    "# 3. Execute the Model\n",
    "# Hidden layer computation (Sigmoid activation)\n",
    "h = forward(x, w1, b1, tf.nn.sigmoid)\n",
    "\n",
    "# Output layer computation (Softmax activation)\n",
    "y = forward(h, w2, b2, tf.nn.softmax)\n",
    "\n",
    "print(\"Input x:\", x)\n",
    "print(\"Hidden h:\", h.numpy())\n",
    "print(\"Output y:\", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation\n",
    "\n",
    "1.  **Initialization:** We create `x` as a NumPy array. In TF2, NumPy arrays are automatically converted to Tensors when passed to TF operations.\n",
    "2.  **Variables:** `w1`, `b1`, `w2`, `b2` are created using `tf.Variable`. This tells TensorFlow that these are parameters we might want to update later (e.g., via gradient descent).\n",
    "3.  **`@tf.function`:** This decorator is critical. When Python executes `forward`, TensorFlow traces the operations (`matmul`, `add`, `act`) and builds a graph. Subsequent calls run this optimized graph, not the Python code.\n",
    "4.  **Execution:** We pass `x` through the first layer to get hidden state `h`, then pass `h` to the second layer to get output `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Code Reproduction: TensorFlow Building Blocks\n",
    "\n",
    "Let's explore `tf.Variable` and `tf.Tensor` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Understanding tf.Variable ---\n",
    "\n",
    "# Creating a variable with a specific shape and value\n",
    "v1 = tf.Variable(tf.constant(2.0, shape=[4]), dtype='float32')\n",
    "print(\"v1:\", v1.numpy())\n",
    "\n",
    "# Mutating a variable (assigning new values)\n",
    "# Note: You cannot simply do v1 = new_value. You must use .assign()\n",
    "v1.assign([10.0, 20.0, 30.0, 40.0])\n",
    "print(\"v1 (after assign):\", v1.numpy())\n",
    "\n",
    "# Modifying specific slices\n",
    "v_mat = tf.Variable(np.zeros(shape=[4, 3], dtype='float32'))\n",
    "print(\"\\nMatrix before:\\n\", v_mat.numpy())\n",
    "\n",
    "# Assign 1.0 to the element at row 0, col 2\n",
    "v_mat[0, 2].assign(1.0)\n",
    "print(\"Matrix after single assign:\\n\", v_mat.numpy())\n",
    "\n",
    "# --- Understanding tf.Tensor ---\n",
    "\n",
    "# Tensors are immutable. You cannot do tensor[0] = 5.\n",
    "a = tf.constant(4, shape=[4], dtype='float32')\n",
    "b = tf.constant(2, shape=[4], dtype='float32')\n",
    "\n",
    "# Basic Arithmetic Operations\n",
    "c = a + b  # Element-wise addition\n",
    "d = a * b  # Element-wise multiplication\n",
    "\n",
    "print(\"\\na + b:\", c.numpy())\n",
    "print(\"a * b:\", d.numpy())\n",
    "\n",
    "# Reduction Operations\n",
    "large_matrix = tf.constant(np.random.normal(size=[5, 4, 3]), dtype='float32')\n",
    "sum_all = tf.reduce_sum(large_matrix)\n",
    "sum_axis0 = tf.reduce_sum(large_matrix, axis=0)\n",
    "\n",
    "print(\"\\nSum of all elements:\", sum_all.numpy())\n",
    "print(\"Sum along axis 0 shape:\", sum_axis0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Code Reproduction: Neural Network Operations\n",
    "\n",
    "### 5.1 Convolution\n",
    "\n",
    "Convolution is the mathematical operation behind \"Filters\" in image processing. We will demonstrate how to perform edge detection manually using `tf.nn.convolution`.\n",
    "\n",
    "We will:\n",
    "1.  Load a sample image.\n",
    "2.  Convert it to grayscale.\n",
    "3.  Define a specific kernel (filter) designed to highlight edges (Laplacian filter).\n",
    "4.  Apply the convolution.\n",
    "\n",
    "*Note: The book uses 'baboon.jpg'. We will download a standard sample image to ensure this code runs immediately for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# 1. Load an image\n",
    "# We download a sample image (Lena or similar standard test image)\n",
    "url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\"\n",
    "response = requests.get(url)\n",
    "img_pil = Image.open(BytesIO(response.content))\n",
    "img_pil = img_pil.resize((256, 256)) # Resize for consistency\n",
    "x_rgb = np.array(img_pil).astype('float32')\n",
    "\n",
    "# 2. Convert to Grayscale using Matrix Multiplication\n",
    "# Formula: 0.3*R + 0.59*G + 0.11*B\n",
    "grayscale_weights = tf.constant([[0.3], [0.59], [0.11]])\n",
    "x_gray = tf.matmul(x_rgb, grayscale_weights)\n",
    "# x_gray shape is (256, 256, 1)\n",
    "\n",
    "# Display Original vs Grayscale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(x_rgb.astype('uint8'))\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(tf.squeeze(x_gray), cmap='gray')\n",
    "plt.title(\"Grayscale\")\n",
    "plt.axis('off')\n",
    "\n",
    "# 3. Edge Detection using Convolution\n",
    "# Define an approximate Laplacian filter (detects edges)\n",
    "laplacian_filter = np.array([\n",
    "    [0,  1, 0],\n",
    "    [1, -4, 1],\n",
    "    [0,  1, 0]\n",
    "], dtype='float32')\n",
    "\n",
    "# Reshape data for TensorFlow Convolution\n",
    "# Input Format: [Batch, Height, Width, Channels]\n",
    "x_input = tf.reshape(x_gray, [1, 256, 256, 1])\n",
    "\n",
    "# Filter Format: [Filter_Height, Filter_Width, In_Channels, Out_Channels]\n",
    "filters = tf.reshape(laplacian_filter, [3, 3, 1, 1])\n",
    "\n",
    "# Apply Convolution\n",
    "y_conv = tf.nn.convolution(x_input, filters)\n",
    "\n",
    "# Display Edges\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(tf.squeeze(y_conv), cmap='gray')\n",
    "plt.title(\"Edge Detection\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation\n",
    "\n",
    "**Input:** A 3-channel RGB image.\n",
    "\n",
    "1.  **Grayscale Conversion:** Instead of a library function, we used `tf.matmul`. We treated each pixel as a vector $[R, G, B]$ and multiplied it by the weight vector $[0.3, 0.59, 0.11]^T$. This projects the 3D color space into 1D intensity space.\n",
    "2.  **Reshaping:** TensorFlow expects 4D tensors for convolution: `[Batch, Height, Width, Channels]`. Even for a single image, we add a batch dimension of size 1.\n",
    "3.  **Filter Definition:** The Laplacian kernel `[[0,1,0],[1,-4,1],[0,1,0]]` sums to 0. In flat areas (constant color), the sum is $1+1+1+1-4 = 0$ (black). At edges where values change rapidly, the sum is non-zero (white/gray).\n",
    "4.  **`tf.nn.convolution`:** This low-level operation slides the filter over the input. Unlike Keras layers (`Conv2D`), this does not create variables; it just performs the math using the tensors provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pooling\n",
    "\n",
    "Pooling reduces the size of the image, making the model computationally efficient and translation invariant. We will compare Max Pooling vs. Average Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the output from the previous convolution (Edge detected image)\n",
    "image_input = y_conv # Shape: [1, 254, 254, 1] (size reduced slightly due to valid padding in conv)\n",
    "\n",
    "# Max Pooling\n",
    "# Window size: 2x2, Stride: 2x2\n",
    "z_max = tf.nn.max_pool(image_input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Average Pooling\n",
    "z_avg = tf.nn.avg_pool(image_input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "print(f\"Original Shape: {image_input.shape}\")\n",
    "print(f\"Pooled Shape:   {z_max.shape}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(tf.squeeze(z_max), cmap='gray')\n",
    "plt.title(\"Max Pooling\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tf.squeeze(z_avg), cmap='gray')\n",
    "plt.title(\"Avg Pooling\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "* **Max Pooling:** Keeps the strongest feature in the window. Notice how the edges remain sharp and bright. It effectively \"zooms out\" while preserving dominant features.\n",
    "* **Avg Pooling:** Averages the window. The result appears blurrier or \"softer\" because sharp edges are averaged with the black background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Chapter Summary\n",
    "\n",
    "* **TensorFlow 2 Philosophy:** Eager execution makes TF behave like standard Python code, while `@tf.function` provides the speed of graph execution.\n",
    "* **Variables vs. Constants:** Use `tf.Variable` for model parameters (weights/biases) that need to be updated. Use `tf.constant` or `tf.Tensor` for data that remains static.\n",
    "* **Math Operations:** Deep learning is built on linear algebra. `tf.matmul` is the core of most networks.\n",
    "* **Computer Vision Primitives:**\n",
    "    * **Convolution** detects local patterns (edges).\n",
    "    * **Pooling** reduces dimensionality and adds invariance.\n",
    "* **Practical Takeaway:** You can manipulate images and data using raw TensorFlow operations just like you would with NumPy, but with the added benefit of GPU acceleration and automatic differentiation (which we will cover in future chapters)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
