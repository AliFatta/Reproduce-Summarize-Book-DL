{
 "cells": [
  {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "# Chapter 11: Sequence-to-Sequence Learning: Part 1\n",
  "\n",
  "## 1️⃣ Chapter Overview\n",
  "\n",
  "In previous chapters, sequence modeling tasks such as Sentiment Analysis (Many-to-One) and Language Modeling (One-to-Many / Many-to-Many) were introduced. These tasks assume either a fixed-length output or a direct alignment between input and output tokens. This chapter extends sequence modeling to a more general and challenging setting known as **Sequence-to-Sequence (Seq2Seq) learning**, where both input and output are variable-length sequences with no explicit alignment.\n",
  "\n",
  "The chapter uses **Machine Translation**, specifically English-to-German translation, as the primary application to motivate Seq2Seq learning. This task highlights several real-world challenges, including differing sentence lengths, word reordering across languages, and the need to preserve semantic meaning rather than surface-level word correspondence.\n",
  "\n",
  "To address these challenges, the chapter introduces the **Encoder–Decoder architecture**, implemented using recurrent neural networks (RNNs) and constructed via the Keras **Functional API**. Unlike the Sequential API, the Functional API enables flexible multi-input and multi-output network topologies, which are essential for encoder–decoder models.\n",
  "\n",
  "The chapter further distinguishes between **training-time behavior**, where Teacher Forcing is employed to stabilize optimization, and **inference-time behavior**, where the model must rely on its own predictions through Recursive (autoregressive) decoding.\n",
  "\n",
  "---\n"
 ]
},
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## 2️⃣ Theoretical Explanation\n",
  "\n",
  "### 2.1 The Sequence-to-Sequence Problem\n",
  "\n",
  "Traditional feedforward neural networks require fixed-size inputs and outputs. Recurrent neural networks (RNNs) relax the fixed-size input constraint by processing sequences of arbitrary length, but they are commonly applied to tasks where the output is either a single label (classification) or a sequence aligned with the input (sequence tagging).\n",
  "\n",
  "Machine translation represents a fundamentally harder problem. The input sequence length (English sentence) and output sequence length (German sentence) are rarely equal, and there is often no one-to-one correspondence between input and output tokens. Additionally, syntactic structures differ across languages, requiring the model to learn reordering and long-range dependencies.\n",
  "\n",
  "Formally, the task is to model the conditional probability distribution:\n",
  "\n",
  "$$ P(Y | X) = \\prod_{t=1}^{T'} P(y_t | y_{<t}, X) $$\n",
  "\n",
  "where $X = (x_1, x_2, ..., x_T)$ is the source sequence and $Y = (y_1, y_2, ..., y_{T'})$ is the target sequence. This formulation naturally leads to **autoregressive decoding**, in which each output token depends on all previously generated tokens and the encoded representation of the input sequence.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.2 The Encoder–Decoder Architecture\n",
  "\n",
  "Seq2Seq models decompose the sequence transformation problem into two distinct components: an **encoder** and a **decoder**. This separation allows the model to first build an abstract representation of the input sequence and then generate the output sequence conditioned on that representation.\n",
  "\n",
  "**The Encoder** processes the source sentence one token at a time and updates its hidden state recursively:\n",
  "\n",
  "$$ h_t^{enc} = f(h_{t-1}^{enc}, x_t) $$\n",
  "\n",
  "where $f(\\cdot)$ denotes a recurrent cell such as a GRU or LSTM. The encoder does not produce predictions at each timestep. Instead, it discards intermediate outputs and retains only the final hidden state, commonly referred to as the **context vector**:\n",
  "\n",
  "$$ c = h_T^{enc} $$\n",
  "\n",
  "This context vector is intended to compress the semantic information of the entire input sentence into a fixed-dimensional representation.\n",
  "\n",
  "In this chapter, a **Bidirectional RNN encoder** is used to improve representational capacity. A forward RNN processes the sequence from left to right, while a backward RNN processes it from right to left. The final context vector is constructed by concatenating the terminal states of both directions:\n",
  "\n",
  "$$ c = [h_T^{forward}; h_1^{backward}] $$\n",
  "\n",
  "This design allows the encoder to capture information from both past and future contexts within the source sentence.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "**The Decoder** is an autoregressive recurrent neural network responsible for generating the target sentence one token at a time. It is initialized using the encoder’s context vector $c$, which conditions the entire generation process on the input sentence.\n",
  "\n",
  "At each decoding timestep $t$, the decoder updates its hidden state according to:\n",
  "\n",
  "$$ h_t^{dec} = f(h_{t-1}^{dec}, y_{t-1}, c) $$\n",
  "\n",
  "The decoder then computes a probability distribution over the target vocabulary:\n",
  "\n",
  "$$ P(y_t | y_{<t}, X) = \\text{softmax}(W h_t^{dec} + b) $$\n",
  "\n",
  "This distribution represents the model’s belief about the most likely next token. The decoding process continues iteratively until a special **end-of-sequence (eos)** token is generated or a predefined maximum length is reached.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.3 Training Strategy: Teacher Forcing\n",
  "\n",
  "During training, the decoder must learn to predict the next target token $y_t$ given the previous tokens and the encoded input. If the decoder were to use its own predictions as inputs at early stages of training, small errors would rapidly accumulate, leading to unstable learning dynamics.\n",
  "\n",
  "**Teacher Forcing** addresses this issue by feeding the **ground-truth token** from the previous timestep as input to the decoder, regardless of what the model actually predicted. As a result, the decoder learns under idealized conditions where previous context is always correct.\n",
  "\n",
  "The training objective minimizes the negative log-likelihood of the target sequence:\n",
  "\n",
  "$$ \\mathcal{L} = - \\sum_{t=1}^{T'} \\log P(y_t^{true} | y_{<t}^{true}, X) $$\n",
  "\n",
  "Teacher forcing significantly improves convergence speed and gradient stability. However, it introduces a mismatch between training and inference, known as **exposure bias**, since the model never observes its own prediction errors during training.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.4 Inference Strategy: Recursive Decoding\n",
  "\n",
  "During inference, ground-truth target tokens are unavailable. The decoder must rely entirely on its own predictions to generate the output sequence. The inference procedure proceeds as follows:\n",
  "\n",
  "1. The encoder processes the source sentence and produces the context vector $c$.\n",
  "2. The decoder is initialized with $c$ and a start-of-sequence (sos) token.\n",
  "3. At each timestep, the decoder predicts the next token:\n",
  "\n",
  "$$ \\hat{y}_t = \\arg\\max P(y_t | \\hat{y}_{<t}, X) $$\n",
  "\n",
  "4. The predicted token is fed back as input to the decoder for the next timestep.\n",
  "5. The process repeats until an eos token is generated.\n",
  "\n",
  "Because inference requires step-by-step state propagation, the training architecture cannot be reused directly. Separate encoder and decoder inference models must be defined, sharing learned parameters but operating under different input assumptions.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## Limitations of Vanilla Seq2Seq Models\n",
  "\n",
  "While encoder–decoder architectures successfully enable variable-length sequence transformation, they suffer from a fundamental limitation: all information from the source sequence must be compressed into a single fixed-size context vector.\n",
  "\n",
  "As input sequences become longer, this bottleneck leads to information loss, degraded long-range dependency modeling, and reduced translation quality. This limitation motivates the introduction of **attention mechanisms**, which allow the decoder to dynamically focus on different parts of the encoder’s hidden states during generation.\n",
  "\n",
  "The next chapter extends the Seq2Seq framework by incorporating attention, effectively removing the fixed-context bottleneck.\n"
 ]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Data Preparation\n",
    "\n",
    "We will use a standard English-German translation dataset from [ManyThings.org](http://www.manythings.org/anki/).\n",
    "\n",
    "**Requirements:**\n",
    "1.  Download the dataset.\n",
    "2.  Clean the text.\n",
    "3.  Add `sos` (Start of Sentence) and `eos` (End of Sentence) tokens to the target (German) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Download Dataset\n",
    "url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"deu-eng.zip\", origin=url, extract=True)\n",
    "text_file = os.path.join(os.path.dirname(zip_path), \"deu.txt\")\n",
    "\n",
    "# 2. Load and Preprocess\n",
    "def load_data(path, num_samples=50000):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    \n",
    "    # Format: English \\t German \\t Attribution\n",
    "    # We only care about the first two columns\n",
    "    data = []\n",
    "    for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            english = parts[0]\n",
    "            # Add start and end tokens to the target (German)\n",
    "            german = f\"sos {parts[1]} eos\"\n",
    "            data.append([english, german])\n",
    "            \n",
    "    return np.array(data)\n",
    "\n",
    "raw_data = load_data(text_file)\n",
    "print(f\"Total Samples: {len(raw_data)}\")\n",
    "print(f\"Sample: {raw_data[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Splitting the Data\n",
    "We split the data into Training, Validation, and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices\n",
    "indices = np.arange(len(raw_data))\n",
    "np.random.shuffle(indices)\n",
    "raw_data = raw_data[indices]\n",
    "\n",
    "# Split 80-10-10\n",
    "num_val_samples = int(0.1 * len(raw_data))\n",
    "num_test_samples = int(0.1 * len(raw_data))\n",
    "num_train_samples = len(raw_data) - num_val_samples - num_test_samples\n",
    "\n",
    "train_pairs = raw_data[:num_train_samples]\n",
    "val_pairs = raw_data[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = raw_data[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text Vectorization\n",
    "We need two vectorizers: one for English (Encoder Input) and one for German (Decoder Input/Output).\n",
    "\n",
    "We will use the `TextVectorization` layer which handles standardization, tokenization, and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# --- English Vectorizer ---\n",
    "en_vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    "    standardize='lower_and_strip_punctuation'\n",
    ")\n",
    "\n",
    "# --- German Vectorizer ---\n",
    "# We keep specific punctuation or do custom stripping if needed, \n",
    "# but defaults are usually fine for basic translation.\n",
    "de_vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH + 1, # +1 for offset\n",
    "    standardize='lower_and_strip_punctuation'\n",
    ")\n",
    "\n",
    "# Adapt to the text\n",
    "train_en_texts = train_pairs[:, 0]\n",
    "train_de_texts = train_pairs[:, 1]\n",
    "\n",
    "en_vectorizer.adapt(train_en_texts)\n",
    "de_vectorizer.adapt(train_de_texts)\n",
    "\n",
    "print(\"English Vocabulary Sample:\", en_vectorizer.get_vocabulary()[:10])\n",
    "print(\"German Vocabulary Sample:\", de_vectorizer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Pipeline for Teacher Forcing\n",
    "\n",
    "For training, we need to prepare the data in a specific tuple format `(inputs, outputs)`.\n",
    "\n",
    "**Inputs:** A dictionary containing:\n",
    "1.  `encoder_inputs`: The English sentence.\n",
    "2.  `decoder_inputs`: The German sentence (including `sos`, but WITHOUT `eos`).\n",
    "\n",
    "**Outputs:**\n",
    "1.  The German sentence shifted by one (including `eos`, but WITHOUT `sos`).\n",
    "\n",
    "Example:\n",
    "* Source: \"I like cats\"\n",
    "* Decoder Input: \"sos Ich mag Katzen\"\n",
    "* Decoder Target: \"Ich mag Katzen eos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, deu):\n",
    "    eng = en_vectorizer(eng)\n",
    "    deu = de_vectorizer(deu)\n",
    "    \n",
    "    # Inputs to the model\n",
    "    # 1. Encoder Input (English)\n",
    "    # 2. Decoder Input (German, excluding the last token <eos>)\n",
    "    decoder_input = deu[:, :-1]\n",
    "    \n",
    "    # Targets (German, excluding the first token <sos>)\n",
    "    decoder_target = deu[:, 1:]\n",
    "    \n",
    "    return (\n",
    "        {\"encoder_inputs\": eng, \"decoder_inputs\": decoder_input},\n",
    "        decoder_target\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    eng_texts, deu_texts = pairs[:, 0], pairs[:, 1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, deu_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# Check shapes\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"Encoder Input Shape: {inputs['encoder_inputs'].shape}\")\n",
    "    print(f\"Decoder Input Shape: {inputs['decoder_inputs'].shape}\")\n",
    "    print(f\"Target Shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Building the Seq2Seq Model\n",
    "\n",
    "We use the **Functional API** to connect the Encoder and Decoder.\n",
    "\n",
    "### 4.1 The Encoder\n",
    "The encoder processes the input English sequence. We use a **Bidirectional GRU** to capture context from both directions. The crucial output here is the **state**, not the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "\n",
    "# --- Encoder ---\n",
    "encoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# Bidirectional GRU\n",
    "# We return the state to initialize the decoder\n",
    "encoder_gru = layers.Bidirectional(layers.GRU(LATENT_DIM // 2, return_state=True), name=\"encoder_gru\")\n",
    "encoder_out, state_h_fwd, state_h_bwd = encoder_gru(x)\n",
    "\n",
    "# Concatenate forward and backward states to pass to decoder\n",
    "encoder_state = layers.Concatenate()([state_h_fwd, state_h_bwd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Decoder (Training)\n",
    "The decoder takes the German sequence (offset by one) and the Encoder's state. It predicts the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decoder ---\n",
    "decoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "decoder_embedding = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)\n",
    "x = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# Decoder GRU\n",
    "# We initialize it with the Encoder State\n",
    "decoder_gru = layers.GRU(LATENT_DIM, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "decoder_outputs, _ = decoder_gru(x, initial_state=encoder_state)\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = layers.Dense(VOCAB_SIZE, activation=\"softmax\", name=\"decoder_final\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- Full Model ---\n",
    "seq2seq_model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"Seq2Seq_Training\")\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "We compile and train the model. Note that we use `sparse_categorical_crossentropy` because our targets are integers, not one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.compile(\n",
    "    optimizer=\"adam\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = seq2seq_model.fit(\n",
    "    train_ds, \n",
    "    epochs=10, \n",
    "    validation_data=val_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Inference: The Recursive Decoder\n",
    "\n",
    "The training model cannot be used directly for inference because it expects the answer (`decoder_inputs`) to be provided. For inference, we need to generate the answer word by word.\n",
    "\n",
    "We need to define a separate inference architecture that shares the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Inference Encoder\n",
    "# Input: English Sentence -> Output: Context Vector (State)\n",
    "encoder_model = models.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "# 2. Define the Inference Decoder\n",
    "# Inputs: Previous Word + Previous State\n",
    "decoder_state_input = layers.Input(shape=(LATENT_DIM,), name=\"input_state\")\n",
    "decoder_word_input = layers.Input(shape=(1,), name=\"input_word\")\n",
    "\n",
    "# Reuse layers from training model\n",
    "x = decoder_embedding(decoder_word_input)\n",
    "decoder_outputs, decoder_state = decoder_gru(x, initial_state=decoder_state_input)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = models.Model(\n",
    "    [decoder_word_input, decoder_state_input], \n",
    "    [decoder_outputs, decoder_state]\n",
    ")\n",
    "\n",
    "# 3. Decoding Loop\n",
    "def decode_sentence(input_sentence):\n",
    "    # 1. Encode the input\n",
    "    input_seq = en_vectorizer([input_sentence])\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # 2. Start with 'sos' token\n",
    "    # We need the ID for 'sos'\n",
    "    vocab = de_vectorizer.get_vocabulary()\n",
    "    sos_id = vocab.index('sos')\n",
    "    eos_id = vocab.index('eos')\n",
    "    \n",
    "    target_seq = np.array([[sos_id]])\n",
    "    decoded_sentence = []\n",
    "\n",
    "    # 3. Loop until 'eos' or max length\n",
    "    for _ in range(SEQUENCE_LENGTH):\n",
    "        output_tokens, h = decoder_model.predict([target_seq, states_value], verbose=0)\n",
    "\n",
    "        # Sample next token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_word = vocab[sampled_token_index]\n",
    "\n",
    "        if sampled_word == 'eos':\n",
    "            break\n",
    "            \n",
    "        decoded_sentence.append(sampled_word)\n",
    "\n",
    "        # Update the target sequence and state for next step\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "        states_value = h\n",
    "\n",
    "    return \" \".join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Testing the Translation\n",
    "Let's see how our model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"I love cats\",\n",
    "    \"He works every day\",\n",
    "    \"She is happy\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "for sent in test_examples:\n",
    "    translation = decode_sentence(sent)\n",
    "    print(f\"English: {sent}\")\n",
    "    print(f\"German:  {translation}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Chapter Summary\n",
    "\n",
    "* **Seq2Seq Tasks:** Mapping sequences to sequences is fundamentally different from classification. It requires handling variable length inputs *and* outputs.\n",
    "* **Encoder:** Compresses the source sentence into a single **Context Vector** (State). We used a Bidirectional GRU for this.\n",
    "* **Decoder:** Unpacks the Context Vector into the target sentence.\n",
    "* **Teacher Forcing:** During training, we feed the *correct* previous word to the decoder. This stabilizes training.\n",
    "* **Inference:** During testing, we must feed the *predicted* previous word to the decoder (Recursive decoding).\n",
    "* **Performance:** While this basic Seq2Seq model works, it struggles with very long sentences because the entire meaning must be compressed into one vector. In the next chapter, we will solve this with **Attention**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
