{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Sequence-to-Sequence Learning: Part 1\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we covered Sentiment Analysis (Many-to-One) and Language Modeling (One-to-Many / Many-to-Many). This chapter introduces **Sequence-to-Sequence (Seq2Seq)** learning, a paradigm used when we need to map an input sequence of arbitrary length to an output sequence of arbitrary length.\n",
    "\n",
    "We will build a **Machine Translation** system to translate English sentences into German. We will move beyond the standard `Sequential` API and strictly use the **Functional API** to handle the complex topology of Encoder-Decoder networks. We will also explore **Teacher Forcing** during training and **Recursive Decoding** during inference.\n",
    "\n",
    "### Key Machine Learning Concepts:\n",
    "* **Encoder-Decoder Architecture:** Separating the understanding of input (encoding) from the generation of output (decoding).\n",
    "* **Context Vector:** The bottleneck vector that compresses the meaning of the source sentence.\n",
    "* **Teacher Forcing:** A training strategy where the model uses the *ground truth* previous token as input instead of its own prediction.\n",
    "* **BLEU Score:** The standard metric for evaluating machine generated text against human references.\n",
    "\n",
    "### Practical Skills:\n",
    "* Using the `TextVectorization` layer for end-to-end text processing within the model.\n",
    "* Building sophisticated models with the Keras Functional API.\n",
    "* Separating Training architecture (Teacher Forcing) from Inference architecture (Recursive).\n",
    "* Handling bilingual datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Sequence-to-Sequence Problem\n",
    "Standard neural networks accept fixed-size inputs. Recurrent networks (RNNs) can handle variable input lengths but typically produce an output for every input (tagging) or one output at the end (classification). \n",
    "\n",
    "Machine translation poses a harder problem: the input length (English source) and output length (German translation) are rarely the same, and the word order often changes.\n",
    "\n",
    "### 2.2 The Encoder-Decoder Architecture\n",
    "To solve this, we use two separate RNNs:\n",
    "\n",
    "1.  **The Encoder:** \n",
    "    * Reads the source sequence (English) one token at a time.\n",
    "    * Updates its internal state.\n",
    "    * Discards the outputs but passes the **Final State** (Context Vector) to the decoder.\n",
    "    * *Analogy:* Reading a book and forming a mental summary.\n",
    "\n",
    "2.  **The Decoder:**\n",
    "    * Initialized with the Context Vector from the encoder.\n",
    "    * Generates the target sequence (German) one token at a time.\n",
    "    * *Analogy:* Writing a summary in a different language based on your mental summary.\n",
    "\n",
    "### 2.3 Training Strategy: Teacher Forcing\n",
    "During training, the decoder needs to learn to predict $y_t$ given the history $y_{t-1}, ... y_0$ and the context.\n",
    "\n",
    "If we let the decoder predict $y_1$, it might be wrong. If we feed that *wrong* prediction as input for $y_2$, the model will drift further and further away (error accumulation), making training slow and unstable.\n",
    "\n",
    "**Teacher Forcing** solves this by feeding the **Ground Truth** token from the previous timestep as input to the current timestep, regardless of what the model actually predicted. \n",
    "\n",
    "* **Input to Decoder:** `<sos> Ich bin gut`\n",
    "* **Target for Decoder:** `Ich bin gut <eos>`\n",
    "\n",
    "### 2.4 Inference Strategy: Recursive Decoding\n",
    "During inference (real-world use), we don't have the ground truth. We must rely on the model's own predictions.\n",
    "\n",
    "1.  Feed Encoder the source sentence $\\rightarrow$ Get Context Vector.\n",
    "2.  Feed Decoder the Context Vector + `<sos>` token.\n",
    "3.  Decoder predicts token `A`.\n",
    "4.  Feed token `A` as input to Decoder to predict token `B`.\n",
    "5.  Repeat until `<eos>` is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Data Preparation\n",
    "\n",
    "We will use a standard English-German translation dataset from [ManyThings.org](http://www.manythings.org/anki/).\n",
    "\n",
    "**Requirements:**\n",
    "1.  Download the dataset.\n",
    "2.  Clean the text.\n",
    "3.  Add `sos` (Start of Sentence) and `eos` (End of Sentence) tokens to the target (German) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Download Dataset\n",
    "url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"deu-eng.zip\", origin=url, extract=True)\n",
    "text_file = os.path.join(os.path.dirname(zip_path), \"deu.txt\")\n",
    "\n",
    "# 2. Load and Preprocess\n",
    "def load_data(path, num_samples=50000):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    \n",
    "    # Format: English \\t German \\t Attribution\n",
    "    # We only care about the first two columns\n",
    "    data = []\n",
    "    for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            english = parts[0]\n",
    "            # Add start and end tokens to the target (German)\n",
    "            german = f\"sos {parts[1]} eos\"\n",
    "            data.append([english, german])\n",
    "            \n",
    "    return np.array(data)\n",
    "\n",
    "raw_data = load_data(text_file)\n",
    "print(f\"Total Samples: {len(raw_data)}\")\n",
    "print(f\"Sample: {raw_data[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Splitting the Data\n",
    "We split the data into Training, Validation, and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices\n",
    "indices = np.arange(len(raw_data))\n",
    "np.random.shuffle(indices)\n",
    "raw_data = raw_data[indices]\n",
    "\n",
    "# Split 80-10-10\n",
    "num_val_samples = int(0.1 * len(raw_data))\n",
    "num_test_samples = int(0.1 * len(raw_data))\n",
    "num_train_samples = len(raw_data) - num_val_samples - num_test_samples\n",
    "\n",
    "train_pairs = raw_data[:num_train_samples]\n",
    "val_pairs = raw_data[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = raw_data[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(val_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text Vectorization\n",
    "We need two vectorizers: one for English (Encoder Input) and one for German (Decoder Input/Output).\n",
    "\n",
    "We will use the `TextVectorization` layer which handles standardization, tokenization, and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# --- English Vectorizer ---\n",
    "en_vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    "    standardize='lower_and_strip_punctuation'\n",
    ")\n",
    "\n",
    "# --- German Vectorizer ---\n",
    "# We keep specific punctuation or do custom stripping if needed, \n",
    "# but defaults are usually fine for basic translation.\n",
    "de_vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH + 1, # +1 for offset\n",
    "    standardize='lower_and_strip_punctuation'\n",
    ")\n",
    "\n",
    "# Adapt to the text\n",
    "train_en_texts = train_pairs[:, 0]\n",
    "train_de_texts = train_pairs[:, 1]\n",
    "\n",
    "en_vectorizer.adapt(train_en_texts)\n",
    "de_vectorizer.adapt(train_de_texts)\n",
    "\n",
    "print(\"English Vocabulary Sample:\", en_vectorizer.get_vocabulary()[:10])\n",
    "print(\"German Vocabulary Sample:\", de_vectorizer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Pipeline for Teacher Forcing\n",
    "\n",
    "For training, we need to prepare the data in a specific tuple format `(inputs, outputs)`.\n",
    "\n",
    "**Inputs:** A dictionary containing:\n",
    "1.  `encoder_inputs`: The English sentence.\n",
    "2.  `decoder_inputs`: The German sentence (including `sos`, but WITHOUT `eos`).\n",
    "\n",
    "**Outputs:**\n",
    "1.  The German sentence shifted by one (including `eos`, but WITHOUT `sos`).\n",
    "\n",
    "Example:\n",
    "* Source: \"I like cats\"\n",
    "* Decoder Input: \"sos Ich mag Katzen\"\n",
    "* Decoder Target: \"Ich mag Katzen eos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, deu):\n",
    "    eng = en_vectorizer(eng)\n",
    "    deu = de_vectorizer(deu)\n",
    "    \n",
    "    # Inputs to the model\n",
    "    # 1. Encoder Input (English)\n",
    "    # 2. Decoder Input (German, excluding the last token <eos>)\n",
    "    decoder_input = deu[:, :-1]\n",
    "    \n",
    "    # Targets (German, excluding the first token <sos>)\n",
    "    decoder_target = deu[:, 1:]\n",
    "    \n",
    "    return (\n",
    "        {\"encoder_inputs\": eng, \"decoder_inputs\": decoder_input},\n",
    "        decoder_target\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    eng_texts, deu_texts = pairs[:, 0], pairs[:, 1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, deu_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# Check shapes\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"Encoder Input Shape: {inputs['encoder_inputs'].shape}\")\n",
    "    print(f\"Decoder Input Shape: {inputs['decoder_inputs'].shape}\")\n",
    "    print(f\"Target Shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Building the Seq2Seq Model\n",
    "\n",
    "We use the **Functional API** to connect the Encoder and Decoder.\n",
    "\n",
    "### 4.1 The Encoder\n",
    "The encoder processes the input English sequence. We use a **Bidirectional GRU** to capture context from both directions. The crucial output here is the **state**, not the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "\n",
    "# --- Encoder ---\n",
    "encoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# Bidirectional GRU\n",
    "# We return the state to initialize the decoder\n",
    "encoder_gru = layers.Bidirectional(layers.GRU(LATENT_DIM // 2, return_state=True), name=\"encoder_gru\")\n",
    "encoder_out, state_h_fwd, state_h_bwd = encoder_gru(x)\n",
    "\n",
    "# Concatenate forward and backward states to pass to decoder\n",
    "encoder_state = layers.Concatenate()([state_h_fwd, state_h_bwd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Decoder (Training)\n",
    "The decoder takes the German sequence (offset by one) and the Encoder's state. It predicts the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decoder ---\n",
    "decoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "decoder_embedding = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)\n",
    "x = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# Decoder GRU\n",
    "# We initialize it with the Encoder State\n",
    "decoder_gru = layers.GRU(LATENT_DIM, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "decoder_outputs, _ = decoder_gru(x, initial_state=encoder_state)\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = layers.Dense(VOCAB_SIZE, activation=\"softmax\", name=\"decoder_final\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- Full Model ---\n",
    "seq2seq_model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"Seq2Seq_Training\")\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "We compile and train the model. Note that we use `sparse_categorical_crossentropy` because our targets are integers, not one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.compile(\n",
    "    optimizer=\"adam\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = seq2seq_model.fit(\n",
    "    train_ds, \n",
    "    epochs=10, \n",
    "    validation_data=val_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Inference: The Recursive Decoder\n",
    "\n",
    "The training model cannot be used directly for inference because it expects the answer (`decoder_inputs`) to be provided. For inference, we need to generate the answer word by word.\n",
    "\n",
    "We need to define a separate inference architecture that shares the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Inference Encoder\n",
    "# Input: English Sentence -> Output: Context Vector (State)\n",
    "encoder_model = models.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "# 2. Define the Inference Decoder\n",
    "# Inputs: Previous Word + Previous State\n",
    "decoder_state_input = layers.Input(shape=(LATENT_DIM,), name=\"input_state\")\n",
    "decoder_word_input = layers.Input(shape=(1,), name=\"input_word\")\n",
    "\n",
    "# Reuse layers from training model\n",
    "x = decoder_embedding(decoder_word_input)\n",
    "decoder_outputs, decoder_state = decoder_gru(x, initial_state=decoder_state_input)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = models.Model(\n",
    "    [decoder_word_input, decoder_state_input], \n",
    "    [decoder_outputs, decoder_state]\n",
    ")\n",
    "\n",
    "# 3. Decoding Loop\n",
    "def decode_sentence(input_sentence):\n",
    "    # 1. Encode the input\n",
    "    input_seq = en_vectorizer([input_sentence])\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # 2. Start with 'sos' token\n",
    "    # We need the ID for 'sos'\n",
    "    vocab = de_vectorizer.get_vocabulary()\n",
    "    sos_id = vocab.index('sos')\n",
    "    eos_id = vocab.index('eos')\n",
    "    \n",
    "    target_seq = np.array([[sos_id]])\n",
    "    decoded_sentence = []\n",
    "\n",
    "    # 3. Loop until 'eos' or max length\n",
    "    for _ in range(SEQUENCE_LENGTH):\n",
    "        output_tokens, h = decoder_model.predict([target_seq, states_value], verbose=0)\n",
    "\n",
    "        # Sample next token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_word = vocab[sampled_token_index]\n",
    "\n",
    "        if sampled_word == 'eos':\n",
    "            break\n",
    "            \n",
    "        decoded_sentence.append(sampled_word)\n",
    "\n",
    "        # Update the target sequence and state for next step\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "        states_value = h\n",
    "\n",
    "    return \" \".join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Testing the Translation\n",
    "Let's see how our model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"I love cats\",\n",
    "    \"He works every day\",\n",
    "    \"She is happy\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "for sent in test_examples:\n",
    "    translation = decode_sentence(sent)\n",
    "    print(f\"English: {sent}\")\n",
    "    print(f\"German:  {translation}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Chapter Summary\n",
    "\n",
    "* **Seq2Seq Tasks:** Mapping sequences to sequences is fundamentally different from classification. It requires handling variable length inputs *and* outputs.\n",
    "* **Encoder:** Compresses the source sentence into a single **Context Vector** (State). We used a Bidirectional GRU for this.\n",
    "* **Decoder:** Unpacks the Context Vector into the target sentence.\n",
    "* **Teacher Forcing:** During training, we feed the *correct* previous word to the decoder. This stabilizes training.\n",
    "* **Inference:** During testing, we must feed the *predicted* previous word to the decoder (Recursive decoding).\n",
    "* **Performance:** While this basic Seq2Seq model works, it struggles with very long sentences because the entire meaning must be compressed into one vector. In the next chapter, we will solve this with **Attention**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
