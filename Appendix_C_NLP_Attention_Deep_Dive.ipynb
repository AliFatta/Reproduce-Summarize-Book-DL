{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: NLP Deep Dive - Attention & Encodings\n",
    "\n",
    "## 1️⃣ Overview\n",
    "\n",
    "In **Chapter 5** and **Chapter 13**, we used Transformers for translation and classification. While we implemented the code, understanding the *geometry* of these models helps debug and optimize them.\n",
    "\n",
    "This appendix focuses on visualizing the two most abstract components of the Transformer:\n",
    "1.  **Positional Encodings:** How do we represent \"order\" in a model that processes everything in parallel? We will visualize the unique properties of Sinusoidal embeddings.\n",
    "2.  **Attention Masks:** How do we prevent the decoder from \"cheating\" (looking into the future) during training? We will visualize the Look-Ahead Mask.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Positional Encoding Visualization\n",
    "\n",
    "Transformers process tokens in parallel. To the model, \"Man bites Dog\" and \"Dog bites Man\" look identical (bag of words) without positional information.\n",
    "\n",
    "We inject information about position $pos$ into the embedding vector of size $d$ using sine and cosine waves of different frequencies:\n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}}) $$\n",
    "\n",
    "Let's visualize why this works. It creates a unique \"fingerprint\" for every position that the model can easily learn to attend to relative distances (e.g., \"word at $pos+k$\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Generate Encoding for a sentence of length 50 with embedding dimension 512\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth (Embedding Dimension)')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position (Sequence Index)')\n",
    "plt.title(\"Positional Encodings (50 Positions x 512 Dims)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* **X-Axis (Depth):** Represents the 512 dimensions of the word vector.\n",
    "* **Y-Axis (Position):** Represents the token's position in the sentence (0 to 50).\n",
    "* **Pattern:** \n",
    "    * On the left (low dimensions), the wave frequency is high (rapid changes).\n",
    "    * On the right (high dimensions), the wave frequency is low (slow changes).\n",
    "* **Interpretation:** This allows the model to differentiate positions nearby (using high-frequency dimensions) and positions far apart (using low-frequency dimensions), similar to how a clock uses seconds, minutes, and hours hands to represent time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Masking Visualization\n",
    "\n",
    "In the Decoder of a Transformer (like GPT), we must ensure that when predicting the word at position $t$, the model can only see words at positions $0$ to $t-1$. It cannot see $t+1$.\n",
    "\n",
    "We achieve this by adding a **Look-Ahead Mask** to the attention scores. We set the scores of future tokens to negative infinity ($-\\infty$). When passed through Softmax, these become exactly **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    # Band part returns the lower triangular part of the matrix\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# Create a mask for a sequence of length 5\n",
    "seq_len = 5\n",
    "temp_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "print(\"Look Ahead Mask (Numerical):\")\n",
    "print(temp_mask.numpy())\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(temp_mask, cmap='binary')\n",
    "plt.title(\"Look Ahead Mask (White = 1 = Masked)\")\n",
    "plt.xlabel(\"Key Position (Attending To)\")\n",
    "plt.ylabel(\"Query Position (Current Word)\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* **Row 0 (Word 0):** Can only attend to Column 0. All other columns are white (1, meaning masked/blocked).\n",
    "* **Row 2 (Word 2):** Can attend to Columns 0, 1, and 2. Columns 3 and 4 are blocked.\n",
    "\n",
    "This triangle ensures causality: past tokens cannot influence future predictions during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Self-Attention Heatmaps (Matrix Math)\n",
    "\n",
    "Let's simulate the dot-product attention mechanism on dummy data to see how the matrix multiplication results in alignment.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_viz(q, k, v, mask=None):\n",
    "    # q, k, v shape: (batch_size, seq_len, d_model)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Scale\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # Add Mask\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # Softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# Simulate 3 words, embedding dimension 4\n",
    "np.random.seed(42)\n",
    "temp_q = tf.constant(np.random.randn(1, 3, 4), dtype=tf.float32)\n",
    "temp_k = tf.constant(np.random.randn(1, 3, 4), dtype=tf.float32)\n",
    "temp_v = tf.constant(np.random.randn(1, 3, 4), dtype=tf.float32)\n",
    "\n",
    "# 1. Without Mask\n",
    "attn_weights_no_mask = scaled_dot_product_attention_viz(temp_q, temp_k, temp_v)\n",
    "\n",
    "# 2. With Look-Ahead Mask\n",
    "mask = create_look_ahead_mask(3)\n",
    "attn_weights_with_mask = scaled_dot_product_attention_viz(temp_q, temp_k, temp_v, mask)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.matshow(attn_weights_no_mask[0], fignum=0, cmap='viridis')\n",
    "plt.title(\"Full Self-Attention (Encoder)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.matshow(attn_weights_with_mask[0], fignum=0, cmap='viridis')\n",
    "plt.title(\"Masked Self-Attention (Decoder)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* **Left Plot (Encoder):** The attention matrix is fully populated. Every word attends to every other word.\n",
    "* **Right Plot (Decoder):** The upper triangle is dark blue (zero probability). The attention is strictly confined to the lower triangle (history)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
