{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Teaching Machines to See - Image Classification with CNNs\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we covered the fundamentals of Convolutional Neural Networks (CNNs) using simple datasets like CIFAR-10. This chapter takes a significant leap forward by tackling a more realistic and complex challenge: classifying images in the **Tiny ImageNet** dataset using a state-of-the-art architecture, **InceptionNet (GoogLeNet)**.\n",
    "\n",
    "We move beyond simple Sequential models to complex, multi-branch architectures using the Keras Functional API. We also dive deep into **Exploratory Data Analysis (EDA)** for images, which is often overlooked in deep learning tutorials but is crucial for real-world success.\n",
    "\n",
    "### Key Learning Goals:\n",
    "1.  **Exploratory Data Analysis (EDA):** How to inspect image datasets, check for class imbalances, and calculate channel statistics before training.\n",
    "2.  **Advanced Data Pipelines:** Using `ImageDataGenerator` for complex directory structures and data augmentation.\n",
    "3.  **Inception Architecture:** Understanding and implementing the **Inception Block**, **1x1 Convolutions** for dimensionality reduction, and **Auxiliary Classifiers**.\n",
    "4.  **Functional API Mastery:** Building non-linear, multi-output topology networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Challenge of Depth\n",
    "As researchers tried to make CNNs deeper to capture more complex features, they ran into two major problems:\n",
    "1.  **Vanishing Gradients:** In very deep networks, the gradient signal fades away as it backpropagates through many layers, making the early layers hard to train.\n",
    "2.  **Computational Explosion:** Adding more layers usually adds more parameters, increasing the risk of overfitting and the computational cost.\n",
    "\n",
    "**InceptionNet** (winner of ILSVRC 2014) proposed a novel solution to these problems: **The Inception Module**.\n",
    "\n",
    "### 2.2 The Inception Module\n",
    "Instead of choosing whether to put a $1\\times1$ convolution, a $3\\times3$ convolution, or a $5\\times5$ convolution at a specific layer, Inception says: **\"Why not do them all?\"**\n",
    "\n",
    "An Inception module applies multiple filters of different sizes to the *same input* in parallel and then concatenates the results. \n",
    "* **Small filters ($1\\times1$, $3\\times3$):** Capture local details.\n",
    "* **Large filters ($5\\times5$):** Capture broader, more abstract features.\n",
    "\n",
    "### 2.3 The Magic of $1\\times1$ Convolutions\n",
    "Doing $5\\times5$ convolutions on inputs with many channels (depth) is expensive. To solve this, Inception uses $1\\times1$ convolutions as a **\"Bottleneck Layer\"**.\n",
    "\n",
    "**Dimensionality Reduction Example:**\n",
    "* **Input:** $28 \\times 28 \\times 192$\n",
    "* **Naive $5\\times5$ Conv (32 filters):** \n",
    "    * Params: $5 \\times 5 \\times 192 \\times 32 \\approx 153,600$\n",
    "* **With $1\\times1$ Bottleneck (reduce depth to 16 first):**\n",
    "    1.  $1\\times1$ Conv (16 filters): $1 \\times 1 \\times 192 \\times 16 \\approx 3,072$\n",
    "    2.  $5\\times5$ Conv (32 filters): $5 \\times 5 \\times 16 \\times 32 \\approx 12,800$\n",
    "    * **Total Params:** $15,872$ (approx **10x reduction!**)\n",
    "\n",
    "### 2.4 Auxiliary Classifiers\n",
    "To combat the vanishing gradient problem in a deep network (22 layers), InceptionNet attaches small \"side\" classifiers to intermediate layers. \n",
    "\n",
    "During training, the loss is calculated as:\n",
    "$$ L_{total} = L_{final} + 0.3 \\cdot L_{aux1} + 0.3 \\cdot L_{aux2} $$\n",
    "\n",
    "These auxiliary branches inject gradient signals earlier in the network, ensuring the lower layers learn useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Data Preparation\n",
    "\n",
    "We will implement the setup for **Tiny ImageNet**, a scaled-down version of the massive ImageNet dataset. It contains 200 classes with 500 training images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, AvgPool2D, Dense, Concatenate, Flatten, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Download Utility\n",
    "Since Tiny ImageNet is not in standard Keras datasets, we write a utility to download and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tiny_imagenet(data_dir='data'):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    \n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(data_dir, 'tiny-imagenet-200.zip')\n",
    "    extract_path = os.path.join(data_dir, 'tiny-imagenet-200')\n",
    "    \n",
    "    if not os.path.exists(extract_path):\n",
    "        print(\"Downloading Tiny ImageNet...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Dataset already ready.\")\n",
    "\n",
    "# Uncomment the line below to download the dataset (approx 250MB)\n",
    "# download_tiny_imagenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before throwing data into a neural network, we must understand its structure. Tiny ImageNet organizes images by **WordNet IDs (wnids)** (e.g., `n01443537`). We need to map these IDs to human-readable labels (e.g., `goldfish`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('data', 'tiny-imagenet-200')\n",
    "WNIDS_PATH = os.path.join(DATA_DIR, 'wnids.txt')\n",
    "WORDS_PATH = os.path.join(DATA_DIR, 'words.txt')\n",
    "\n",
    "def load_class_names(wnids_path, words_path):\n",
    "    # 1. Load the list of classes used in this dataset\n",
    "    if not os.path.exists(wnids_path):\n",
    "        print(\"Dataset not found. Skipping EDA.\")\n",
    "        return {}\n",
    "\n",
    "    with open(wnids_path, 'r') as f:\n",
    "        wnids = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    # 2. Load the mapping from ID to English Word\n",
    "    with open(words_path, 'r') as f:\n",
    "        words = {}\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split('\\t')\n",
    "                words[parts[0]] = parts[1]\n",
    "    \n",
    "    # 3. Create the specific map for our 200 classes\n",
    "    id_to_class = {wnid: words[wnid] for wnid in wnids if wnid in words}\n",
    "    return id_to_class\n",
    "\n",
    "class_map = load_class_names(WNIDS_PATH, WORDS_PATH)\n",
    "print(f\"Loaded {len(class_map)} classes. Example: n01443537 -> {class_map.get('n01443537', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing the Data\n",
    "Let's write a function to display images from the training set to verify their quality and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(data_dir, class_map, n_classes=5):\n",
    "    if not os.path.exists(data_dir): return\n",
    "    \n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    # Pick random classes\n",
    "    classes = np.random.choice(list(class_map.keys()), n_classes, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, cls in enumerate(classes):\n",
    "        img_dir = os.path.join(train_dir, cls, 'images')\n",
    "        # Pick random image in that class\n",
    "        img_name = np.random.choice(os.listdir(img_dir))\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1, n_classes, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_map[cls].split(',')[0]) # Use first common name\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(DATA_DIR, class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Building Data Pipelines\n",
    "\n",
    "We use `ImageDataGenerator` for efficient loading. \n",
    "* **Note:** Tiny ImageNet has a tricky validation folder structure (images are not separated into subfolders by class). For this tutorial, to keep it runnable, we will treat the **train** folder as our source and split it into training and validation sets.\n",
    "\n",
    "**InceptionNet Input Size:** Original InceptionNet uses $224 \\times 224$. Tiny ImageNet is $64 \\times 64$. We will resize images to **$56 \\times 56$** to suit the modified architecture we will build later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is not present, we will mock generators for demonstration purposes\n",
    "# to ensure the notebook runs without erroring on file not found.\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TARGET_SIZE = (56, 56) # Modified for our scaled-down architecture\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.1,  # Use 10% of training data for validation\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'train'),\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'train'),\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Dataset not found. Using Mock Data Generators for architecture verification.\")\n",
    "    # Mock generator for environments without the 250MB dataset\n",
    "    def mock_gen():\n",
    "        while True:\n",
    "            # Input: Batch of images\n",
    "            X = np.random.rand(BATCH_SIZE, 56, 56, 3)\n",
    "            # Output: One-hot encoded labels for 200 classes\n",
    "            # IMPORTANT: InceptionNet has 3 outputs (1 main, 2 auxiliary)\n",
    "            # We must replicate the target 3 times.\n",
    "            y = np.eye(200)[np.random.choice(200, BATCH_SIZE)]\n",
    "            yield X, [y, y, y]\n",
    "            \n",
    "    train_generator = mock_gen()\n",
    "    validation_generator = mock_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adapting Generators for Multi-Output Models\n",
    "InceptionNet has **3 outputs** (Main, Aux 1, Aux 2). Standard generators yield `(X, y)`. We need a generator that yields `(X, [y, y, y])` so that all three classifiers can calculate loss against the same true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_output_generator(generator):\n",
    "    \"\"\"\n",
    "    Wraps a standard Keras generator to support multi-output models.\n",
    "    Yields X, [y, y, y]\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        X, y = next(generator)\n",
    "        yield X, [y, y, y]\n",
    "\n",
    "# Wrap the generators if they are real Keras DirectoryIterators\n",
    "if isinstance(train_generator, tf.keras.preprocessing.image.DirectoryIterator):\n",
    "    train_gen_wrapper = multi_output_generator(train_generator)\n",
    "    val_gen_wrapper = multi_output_generator(validation_generator)\n",
    "else:\n",
    "    # Mock generator is already handling this\n",
    "    train_gen_wrapper = train_generator\n",
    "    val_gen_wrapper = validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Implementing InceptionNet (GoogLeNet)\n",
    "\n",
    "We will build this complex architecture from the ground up. \n",
    "\n",
    "### 5.1 The Inception Block\n",
    "This function defines the parallel branches:\n",
    "1.  $1\\times1$ Conv\n",
    "2.  $1\\times1$ Reduce -> $3\\times3$ Conv\n",
    "3.  $1\\times1$ Reduce -> $5\\times5$ Conv\n",
    "4.  $3\\times3$ MaxPool -> $1\\times1$ Conv\n",
    "\n",
    "Finally, it concatenates them along the channel axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x, filters):\n",
    "    \"\"\"\n",
    "    Creates an Inception block.\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        filters: List of filter counts [f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pool]\n",
    "    \"\"\"\n",
    "    f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pool = filters\n",
    "\n",
    "    # Branch 1: 1x1 Conv\n",
    "    path1 = Conv2D(f_1x1, (1,1), padding='same', activation='relu')(x)\n",
    "\n",
    "    # Branch 2: 1x1 Reduce -> 3x3 Conv\n",
    "    path2 = Conv2D(f_3x3_r, (1,1), padding='same', activation='relu')(x)\n",
    "    path2 = Conv2D(f_3x3, (3,3), padding='same', activation='relu')(path2)\n",
    "\n",
    "    # Branch 3: 1x1 Reduce -> 5x5 Conv\n",
    "    path3 = Conv2D(f_5x5_r, (1,1), padding='same', activation='relu')(x)\n",
    "    path3 = Conv2D(f_5x5, (5,5), padding='same', activation='relu')(path3)\n",
    "\n",
    "    # Branch 4: MaxPool -> 1x1 Conv\n",
    "    path4 = MaxPool2D((3,3), strides=(1,1), padding='same')(x)\n",
    "    path4 = Conv2D(f_pool, (1,1), padding='same', activation='relu')(path4)\n",
    "\n",
    "    # Concatenate filters\n",
    "    return Concatenate(axis=-1)([path1, path2, path3, path4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The Auxiliary Classifier\n",
    "This small sub-network branches off from the middle of the main network to perform classification. It helps push gradients to the earlier layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary_classifier(x, num_classes, name=None):\n",
    "    x = AvgPool2D((5,5), strides=(3,3))(x)\n",
    "    x = Conv2D(128, (1,1), padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name=name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 The Full Architecture\n",
    "We assemble the pieces: Stem -> Inception Blocks -> Aux Outputs -> Classifier.\n",
    "\n",
    "*Note: We adapt the stem (initial layers) slightly to handle $56\\times56$ inputs instead of the original $224\\times224$ inputs by removing some aggressive downsampling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v1(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # --- STEM ---\n",
    "    # Modified for 56x56 input: Less striding\n",
    "    x = Conv2D(64, (7,7), strides=(1,1), padding='same', activation='relu')(input_layer)\n",
    "    x = MaxPool2D((3,3), strides=(2,2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(64, (1,1), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(192, (3,3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((3,3), strides=(2,2), padding='same')(x)\n",
    "\n",
    "    # --- Inception Blocks (3a, 3b) ---\n",
    "    x = inception_block(x, [64, 96, 128, 16, 32, 32])\n",
    "    x = inception_block(x, [128, 128, 192, 32, 96, 64])\n",
    "    x = MaxPool2D((3,3), strides=(2,2), padding='same')(x)\n",
    "\n",
    "    # --- Inception Blocks (4a - 4e) ---\n",
    "    x = inception_block(x, [192, 96, 208, 16, 48, 64])\n",
    "    \n",
    "    # Auxiliary Output 1\n",
    "    aux1 = auxiliary_classifier(x, num_classes, name='aux1')\n",
    "    \n",
    "    x = inception_block(x, [160, 112, 224, 24, 64, 64])\n",
    "    x = inception_block(x, [128, 128, 256, 24, 64, 64])\n",
    "    x = inception_block(x, [112, 144, 288, 32, 64, 64])\n",
    "    \n",
    "    # Auxiliary Output 2\n",
    "    aux2 = auxiliary_classifier(x, num_classes, name='aux2')\n",
    "    \n",
    "    x = inception_block(x, [256, 160, 320, 32, 128, 128])\n",
    "    x = MaxPool2D((3,3), strides=(2,2), padding='same')(x)\n",
    "\n",
    "    # --- Inception Blocks (5a, 5b) ---\n",
    "    x = inception_block(x, [256, 160, 320, 32, 128, 128])\n",
    "    x = inception_block(x, [384, 192, 384, 48, 128, 128])\n",
    "    \n",
    "    # --- Final Classifier ---\n",
    "    x = AvgPool2D((7,7), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(num_classes, activation='softmax', name='main_out')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=[output, aux1, aux2])\n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "model = inception_v1((56, 56, 3), 200)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Training\n",
    "\n",
    "We compile the model with losses. We weight the main loss higher (1.0) and auxiliary losses lower (0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "    loss_weights=[1.0, 0.3, 0.3],\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create directory for models\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint('models/inception_v1_best.h5', save_best_only=True, monitor='val_main_out_accuracy'),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    CSVLogger('training_log.csv')\n",
    "]\n",
    "\n",
    "# Training steps\n",
    "# Note: If using mock data, steps_per_epoch ensures the loop finishes\n",
    "steps_per_epoch = 50000 // BATCH_SIZE if os.path.exists(DATA_DIR) else 10\n",
    "validation_steps = 5000 // BATCH_SIZE if os.path.exists(DATA_DIR) else 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen_wrapper,\n",
    "    validation_data=val_gen_wrapper,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Evaluation and Analysis\n",
    "\n",
    "Let's visualize the training progress. Since we have multiple outputs, Keras returns metrics for all of them. We focus on the `main_out_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot Main Output Accuracy\n",
    "    plt.plot(history.history['main_out_accuracy'], label='Train Acc (Main)')\n",
    "    plt.plot(history.history['val_main_out_accuracy'], label='Val Acc (Main)')\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Chapter Summary\n",
    "\n",
    "In this chapter, we tackled a complex image classification task using InceptionNet.\n",
    "\n",
    "* **Data Matters:** We performed EDA to understand the WordNet IDs and class distributions of Tiny ImageNet.\n",
    "* **Advanced Architectures:** We broke down the Inception module, understanding how parallel convolutions allow the network to capture features at multiple scales simultaneously.\n",
    "* **Efficiency:** We saw how $1\\times1$ convolutions act as bottlenecks to reduce parameter count, allowing us to build deeper networks without exploding computational costs.\n",
    "* **Training Stability:** We implemented Auxiliary Classifiers to combat the vanishing gradient problem in deep networks.\n",
    "* **Keras Mastery:** We used the Functional API to handle multi-input/multi-output architectures and custom data generators to feed complex label structures.\n",
    "\n",
    "In the next chapter, we will learn how to make these models perform even better using **Transfer Learning** and how to visualize what they are actually seeing using **Grad-CAM**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
