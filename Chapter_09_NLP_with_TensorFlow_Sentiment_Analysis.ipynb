{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Natural Language Processing with TensorFlow: Sentiment Analysis\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we focused on Computer Vision. Now, we switch gears to **Natural Language Processing (NLP)**. NLP is a field of AI focused on enabling computers to understand, interpret, and generate human language. \n",
    "\n",
    "In this chapter, we will build a **Sentiment Analysis** model. The goal is to classify Amazon video game reviews as either **Positive** or **Negative** based on the text content. We will move beyond simple dense networks and introduce **Recurrent Neural Networks (RNNs)**, specifically the **Long Short-Term Memory (LSTM)** network, which is designed to handle sequential data like text.\n",
    "\n",
    "### Key Machine Learning Concepts:\n",
    "* **Text Preprocessing:** Tokenization, Lemmatization, and Stop-word removal.\n",
    "* **Text Representation:** One-Hot Encoding vs. Word Embeddings.\n",
    "* **Sequential Models:** Understanding RNNs and LSTMs.\n",
    "* **Handling Class Imbalance:** Using class weights during training.\n",
    "\n",
    "### Practical Skills:\n",
    "* Building efficient text data pipelines using `tf.data` and `RaggedTensors`.\n",
    "* Implementing custom Keras layers for text vectorization.\n",
    "* Training LSTM models for sequence classification.\n",
    "* Using `tf.keras.layers.Embedding` to learn dense word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 NLP Preprocessing\n",
    "Raw text is messy. Before a model can learn from it, we must clean it. Common steps include:\n",
    "1.  **Tokenization:** Breaking a sentence into individual words (tokens).\n",
    "2.  **Stop Word Removal:** Removing common words that add little meaning (e.g., \"the\", \"is\", \"at\"). *Note: In sentiment analysis, words like \"not\" are crucial and should NOT be removed.*\n",
    "3.  **Lemmatization:** Converting words to their base root form (e.g., \"running\" $\\rightarrow$ \"run\", \"better\" $\\rightarrow$ \"good\"). This reduces the vocabulary size.\n",
    "\n",
    "### 2.2 Representing Text as Numbers\n",
    "Models process numbers, not strings. We need to convert tokens into numerical vectors.\n",
    "\n",
    "#### One-Hot Encoding\n",
    "Each word is represented by a vector of size $V$ (vocabulary size). It contains a single `1` at the index of the word and `0`s elsewhere.\n",
    "* *Pros:* Simple.\n",
    "* *Cons:* High dimensionality, sparse, does not capture semantic similarity (e.g., \"cat\" and \"dog\" are as different as \"cat\" and \"car\").\n",
    "\n",
    "#### Word Embeddings\n",
    "Each word is represented by a dense vector of fixed size $D$ (e.g., 128). These vectors are learned during training.\n",
    "* *Pros:* Lower dimensionality, captures semantic meaning (e.g., \"king\" - \"man\" + \"woman\" $\\approx$ \"queen\").\n",
    "\n",
    "### 2.3 Long Short-Term Memory (LSTM)\n",
    "Standard Feed-Forward networks cannot handle sequential data because they don't have \"memory\" of previous inputs. **Recurrent Neural Networks (RNNs)** solve this by maintaining a hidden state that passes information from one time step to the next.\n",
    "\n",
    "However, standard RNNs suffer from the **Vanishing Gradient Problem**, making them forget long-term dependencies. \n",
    "\n",
    "**LSTMs** improve on RNNs by introducing a **Cell State** ($C_t$) and three gates:\n",
    "1.  **Forget Gate:** Decides what information to discard from the cell state.\n",
    "2.  **Input Gate:** Decides what new information to store in the cell state.\n",
    "3.  **Output Gate:** Decides what to output based on the cell state.\n",
    "\n",
    "This allows LSTMs to remember important context over long sequences, making them ideal for processing reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Data Preparation\n",
    "\n",
    "We will use the **Amazon Video Games Review** dataset. We need to download it, clean the text using `NLTK`, and split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 1. Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 2. Download Dataset\n",
    "def download_data():\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    file_path = os.path.join('data', 'Video_Games_5.json.gz')\n",
    "    json_path = os.path.join('data', 'Video_Games_5.json')\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    if not os.path.exists(json_path):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with gzip.open(file_path, 'rb') as f_in:\n",
    "            with open(json_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    return json_path\n",
    "\n",
    "json_path = download_data()\n",
    "\n",
    "# 3. Load Data\n",
    "# We read the JSON file line by line\n",
    "df = pd.read_json(json_path, lines=True)\n",
    "df = df[['overall', 'verified', 'reviewText']]\n",
    "\n",
    "# Filter for verified reviews and remove empty text\n",
    "df = df[df['verified'] == True]\n",
    "df = df.dropna(subset=['reviewText'])\n",
    "df = df[df['reviewText'].apply(lambda x: len(str(x).strip()) > 0)]\n",
    "\n",
    "# Create Binary Labels\n",
    "# 4, 5 stars -> 1 (Positive)\n",
    "# 1, 2, 3 stars -> 0 (Negative/Neutral)\n",
    "df['label'] = df['overall'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "# Shuffle data\n",
    "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text Cleaning\n",
    "We define a function to clean the text. Note that we strictly exclude 'not' and 'no' from the stopwords list because they flip the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no'}\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Expand contractions (simple heuristic)\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation, and lemmatize\n",
    "    cleaned_tokens = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words and w.isalnum():\n",
    "            # Lemmatize verbs and nouns\n",
    "            lemma = lemmatizer.lemmatize(w, pos='v')\n",
    "            cleaned_tokens.append(lemma)\n",
    "            \n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply cleaning (Using a subset for speed in this demo, use full df for real training)\n",
    "df_small = df.iloc[:20000].copy()\n",
    "print(\"Cleaning text... (this may take a moment)\")\n",
    "df_small['clean_text'] = df_small['reviewText'].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df_small[['reviewText', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Splitting Data\n",
    "We split the data into Training (80%), Validation (10%), and Test (10%). We ensure the validation and test sets are balanced to correctly evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_small['clean_text'].values\n",
    "y = df_small['label'].values\n",
    "\n",
    "# First split: Train vs Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Second split: Val vs Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Val size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tokenization\n",
    "We use Keras `Tokenizer` to convert text to sequences of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(f\"Example sequence: {train_seq[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Building the Data Pipeline\n",
    "\n",
    "We will use `tf.data` to create an efficient pipeline. A key optimization here is **Bucketing**. Instead of padding every sentence to the maximum length (which wastes memory), we group sentences of similar lengths together and pad them to the length of the longest sentence *in that bucket*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sequences, labels, batch_size=32, shuffle=True):\n",
    "    # Create a RaggedTensor (handles variable lengths)\n",
    "    ragged_data = tf.ragged.constant(sequences)\n",
    "    data = tf.data.Dataset.from_tensor_slices((ragged_data, labels))\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.shuffle(buffer_size=1000)\n",
    "\n",
    "    # Bucketing logic\n",
    "    # We define bucket boundaries (e.g., sentences < 10, 10-25, 25-50, >50)\n",
    "    bucket_boundaries = [10, 25, 50]\n",
    "    bucket_batch_sizes = [batch_size] * (len(bucket_boundaries) + 1)\n",
    "    \n",
    "    # Function to get sequence length\n",
    "    length_func = lambda x, y: tf.shape(x)[0]\n",
    "    \n",
    "    # Transformation\n",
    "    dataset = data.apply(\n",
    "        tf.data.experimental.bucket_by_sequence_length(\n",
    "            element_length_func=length_func,\n",
    "            bucket_boundaries=bucket_boundaries,\n",
    "            bucket_batch_sizes=bucket_batch_sizes,\n",
    "            padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])), # Pad x to variable len, y is scalar\n",
    "            padding_values=(0, 0),\n",
    "            drop_remainder=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = create_dataset(train_seq, y_train, BATCH_SIZE)\n",
    "val_ds = create_dataset(val_seq, y_val, BATCH_SIZE, shuffle=False)\n",
    "test_ds = create_dataset(test_seq, y_test, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Model A: One-Hot Encoding + LSTM\n",
    "\n",
    "First, we build a baseline model using a custom **One-Hot Encoding** layer. \n",
    "\n",
    "**Architecture:**\n",
    "Input $\\rightarrow$ Masking $\\rightarrow$ OneHot $\\rightarrow$ LSTM $\\rightarrow$ Dense $\\rightarrow$ Output\n",
    "\n",
    "We need a **Masking layer** because padded zeros should be ignored by the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnehotEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OnehotEncoder, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs come in as (batch, seq_len)\n",
    "        # Cast to int32\n",
    "        x = tf.cast(inputs, tf.int32)\n",
    "        # One-hot encode: (batch, seq_len, depth)\n",
    "        return tf.one_hot(x, depth=self.depth)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask from previous layer\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'depth': self.depth})\n",
    "        return config\n",
    "\n",
    "def build_onehot_model(vocab_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Masking layer: ignores inputs with value 0\n",
    "        tf.keras.layers.Masking(mask_value=0, input_shape=(None,)),\n",
    "        \n",
    "        # Custom One-Hot layer\n",
    "        OnehotEncoder(depth=vocab_size),\n",
    "        \n",
    "        # LSTM Layer\n",
    "        # return_sequences=False -> return only the final hidden state\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        \n",
    "        # Classifier\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_onehot = build_onehot_model(VOCAB_SIZE)\n",
    "model_onehot.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Handling Class Imbalance\n",
    "Review datasets are often imbalanced (more positive reviews than negative). We compute **Class Weights** to force the model to pay more attention to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count = np.sum(y_train == 0)\n",
    "pos_count = np.sum(y_train == 1)\n",
    "total = len(y_train)\n",
    "\n",
    "# Weight for class 0\n",
    "weight_0 = (1 / neg_count) * (total / 2.0)\n",
    "# Weight for class 1\n",
    "weight_1 = (1 / pos_count) * (total / 2.0)\n",
    "\n",
    "class_weights = {0: weight_0, 1: weight_1}\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Compile\n",
    "model_onehot.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Model A\n",
    "*(Note: We train for fewer epochs here for demonstration purposes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_onehot = model_onehot.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Model B: Embeddings + LSTM\n",
    "\n",
    "One-hot encoding creates huge, sparse vectors. **Embeddings** are superior because they are dense and learned. We replace the `OnehotEncoder` with `tf.keras.layers.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_model(vocab_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Embedding Layer\n",
    "        # mask_zero=True acts as the Masking layer automatically\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True),\n",
    "        \n",
    "        # LSTM\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        \n",
    "        # Classifier\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_emb = build_embedding_model(VOCAB_SIZE)\n",
    "model_emb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Training Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_emb = model_emb.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Step-by-Step Explanation\n",
    "\n",
    "### 1. Data Pipeline Construction\n",
    "* **Input:** We have variable-length reviews (lists of integers).\n",
    "* **Process:** We use `tf.ragged.constant` to handle the ragged edges. The `bucket_by_sequence_length` function groups sequences into buckets (e.g., small, medium, large). In a batch of small sequences, we only pad to the size of the longest small sequence, saving massive computation.\n",
    "* **Output:** A `tf.data.Dataset` yielding batches of `(input, label)`.\n",
    "\n",
    "### 2. LSTM Mechanism\n",
    "* **Input:** A sequence of vectors (either one-hot or embeddings).\n",
    "* **Process:** The LSTM loops through the sequence. At each step $t$, it looks at the current word $x_t$ and its previous memory (hidden state $h_{t-1}$ and cell state $C_{t-1}$). It decides what to remember and what to forget using gates.\n",
    "* **Output:** We use `return_sequences=False`, so it outputs only the final state $h_T$ after seeing the whole review. This final vector summarizes the sentiment.\n",
    "\n",
    "### 3. Embedding vs One-Hot\n",
    "* **One-Hot:** The vector size is 10,000 (vocab size). It is mostly zeros.\n",
    "* **Embedding:** The vector size is 128. It is dense. The model learns that words like \"good\" and \"great\" should have similar vectors (be close in vector space), which helps generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Chapter Summary\n",
    "\n",
    "* **NLP Pipeline:** Cleaning text (NLTK) $\\rightarrow$ Tokenizing (Keras) $\\rightarrow$ Bucketing & Batching (`tf.data`).\n",
    "* **Masking:** Crucial for variable-length sequences. It tells the model to ignore the zeros added for padding.\n",
    "* **LSTM:** A powerful RNN variant that mitigates the vanishing gradient problem, allowing it to learn long-term dependencies in text.\n",
    "* **Embeddings:** Learned dense representations of words are far superior to One-Hot encoding for deep learning models.\n",
    "* **Imbalanced Data:** Using class weights ensures the model doesn't just memorize the majority class (Positive reviews) but also learns to detect Negative reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
