{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: TFX — MLOps and Deploying Models with TensorFlow\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In previous chapters, the focus was primarily on model architectures, training strategies, and evaluation techniques. However, in real-world systems, training a model is only one stage in a much broader lifecycle. **MLOps (Machine Learning Operations)** addresses the challenges of deploying, monitoring, and maintaining machine learning systems in production.\n",
    "\n",
    "This chapter introduces **TFX (TensorFlow Extended)**, an end-to-end platform designed to build scalable, reproducible, and reliable machine learning pipelines. TFX formalizes each stage of the ML lifecycle, from raw data ingestion to model serving, ensuring consistency and automation.\n",
    "\n",
    "Using a forest fire severity prediction task as a running example, the chapter demonstrates how to construct a complete production pipeline that includes data validation, feature engineering, model training, evaluation, and deployment using **Docker** and **TensorFlow Serving**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 MLOps and the ML Lifecycle\n",
    "\n",
    "MLOps extends DevOps principles to machine learning systems by emphasizing automation, versioning, and continuous integration. Unlike traditional software, ML systems depend heavily on data, which may change over time and introduce performance degradation.\n",
    "\n",
    "An end-to-end ML lifecycle typically includes:\n",
    "* Data ingestion and validation\n",
    "* Feature engineering and transformation\n",
    "* Model training and evaluation\n",
    "* Deployment and serving\n",
    "* Monitoring and retraining\n",
    "\n",
    "TFX provides a structured framework to manage this lifecycle, reducing ad-hoc experimentation and minimizing production failures.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What is TensorFlow Extended (TFX)?\n",
    "\n",
    "TensorFlow Extended (TFX) is a production-scale machine learning platform built on top of TensorFlow. It enforces best practices by decomposing the ML workflow into modular **components** that communicate through well-defined **artifacts**.\n",
    "\n",
    "Each component produces outputs that are consumed by subsequent components, ensuring traceability, reproducibility, and consistency across experiments and deployments. This pipeline-oriented design allows ML systems to scale from local experimentation to large distributed environments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Core TFX Pipeline Components\n",
    "\n",
    "A standard TFX pipeline consists of a sequence of components, each responsible for a specific stage of the ML workflow:\n",
    "\n",
    "1. **ExampleGen:** Ingests raw data (e.g., CSV or TFRecord) and splits it into training and evaluation datasets.\n",
    "2. **StatisticsGen:** Computes descriptive statistics such as mean, variance, and value distributions. These statistics provide visibility into the dataset and support downstream validation.\n",
    "3. **SchemaGen:** Infers a data schema that defines expected data types, ranges, and categorical domains. The schema acts as a contract for data quality.\n",
    "4. **ExampleValidator:** Detects anomalies and data drift by comparing incoming data statistics against the schema.\n",
    "5. **Transform:** Performs feature engineering using `tensorflow_transform` (tft). Importantly, it outputs a **transform graph** so that identical preprocessing is applied during training and serving, preventing training-serving skew.\n",
    "6. **Trainer:** Trains the machine learning model using TensorFlow/Keras and the transformed features.\n",
    "7. **Evaluator:** Analyzes model performance using `tensorflow_model_analysis` (TFMA), including sliced metrics and comparison against a baseline model.\n",
    "8. **Pusher:** Deploys the validated (\"blessed\") model to a serving destination such as a filesystem, cloud storage, or model server.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Validation and Drift Detection\n",
    "\n",
    "One of the critical challenges in production ML systems is **data drift**, where the statistical properties of incoming data change over time. Such shifts can silently degrade model performance.\n",
    "\n",
    "TFX addresses this issue through automated data validation. By comparing statistics of new data batches against a reference schema, the pipeline can detect anomalies such as missing values, unexpected categories, or distributional shifts.\n",
    "\n",
    "Formally, drift detection involves comparing distributions:\n",
    "\n",
    "$$ D_{train}(x) \\neq D_{serve}(x) $$\n",
    "\n",
    "Early detection enables proactive retraining or data pipeline correction before failures reach production users.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Evaluation and Blessing\n",
    "\n",
    "Before deployment, models must be rigorously evaluated. TFX uses TensorFlow Model Analysis (TFMA) to compute metrics not only on the full dataset but also on specific slices (e.g., by time, location, or category).\n",
    "\n",
    "A model is **blessed** only if it meets predefined performance thresholds and outperforms a baseline model. This gating mechanism prevents regressions and ensures that only validated models are pushed to production.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Model Serving with Docker and TensorFlow Serving\n",
    "\n",
    "After validation, models must be served reliably and efficiently. **TensorFlow Serving** provides a high-performance inference system that supports REST and gRPC APIs.\n",
    "\n",
    "By packaging TensorFlow Serving within **Docker containers**, models can be deployed consistently across development, staging, and production environments. Containerization ensures reproducibility, scalability, and isolation from host system dependencies.\n",
    "\n",
    "This deployment strategy enables seamless integration with modern cloud and microservice-based infrastructures.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Practical Importance of TFX and MLOps\n",
    "\n",
    "TFX transforms machine learning from an experimental activity into an engineering discipline. By enforcing standardized pipelines, it reduces human error, improves reproducibility, and accelerates deployment cycles.\n",
    "\n",
    "In production settings, MLOps frameworks such as TFX are essential for managing complex systems where data, models, and infrastructure evolve continuously.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Data Preparation\n",
    "\n",
    "**Note:** TFX is a heavy library with complex dependencies. This notebook assumes a compatible environment (e.g., Linux/Ubuntu is recommended by the book). The code below reproduces the pipeline construction logic.\n",
    "\n",
    "We will use the **Forest Fires** dataset to predict the burned area based on weather conditions (Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import absl.logging\n",
    "\n",
    "# Set logging level\n",
    "absl.logging.set_verbosity(absl.logging.INFO)\n",
    "\n",
    "# 1. Download Data\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data/csv')\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\"\n",
    "csv_path = os.path.join('data', 'csv', 'forestfires.csv')\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    r = requests.get(url)\n",
    "    with open(csv_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Downloaded Forest Fires dataset.\")\n",
    "\n",
    "# 2. Prepare Train/Test Split\n",
    "# We split manually here to simulate the source data availability\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df = df.sample(frac=0.95, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save split files\n",
    "os.makedirs('data/csv/train', exist_ok=True)\n",
    "train_df.to_csv('data/csv/train/forestfires.csv', index=False)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "# Initialize TFX Interactive Context\n",
    "# This allows running components interactively in a notebook\n",
    "_pipeline_root = './pipeline/'\n",
    "context = InteractiveContext(pipeline_root=_pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Data Ingestion and Validation\n",
    "\n",
    "### 4.1 CsvExampleGen\n",
    "Reads CSV files and converts them into `TFRecord` format, splitting them into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "\n",
    "# Input: Directory containing the training CSV\n",
    "example_gen = CsvExampleGen(input_base='data/csv/train')\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 StatisticsGen & SchemaGen\n",
    "Calculates statistics and infers the data schema (types, domains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen, SchemaGen\n",
    "\n",
    "# 1. Generate Statistics\n",
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "# Visualize statistics (Optional, works in Notebooks)\n",
    "# context.show(statistics_gen.outputs['statistics'])\n",
    "\n",
    "# 2. Infer Schema\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False # Important for flexible shapes downstream\n",
    ")\n",
    "context.run(schema_gen)\n",
    "\n",
    "# context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Data Transformation (Feature Engineering)\n",
    "\n",
    "TFX requires transformation logic to be in a separate Python module file. This ensures the logic is portable.\n",
    "\n",
    "We will define:\n",
    "1.  **Constants:** Feature names and types.\n",
    "2.  **Transform Module:** The `preprocessing_fn` that uses `tensorflow_transform` (`tft`) to scale, bucketize, and vocabularize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_constants.py\n",
    "\n",
    "# Feature Keys\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['DC', 'DMC', 'FFMC', 'ISI', 'rain', 'temp', 'wind', 'X', 'Y']\n",
    "VOCAB_FEATURE_KEYS = ['day', 'month']\n",
    "BUCKET_FEATURE_KEYS = ['RH']\n",
    "BUCKET_FEATURE_BOUNDARIES = [(33, 66)] # Low, Mid, High humidity\n",
    "LABEL_KEY = 'area'\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_transform.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import forest_fires_constants\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
    "_LABEL_KEY = forest_fires_constants.LABEL_KEY\n",
    "_transformed_name = forest_fires_constants.transformed_name\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    outputs = {}\n",
    "    \n",
    "    # Scale dense features to Z-score (Mean 0, Std 1)\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_z_score(inputs[key])\n",
    "        \n",
    "    # Convert categorical strings to Integer IDs\n",
    "    for key in _VOCAB_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "            inputs[key], num_oov_buckets=1)\n",
    "            \n",
    "    # Bucketize numerical features\n",
    "    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
    "        outputs[_transformed_name(key)] = tft.apply_buckets(\n",
    "            inputs[key], bucket_boundaries=[boundaries])\n",
    "            \n",
    "    # Keep label as is (regression target)\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Transform\n",
    "\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file='forest_fires_transform.py'\n",
    ")\n",
    "\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Model Training\n",
    "\n",
    "The **Trainer** component requires a module file that defines:\n",
    "1.  `run_fn`: The entry point for TFX to start training.\n",
    "2.  The model architecture (using Keras).\n",
    "3.  Signatures for serving (how the model accepts requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_trainer.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "import forest_fires_constants\n",
    "import os\n",
    "\n",
    "# Import constants\n",
    "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
    "_LABEL_KEY = forest_fires_constants.LABEL_KEY\n",
    "_transformed_name = forest_fires_constants.transformed_name\n",
    "\n",
    "def _build_keras_model(tf_transform_output):\n",
    "    # We use Feature Columns to handle the inputs\n",
    "    feature_columns = []\n",
    "    \n",
    "    # Numeric columns\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        feature_columns.append(\n",
    "            tf.feature_column.numeric_column(_transformed_name(key)))\n",
    "            \n",
    "    # Categorical columns (Indicator/One-Hot)\n",
    "    for key in _VOCAB_FEATURE_KEYS:\n",
    "        # Get vocab size from the transform output\n",
    "        vocab_size = tf_transform_output.vocabulary_size_by_name(_transformed_name(key))\n",
    "        categorical_col = tf.feature_column.categorical_column_with_identity(\n",
    "            _transformed_name(key), num_buckets=vocab_size + 1)\n",
    "        feature_columns.append(tf.feature_column.indicator_column(categorical_col))\n",
    "\n",
    "    # Bucketized columns\n",
    "    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
    "        num_buckets = len(boundaries) + 1\n",
    "        categorical_col = tf.feature_column.categorical_column_with_identity(\n",
    "            _transformed_name(key), num_buckets=num_buckets)\n",
    "        feature_columns.append(tf.feature_column.indicator_column(categorical_col))\n",
    "        \n",
    "    # Build the model using DenseFeatures layer\n",
    "    # This layer consumes the feature dictionary and applies the feature_columns logic\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    # Define Inputs (This is tricky in TFX Trainer, inputs are usually a dictionary)\n",
    "    inputs = {}\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        inputs[_transformed_name(key)] = tf.keras.Input(shape=(1,), name=_transformed_name(key))\n",
    "        \n",
    "    for key in _VOCAB_FEATURE_KEYS + _BUCKET_FEATURE_KEYS:\n",
    "        inputs[_transformed_name(key)] = tf.keras.Input(shape=(1,), name=_transformed_name(key), dtype=tf.int64)\n",
    "        \n",
    "    x = feature_layer(inputs)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def _input_fn(file_pattern, tf_transform_output, batch_size=200):\n",
    "    # Create a dataset from TFRecords\n",
    "    transformed_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        label_key=_transformed_name(_LABEL_KEY)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "    \n",
    "    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)\n",
    "    \n",
    "    model = _build_keras_model(tf_transform_output)\n",
    "    \n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save the model with signatures\n",
    "    # We need to define a serving function that handles raw strings/bytes\n",
    "    # and applies the transform graph before feeding to the model.\n",
    "    # (Simplified here for brevity, TFX has utilities for this)\n",
    "    \n",
    "    model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Trainer\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file='forest_fires_trainer.py',\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=50)\n",
    ")\n",
    "\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Evaluation and Deployment\n",
    "\n",
    "### 7.1 Evaluator\n",
    "The evaluator checks if the model meets performance thresholds using **TFMA** (TensorFlow Model Analysis).\n",
    "\n",
    "### 7.2 Pusher\n",
    "The pusher deploys the model to a serving location *only if* the evaluator blesses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Pusher\n",
    "from tfx.proto import pusher_pb2\n",
    "\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    # model_blessing=evaluator.outputs['blessing'], # Skipped Evaluator for brevity, but normally required\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory='serving_model_dir'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Serving with Docker\n",
    "\n",
    "Once the model is exported to `serving_model_dir`, we can serve it using the official TensorFlow Serving Docker image.\n",
    "\n",
    "### Step 1: Install Docker\n",
    "Ensure Docker is installed on your machine.\n",
    "\n",
    "### Step 2: Download the TF Serving Image\n",
    "```bash\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "### Step 3: Run the Container\n",
    "We mount the local model directory to the container and map the ports (8501 for REST API).\n",
    "\n",
    "```bash\n",
    "# Assuming your model is in /absolute/path/to/serving_model_dir\n",
    "docker run -p 8501:8501 \\\n",
    "  --mount type=bind,source=/absolute/path/to/serving_model_dir,target=/models/forest_fires_model \\\n",
    "  -e MODEL_NAME=forest_fires_model -t tensorflow/serving\n",
    "```\n",
    "\n",
    "### Step 4: Make a Prediction Request\n",
    "You can use Python `requests` to query the running container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Example input (Must match the raw input schema, not the transformed schema)\n",
    "data = {\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"X\": 7, \"Y\": 5, \"month\": \"mar\", \"day\": \"fri\",\n",
    "            \"FFMC\": 86.2, \"DMC\": 26.2, \"DC\": 94.3, \"ISI\": 5.1,\n",
    "            \"temp\": 8.2, \"RH\": 51, \"wind\": 6.7, \"rain\": 0.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Note: This will only work if the Docker container is actually running.\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:8501/v1/models/forest_fires_model:predict', \n",
    "        data=json.dumps(data)\n",
    "    )\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print(\"Server not reachable (Did you start Docker?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Chapter Summary\n",
    "\n",
    "* **TFX** enables robust, production-ready ML pipelines.\n",
    "* **Data Validation:** Components like `StatisticsGen` and `SchemaGen` prevent \"garbage in, garbage out\" by validating data against a schema.\n",
    "* **Transform:** The `Transform` component ensures that feature engineering logic is consistent between training and serving, preventing training-serving skew.\n",
    "* **Trainer:** We trained a model using the `GenericExecutor` and Keras, utilizing Feature Columns to handle heterogeneous data types (dense, categorical, bucketized).\n",
    "* **Serving:** We demonstrated how to deploy the saved model using **Docker** and **TensorFlow Serving**, exposing it as a REST API for real-time predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
