{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: TFX: MLOps and Deploying Models with TensorFlow\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we focused on model architecture and training. However, in a real-world production environment, training a model is just one small part of the lifecycle. **MLOps** (Machine Learning Operations) focuses on the automation, scalability, and reliability of ML pipelines.\n",
    "\n",
    "This chapter introduces **TFX (TensorFlow Extended)**, an end-to-end platform for deploying production ML pipelines. We will build a complete pipeline to predict forest fire severity, covering data ingestion, validation, transformation, training, evaluation, and finally, serving the model using **Docker** and **TensorFlow Serving**.\n",
    "\n",
    "### Key Machine Learning Concepts:\n",
    "* **MLOps:** DevOps principles applied to Machine Learning (CI/CD/CT).\n",
    "* **TFX Components:** ExampleGen, StatisticsGen, SchemaGen, Transform, Trainer, Evaluator, Pusher.\n",
    "* **Data Drift & Validation:** Automatically detecting anomalies in incoming data.\n",
    "* **Model Serving:** Exposing a trained model via a REST API using Docker containers.\n",
    "\n",
    "### Practical Skills:\n",
    "* Building a TFX pipeline using `InteractiveContext`.\n",
    "* preprocessing data using `tensorflow_transform` (tft).\n",
    "* Validating models with `tensorflow_model_analysis` (TFMA) before deployment.\n",
    "* Containerizing a model server using Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 What is TFX?\n",
    "**TensorFlow Extended (TFX)** is a production-scale machine learning platform based on TensorFlow. Unlike standard research code where data loading and preprocessing might be ad-hoc, TFX standardizes these steps into **Components**.\n",
    "\n",
    "### 2.2 The TFX Pipeline Components\n",
    "A standard TFX pipeline consists of a sequence of components that pass **Artifacts** (data, models, statistics) to each other:\n",
    "\n",
    "1.  **ExampleGen:** Ingests data (CSV, TFRecord) and splits it into training/eval sets.\n",
    "2.  **StatisticsGen:** Calculates statistics (mean, distribution, zeros) for the dataset. Useful for visualization and validation.\n",
    "3.  **SchemaGen:** Infers the data schema (data types, expected range, categories). Acts as a contract for data quality.\n",
    "4.  **ExampleValidator:** Detects anomalies (e.g., missing values in a required column, data drift) based on the schema.\n",
    "5.  **Transform:** Performs feature engineering (normalization, vocabularies, bucketing). Crucially, it outputs a **Transform Graph** so the *exact same* transformations are applied during serving to prevent training-serving skew.\n",
    "6.  **Trainer:** Trains the model using TensorFlow/Keras. It uses the transformed data and the transform graph.\n",
    "7.  **Evaluator:** deep analysis of model performance. It computes metrics on slices of data (e.g., \"How does the model perform on Mondays?\") and validates if the model is \"blessed\" (better than the baseline).\n",
    "8.  **Pusher:** Pushes the blessed model to a serving infrastructure (e.g., filesystem, cloud bucket).\n",
    "\n",
    "### 2.3 Model Serving with Docker\n",
    "**Docker** packages software into containers that run reliably in any environment. **TensorFlow Serving** is a flexible, high-performance serving system for ML models. By combining them, we can spin up a lightweight server that exposes our model via HTTP/REST or gRPC endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Data Preparation\n",
    "\n",
    "**Note:** TFX is a heavy library with complex dependencies. This notebook assumes a compatible environment (e.g., Linux/Ubuntu is recommended by the book). The code below reproduces the pipeline construction logic.\n",
    "\n",
    "We will use the **Forest Fires** dataset to predict the burned area based on weather conditions (Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import absl.logging\n",
    "\n",
    "# Set logging level\n",
    "absl.logging.set_verbosity(absl.logging.INFO)\n",
    "\n",
    "# 1. Download Data\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data/csv')\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\"\n",
    "csv_path = os.path.join('data', 'csv', 'forestfires.csv')\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    r = requests.get(url)\n",
    "    with open(csv_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Downloaded Forest Fires dataset.\")\n",
    "\n",
    "# 2. Prepare Train/Test Split\n",
    "# We split manually here to simulate the source data availability\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df = df.sample(frac=0.95, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save split files\n",
    "os.makedirs('data/csv/train', exist_ok=True)\n",
    "train_df.to_csv('data/csv/train/forestfires.csv', index=False)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "# Initialize TFX Interactive Context\n",
    "# This allows running components interactively in a notebook\n",
    "_pipeline_root = './pipeline/'\n",
    "context = InteractiveContext(pipeline_root=_pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Data Ingestion and Validation\n",
    "\n",
    "### 4.1 CsvExampleGen\n",
    "Reads CSV files and converts them into `TFRecord` format, splitting them into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "\n",
    "# Input: Directory containing the training CSV\n",
    "example_gen = CsvExampleGen(input_base='data/csv/train')\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 StatisticsGen & SchemaGen\n",
    "Calculates statistics and infers the data schema (types, domains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen, SchemaGen\n",
    "\n",
    "# 1. Generate Statistics\n",
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "# Visualize statistics (Optional, works in Notebooks)\n",
    "# context.show(statistics_gen.outputs['statistics'])\n",
    "\n",
    "# 2. Infer Schema\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False # Important for flexible shapes downstream\n",
    ")\n",
    "context.run(schema_gen)\n",
    "\n",
    "# context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Data Transformation (Feature Engineering)\n",
    "\n",
    "TFX requires transformation logic to be in a separate Python module file. This ensures the logic is portable.\n",
    "\n",
    "We will define:\n",
    "1.  **Constants:** Feature names and types.\n",
    "2.  **Transform Module:** The `preprocessing_fn` that uses `tensorflow_transform` (`tft`) to scale, bucketize, and vocabularize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_constants.py\n",
    "\n",
    "# Feature Keys\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['DC', 'DMC', 'FFMC', 'ISI', 'rain', 'temp', 'wind', 'X', 'Y']\n",
    "VOCAB_FEATURE_KEYS = ['day', 'month']\n",
    "BUCKET_FEATURE_KEYS = ['RH']\n",
    "BUCKET_FEATURE_BOUNDARIES = [(33, 66)] # Low, Mid, High humidity\n",
    "LABEL_KEY = 'area'\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_transform.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import forest_fires_constants\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
    "_LABEL_KEY = forest_fires_constants.LABEL_KEY\n",
    "_transformed_name = forest_fires_constants.transformed_name\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    outputs = {}\n",
    "    \n",
    "    # Scale dense features to Z-score (Mean 0, Std 1)\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_z_score(inputs[key])\n",
    "        \n",
    "    # Convert categorical strings to Integer IDs\n",
    "    for key in _VOCAB_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "            inputs[key], num_oov_buckets=1)\n",
    "            \n",
    "    # Bucketize numerical features\n",
    "    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
    "        outputs[_transformed_name(key)] = tft.apply_buckets(\n",
    "            inputs[key], bucket_boundaries=[boundaries])\n",
    "            \n",
    "    # Keep label as is (regression target)\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Transform\n",
    "\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file='forest_fires_transform.py'\n",
    ")\n",
    "\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Model Training\n",
    "\n",
    "The **Trainer** component requires a module file that defines:\n",
    "1.  `run_fn`: The entry point for TFX to start training.\n",
    "2.  The model architecture (using Keras).\n",
    "3.  Signatures for serving (how the model accepts requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile forest_fires_trainer.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "import forest_fires_constants\n",
    "import os\n",
    "\n",
    "# Import constants\n",
    "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
    "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
    "_LABEL_KEY = forest_fires_constants.LABEL_KEY\n",
    "_transformed_name = forest_fires_constants.transformed_name\n",
    "\n",
    "def _build_keras_model(tf_transform_output):\n",
    "    # We use Feature Columns to handle the inputs\n",
    "    feature_columns = []\n",
    "    \n",
    "    # Numeric columns\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        feature_columns.append(\n",
    "            tf.feature_column.numeric_column(_transformed_name(key)))\n",
    "            \n",
    "    # Categorical columns (Indicator/One-Hot)\n",
    "    for key in _VOCAB_FEATURE_KEYS:\n",
    "        # Get vocab size from the transform output\n",
    "        vocab_size = tf_transform_output.vocabulary_size_by_name(_transformed_name(key))\n",
    "        categorical_col = tf.feature_column.categorical_column_with_identity(\n",
    "            _transformed_name(key), num_buckets=vocab_size + 1)\n",
    "        feature_columns.append(tf.feature_column.indicator_column(categorical_col))\n",
    "\n",
    "    # Bucketized columns\n",
    "    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
    "        num_buckets = len(boundaries) + 1\n",
    "        categorical_col = tf.feature_column.categorical_column_with_identity(\n",
    "            _transformed_name(key), num_buckets=num_buckets)\n",
    "        feature_columns.append(tf.feature_column.indicator_column(categorical_col))\n",
    "        \n",
    "    # Build the model using DenseFeatures layer\n",
    "    # This layer consumes the feature dictionary and applies the feature_columns logic\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    # Define Inputs (This is tricky in TFX Trainer, inputs are usually a dictionary)\n",
    "    inputs = {}\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        inputs[_transformed_name(key)] = tf.keras.Input(shape=(1,), name=_transformed_name(key))\n",
    "        \n",
    "    for key in _VOCAB_FEATURE_KEYS + _BUCKET_FEATURE_KEYS:\n",
    "        inputs[_transformed_name(key)] = tf.keras.Input(shape=(1,), name=_transformed_name(key), dtype=tf.int64)\n",
    "        \n",
    "    x = feature_layer(inputs)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def _input_fn(file_pattern, tf_transform_output, batch_size=200):\n",
    "    # Create a dataset from TFRecords\n",
    "    transformed_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        label_key=_transformed_name(_LABEL_KEY)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "    \n",
    "    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)\n",
    "    \n",
    "    model = _build_keras_model(tf_transform_output)\n",
    "    \n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save the model with signatures\n",
    "    # We need to define a serving function that handles raw strings/bytes\n",
    "    # and applies the transform graph before feeding to the model.\n",
    "    # (Simplified here for brevity, TFX has utilities for this)\n",
    "    \n",
    "    model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Trainer\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file='forest_fires_trainer.py',\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=50)\n",
    ")\n",
    "\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Evaluation and Deployment\n",
    "\n",
    "### 7.1 Evaluator\n",
    "The evaluator checks if the model meets performance thresholds using **TFMA** (TensorFlow Model Analysis).\n",
    "\n",
    "### 7.2 Pusher\n",
    "The pusher deploys the model to a serving location *only if* the evaluator blesses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Pusher\n",
    "from tfx.proto import pusher_pb2\n",
    "\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    # model_blessing=evaluator.outputs['blessing'], # Skipped Evaluator for brevity, but normally required\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory='serving_model_dir'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Serving with Docker\n",
    "\n",
    "Once the model is exported to `serving_model_dir`, we can serve it using the official TensorFlow Serving Docker image.\n",
    "\n",
    "### Step 1: Install Docker\n",
    "Ensure Docker is installed on your machine.\n",
    "\n",
    "### Step 2: Download the TF Serving Image\n",
    "```bash\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "### Step 3: Run the Container\n",
    "We mount the local model directory to the container and map the ports (8501 for REST API).\n",
    "\n",
    "```bash\n",
    "# Assuming your model is in /absolute/path/to/serving_model_dir\n",
    "docker run -p 8501:8501 \\\n",
    "  --mount type=bind,source=/absolute/path/to/serving_model_dir,target=/models/forest_fires_model \\\n",
    "  -e MODEL_NAME=forest_fires_model -t tensorflow/serving\n",
    "```\n",
    "\n",
    "### Step 4: Make a Prediction Request\n",
    "You can use Python `requests` to query the running container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Example input (Must match the raw input schema, not the transformed schema)\n",
    "data = {\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"X\": 7, \"Y\": 5, \"month\": \"mar\", \"day\": \"fri\",\n",
    "            \"FFMC\": 86.2, \"DMC\": 26.2, \"DC\": 94.3, \"ISI\": 5.1,\n",
    "            \"temp\": 8.2, \"RH\": 51, \"wind\": 6.7, \"rain\": 0.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Note: This will only work if the Docker container is actually running.\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:8501/v1/models/forest_fires_model:predict', \n",
    "        data=json.dumps(data)\n",
    "    )\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print(\"Server not reachable (Did you start Docker?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Chapter Summary\n",
    "\n",
    "* **TFX** enables robust, production-ready ML pipelines.\n",
    "* **Data Validation:** Components like `StatisticsGen` and `SchemaGen` prevent \"garbage in, garbage out\" by validating data against a schema.\n",
    "* **Transform:** The `Transform` component ensures that feature engineering logic is consistent between training and serving, preventing training-serving skew.\n",
    "* **Trainer:** We trained a model using the `GenericExecutor` and Keras, utilizing Feature Columns to handle heterogeneous data types (dense, categorical, bucketized).\n",
    "* **Serving:** We demonstrated how to deploy the saved model using **Docker** and **TensorFlow Serving**, exposing it as a REST API for real-time predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
