{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Sequence-to-Sequence Learning: Part 2 (Attention)\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In Chapter 11, we built a standard Encoder-Decoder model for machine translation. While effective for short sentences, standard Seq2Seq models suffer from a fundamental **bottleneck**: the Encoder must compress the entire information of a source sentence into a *single fixed-size vector* (the Context Vector).\n",
    "\n",
    "This chapter introduces the **Attention Mechanism** (specifically **Bahdanau Attention**), which solves this bottleneck. Instead of relying on a single static context vector, Attention allows the Decoder to \"look back\" at the entire sequence of Encoder outputs and focus on specific words relevant to the current time step.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **The Information Bottleneck:** Why fixed-size context vectors fail for long sequences.\n",
    "* **Bahdanau Attention (Additive Attention):** A mechanism to compute a dynamic context vector for every decoding step.\n",
    "* **Alignment Scores:** Calculating how relevant an encoder state is to the current decoder state.\n",
    "* **Model Interpretability:** Using attention weights to visualize word-to-word alignment (Heatmaps).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing a custom Keras Layer for Attention (`DecoderRNNAttentionWrapper`) from scratch.\n",
    "* Integrating Attention into a GRU-based Decoder.\n",
    "* Extracting attention weights during inference to generate Alignment Heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Bottleneck Problem\n",
    "In a standard Seq2Seq model:\n",
    "1.  The Encoder reads the input: $X = [x_1, x_2, ..., x_T]$.\n",
    "2.  It produces a final hidden state: $h_T$.\n",
    "3.  The Decoder uses $h_T$ as the *only* source of information to generate the translation.\n",
    "\n",
    "If the sentence is \"The quick brown fox jumps over the lazy dog\" (9 words) or a complex paragraph (100 words), the Encoder is forced to squash all that meaning into a vector of the same size (e.g., 256 floats). Information loss is inevitable, leading to poor translations for long sentences.\n",
    "\n",
    "### 2.2 The Attention Solution\n",
    "Attention allows the Decoder to access **all** encoder hidden states $[h_1, h_2, ..., h_T]$ at every time step. \n",
    "\n",
    "For each step $t$ in the Decoder:\n",
    "1.  **Score:** The model calculates an **alignment score** between the Decoder's previous state $s_{t-1}$ and every Encoder state $h_j$. This score answers: *\"How relevant is the word at position $j$ for generating the next word?\"*\n",
    "2.  **Weight:** The scores are normalized using Softmax to create **Attention Weights** $\\alpha_{tj}$. These weights sum to 1.\n",
    "3.  **Context:** A weighted sum of all encoder states is computed: $c_t = \\sum \\alpha_{tj} h_j$.\n",
    "\n",
    "### 2.3 Bahdanau Attention (Additive)\n",
    "Proposed by Bahdanau et al. (2014), the alignment score is calculated using a small feed-forward neural network:\n",
    "\n",
    "$$ \\text{score}(s_{t-1}, h_j) = v_a^T \\tanh(W_a s_{t-1} + U_a h_j) $$\n",
    "\n",
    "Where $W_a$, $U_a$, and $v_a$ are learnable weight matrices. This mechanism allows the model to learn *where* to look without explicit alignment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Data Preparation\n",
    "\n",
    "We will reuse the English-German translation dataset preparation steps from Chapter 11. We load the data, clean it, vectorization it, and prepare the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- 1. Data Loading (Same as Chapter 11) ---\n",
    "url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"deu-eng.zip\", origin=url, extract=True)\n",
    "text_file = os.path.join(os.path.dirname(zip_path), \"deu.txt\")\n",
    "\n",
    "def load_data(path, num_samples=50000):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    data = []\n",
    "    for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            english = parts[0]\n",
    "            german = f\"sos {parts[1]} eos\" # Start/End tokens\n",
    "            data.append([english, german])\n",
    "    return np.array(data)\n",
    "\n",
    "raw_data = load_data(text_file)\n",
    "print(f\"Loaded {len(raw_data)} samples.\")\n",
    "\n",
    "# --- 2. Train/Val/Test Split ---\n",
    "indices = np.arange(len(raw_data))\n",
    "np.random.shuffle(indices)\n",
    "raw_data = raw_data[indices]\n",
    "\n",
    "num_val = int(0.1 * len(raw_data))\n",
    "num_test = int(0.1 * len(raw_data))\n",
    "train_pairs = raw_data[:-(num_val + num_test)]\n",
    "val_pairs = raw_data[-(num_val + num_test):-num_test]\n",
    "test_pairs = raw_data[-num_test:]\n",
    "\n",
    "# --- 3. Text Vectorization ---\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "en_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH,\n",
    "    standardize='lower_and_strip_punctuation')\n",
    "\n",
    "de_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1,\n",
    "    standardize='lower_and_strip_punctuation')\n",
    "\n",
    "en_vectorizer.adapt(train_pairs[:, 0])\n",
    "de_vectorizer.adapt(train_pairs[:, 1])\n",
    "\n",
    "# --- 4. Dataset Pipeline ---\n",
    "def format_dataset(eng, deu):\n",
    "    eng = en_vectorizer(eng)\n",
    "    deu = de_vectorizer(deu)\n",
    "    return ({'encoder_inputs': eng, 'decoder_inputs': deu[:, :-1]}, deu[:, 1:])\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    eng_texts, deu_texts = pairs[:, 0], pairs[:, 1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, deu_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Custom Attention Layer\n",
    "\n",
    "This is the core contribution of this chapter. Since Keras did not natively support a simple plug-and-play Bahdanau Attention wrapper for RNN cells at the time of writing, we implement it manually.\n",
    "\n",
    "We define `DecoderRNNAttentionWrapper`. This layer acts as a wrapper around a standard GRU cell. Inside its `call` method, it calculates the attention context and feeds the concatenated `[input, context]` to the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNAttentionWrapper(layers.Layer):\n",
    "    def __init__(self, cell_fn, units, **kwargs):\n",
    "        super(DecoderRNNAttentionWrapper, self).__init__(**kwargs)\n",
    "        self._cell_fn = cell_fn  # The basic GRU Cell\n",
    "        self.units = units       # Dimension of Attention internal layers\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape comes as [Encoder_Out_Shape, Decoder_Input_Shape]\n",
    "        # Encoder Out Shape: (batch, time, hidden)\n",
    "        \n",
    "        # W_a: Weight for Encoder Outputs (Values)\n",
    "        self.W_a = self.add_weight(\n",
    "            name='W_a', \n",
    "            shape=tf.TensorShape((input_shape[0][2], self.units)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        # U_a: Weight for Decoder State (Query)\n",
    "        self.U_a = self.add_weight(\n",
    "            name='U_a', \n",
    "            shape=tf.TensorShape((self._cell_fn.units, self.units)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        # V_a: Weight for calculating the final score\n",
    "        self.V_a = self.add_weight(\n",
    "            name='V_a', \n",
    "            shape=tf.TensorShape((self.units, 1)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        super(DecoderRNNAttentionWrapper, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, initial_state, training=False):\n",
    "        # inputs is a list: [encoder_outputs, decoder_inputs]\n",
    "        encoder_outputs, decoder_inputs = inputs\n",
    "        \n",
    "        # Define the step function for K.rnn\n",
    "        # This function runs for every time step of the Decoder\n",
    "        def _step(inputs, states):\n",
    "            # inputs: Single decoder step input\n",
    "            # states: List containing [prev_decoder_state, encoder_outputs]\n",
    "            \n",
    "            prev_decoder_state = states[0]\n",
    "            full_encoder_outputs = states[-1] # Accessed via constants\n",
    "\n",
    "            # --- Attention Score Calculation ---\n",
    "            \n",
    "            # 1. Transform Encoder Outputs: H * W_a \n",
    "            # Result shape: (batch, en_seq_len, units)\n",
    "            W_a_dot_h = K.dot(full_encoder_outputs, self.W_a)\n",
    "            \n",
    "            # 2. Transform Previous Decoder State: S_{t-1} * U_a\n",
    "            # Result shape: (batch, 1, units)\n",
    "            U_a_dot_s = K.expand_dims(K.dot(prev_decoder_state, self.U_a), 1)\n",
    "            \n",
    "            # 3. Tanh Activation: tanh(W_a.H + U_a.S)\n",
    "            # Broadcasting happens here to add (batch, 1, units) to (batch, seq, units)\n",
    "            Wa_plus_Ua = K.tanh(W_a_dot_h + U_a_dot_s)\n",
    "            \n",
    "            # 4. Calculate Score: V_a^T * tanh(...)\n",
    "            # Shape: (batch, en_seq_len, 1)\n",
    "            e_i = K.dot(Wa_plus_Ua, self.V_a)\n",
    "            # Squeeze to remove last dim -> (batch, en_seq_len)\n",
    "            e_i = K.squeeze(e_i, axis=-1) \n",
    "            \n",
    "            # 5. Softmax to get Attention Weights (alpha)\n",
    "            a_i = K.softmax(e_i)\n",
    "            \n",
    "            # --- Context Vector Calculation ---\n",
    "            # Weighted sum of encoder outputs\n",
    "            # context = sum(alpha * H)\n",
    "            c_i = K.sum(full_encoder_outputs * K.expand_dims(a_i, -1), axis=1)\n",
    "            \n",
    "            # --- GRU Step ---\n",
    "            # Concatenate Context Vector + Current Decoder Input\n",
    "            gru_input = K.concatenate([inputs, c_i], axis=-1)\n",
    "            \n",
    "            # Call the basic GRU cell\n",
    "            output, new_states = self._cell_fn(gru_input, states)\n",
    "            \n",
    "            # Return: (Output, Attention_Weights), [New_State]\n",
    "            return (output, a_i), new_states\n",
    "\n",
    "        # K.rnn loops the _step function over the time dimension of decoder_inputs\n",
    "        # constants=[encoder_outputs] ensures encoder_outputs is available in _step\n",
    "        _, outputs, _ = K.rnn(\n",
    "            step_function=_step,\n",
    "            inputs=decoder_inputs,\n",
    "            initial_states=[initial_state],\n",
    "            constants=[encoder_outputs]\n",
    "        )\n",
    "        \n",
    "        # Unpack outputs (GRU outputs, Attention Weights)\n",
    "        decoder_outputs, attention_weights = outputs\n",
    "        return decoder_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Code Explanation\n",
    "\n",
    "1.  **Input:** The layer receives `encoder_outputs` (the full history of the source sentence) and `decoder_inputs` (the target sentence offset by one).\n",
    "2.  **`K.rnn` Loop:** Unlike standard layers that process matrices, we need to iterate step-by-step because the attention for time $t$ depends on the state from $t-1$. `K.rnn` handles this unrolling efficiently.\n",
    "3.  **`_step` Function:** Inside the loop:\n",
    "    * We project the encoder states (`W_a`) and the current decoder state (`U_a`) into a shared latent space `units`.\n",
    "    * We apply `tanh` nonlinearity.\n",
    "    * We project down to a score using `V_a`.\n",
    "    * `K.softmax` converts scores into probabilities ($\\{ \\alpha_1, ..., \\alpha_T \\}$).\n",
    "    * We compute the **Context Vector** as the weighted sum of encoder states.\n",
    "    * We feed `[Decoder_Input, Context_Vector]` into the actual GRU cell.\n",
    "4.  **Output:** The layer returns the GRU outputs (for prediction) AND the attention weights (for visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Building the Attention-Based Seq2Seq Model\n",
    "\n",
    "Now we integrate our custom layer into the functional Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "LATENT_DIM = 256\n",
    "\n",
    "# --- Encoder (Same as Chapter 11) ---\n",
    "encoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "enc_emb = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "encoder_gru = layers.Bidirectional(layers.GRU(LATENT_DIM // 2, return_sequences=True, return_state=True))\n",
    "encoder_out, fwd_state, bwd_state = encoder_gru(enc_emb)\n",
    "\n",
    "# Concatenate forward and backward states for the initial decoder state\n",
    "encoder_state = layers.Concatenate()([fwd_state, bwd_state])\n",
    "\n",
    "# --- Decoder with Attention ---\n",
    "decoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "dec_emb_layer = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Define the Basic GRU Cell\n",
    "decoder_cell = layers.GRUCell(LATENT_DIM)\n",
    "\n",
    "# Wrap it with Attention\n",
    "# Note: We pass the encoder outputs AND decoder embeddings to the call\n",
    "attention_wrapper = DecoderRNNAttentionWrapper(cell_fn=decoder_cell, units=512)\n",
    "decoder_outputs, attention_weights = attention_wrapper(\n",
    "    [encoder_out, dec_emb],\n",
    "    initial_state=encoder_state\n",
    ")\n",
    "\n",
    "# Final Output Layer\n",
    "decoder_dense = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "final_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- Model Compilation ---\n",
    "attn_model = models.Model([encoder_inputs, decoder_inputs], final_outputs)\n",
    "\n",
    "attn_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "attn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Training the Model\n",
    "\n",
    "We train using Teacher Forcing (feeding the correct previous word to generate the next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = attn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5, # Reduced for demonstration speed\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Visualization: Peeking into the \"Brain\"\n",
    "\n",
    "The beauty of Attention is interpretability. We can extract the attention weights to see which English words the model focused on when generating a specific German word.\n",
    "\n",
    "To do this, we need a separate **Inference Model** that outputs the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define Inference Model for Visualization ---\n",
    "# This model takes inputs and returns: Predictions, Attention Weights\n",
    "vis_model = models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs], \n",
    "    outputs=[final_outputs, attention_weights]\n",
    ")\n",
    "\n",
    "def plot_attention(text_en, text_de_input):\n",
    "    # Preprocess\n",
    "    en_seq = en_vectorizer([text_en])\n",
    "    de_seq = de_vectorizer([text_de_input])[:, :-1] # Remove end token for input\n",
    "    \n",
    "    # Predict\n",
    "    preds, attn_weights = vis_model.predict([en_seq, de_seq])\n",
    "    \n",
    "    # attn_weights shape: (1, dec_len, enc_len)\n",
    "    # Squeeze batch dimension\n",
    "    attn_weights = attn_weights[0]\n",
    "    \n",
    "    # Get words for plotting\n",
    "    en_words = text_en.split()\n",
    "    de_words = text_de_input.split()[1:] # Skip 'sos'\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    # Slice the attention matrix to match sentence lengths\n",
    "    # (The model pads to 20, but we only want to visualize real words)\n",
    "    ax.matshow(attn_weights[:len(de_words), :len(en_words)], cmap='viridis')\n",
    "    \n",
    "    ax.set_xticklabels([''] + en_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + de_words)\n",
    "    \n",
    "    plt.xlabel('Source (English)')\n",
    "    plt.ylabel('Target (German)')\n",
    "    plt.show()\n",
    "\n",
    "# Example Visualization\n",
    "# Ideally, we generate the translation first, but for visualization here\n",
    "# we will force feed a known pair to see alignment.\n",
    "sample_en = \"i am very happy\"\n",
    "sample_de = \"sos ich bin sehr glücklich\"\n",
    "\n",
    "plot_attention(sample_en, sample_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Plot\n",
    "\n",
    "If the model learned correctly, you should see a diagonal alignment:\n",
    "* \"ich\" (I) should attend to \"i\".\n",
    "* \"bin\" (am) should attend to \"am\".\n",
    "* \"sehr\" (very) should attend to \"very\".\n",
    "\n",
    "In complex translations where word order flips (e.g., German verbs moving to the end), the attention mechanism allows the model to jump to the correct encoder position effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Chapter Summary\n",
    "\n",
    "* **Limitations of Fixed Context:** Standard Seq2Seq models struggle with long sentences because they compress everything into one vector.\n",
    "* **Attention Mechanism:** Allows the Decoder to create a dynamic context vector at every time step by taking a weighted sum of Encoder states.\n",
    "* **Implementation:** We built a custom `DecoderRNNAttentionWrapper` because `K.rnn` allows us to define custom step logic involving alignment score calculation.\n",
    "* **Alignment:** The attention weights $\\alpha_{tj}$ represent the alignment between the target word at step $t$ and source word at step $j$.\n",
    "* **Result:** Attention not only improves translation accuracy (BLEU scores) but makes the model interpretable via heatmaps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
