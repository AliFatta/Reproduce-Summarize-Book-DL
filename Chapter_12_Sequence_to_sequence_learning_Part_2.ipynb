{
 "cells": [
  {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "# Chapter 12: Sequence-to-Sequence Learning: Part 2 (Attention)\n",
  "\n",
  "## 1️⃣ Chapter Overview\n",
  "\n",
  "In Chapter 11, we implemented a standard Encoder–Decoder Seq2Seq model for machine translation. While effective for short sequences, this architecture suffers from a fundamental limitation known as the **information bottleneck**: the encoder must compress the entire source sentence into a single fixed-size context vector.\n",
  "\n",
  "As sentence length increases, this bottleneck leads to information loss, degraded long-range dependency modeling, and poor translation quality. This chapter introduces the **Attention Mechanism**, specifically **Bahdanau Attention (Additive Attention)**, as a principled solution to this problem.\n",
  "\n",
  "Instead of relying on a single static context vector, Attention allows the decoder to dynamically access **all encoder hidden states** at every decoding step. This enables the model to selectively focus on the most relevant source words when generating each target token.\n",
  "\n",
  "Beyond improving performance, attention also provides a degree of **model interpretability**, as attention weights can be visualized to reveal soft alignments between source and target tokens.\n",
  "\n",
  "---\n"
 ]
},
  {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## 2️⃣ Theoretical Explanation\n",
  "\n",
  "### 2.1 The Bottleneck Problem in Vanilla Seq2Seq Models\n",
  "\n",
  "In a standard Seq2Seq model without attention, the encoder processes the source sequence $X = (x_1, x_2, ..., x_T)$ and produces a sequence of hidden states. However, only the **final hidden state** $h_T$ is passed to the decoder as the context vector.\n",
  "\n",
  "This design forces the model to encode all semantic, syntactic, and contextual information of the source sentence into a vector of fixed dimensionality, regardless of sentence length. Consequently, short sentences and very long sentences are represented using vectors of identical size.\n",
  "\n",
  "As sequence length grows, the encoder must compress increasingly complex information into the same representational capacity. This results in information loss and causes the decoder to struggle with long sentences, rare words, and long-range dependencies. Empirically, this manifests as degraded translation quality for longer inputs.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.2 Attention as a Solution to the Bottleneck\n",
  "\n",
  "The Attention mechanism eliminates the fixed-context bottleneck by allowing the decoder to access **all encoder hidden states** $[h_1, h_2, ..., h_T]$ at each decoding timestep. Instead of relying on a single summary vector, the decoder computes a **dynamic context vector** that changes at every timestep.\n",
  "\n",
  "At decoding step $t$, attention answers the question: *Which parts of the source sentence are most relevant for generating the next target token?* To do this, the model computes a relevance score between the decoder’s previous hidden state and each encoder hidden state.\n",
  "\n",
  "This process consists of three conceptual stages: scoring, weighting, and context aggregation. Together, these steps allow the model to softly align target tokens with source tokens during generation.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "At each decoder timestep $t$, the attention mechanism performs the following computations:\n",
  "\n",
  "1. **Alignment Score Computation**: For each encoder hidden state $h_j$, the model computes an alignment score with the decoder’s previous hidden state $s_{t-1}$. This score reflects how relevant the source position $j$ is for predicting the next target token.\n",
  "\n",
  "2. **Attention Weight Normalization**: The alignment scores are normalized using the Softmax function to produce attention weights $\\alpha_{tj}$, which form a probability distribution over source positions:\n",
  "\n",
  "$$ \\alpha_{tj} = \\frac{\\exp(\\text{score}(s_{t-1}, h_j))}{\\sum_{k=1}^{T} \\exp(\\text{score}(s_{t-1}, h_k))} $$\n",
  "\n",
  "3. **Context Vector Construction**: The dynamic context vector for timestep $t$ is computed as a weighted sum of encoder hidden states:\n",
  "\n",
  "$$ c_t = \\sum_{j=1}^{T} \\alpha_{tj} h_j $$\n",
  "\n",
  "This context vector captures the most relevant source-side information needed to generate the next target token.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.3 Bahdanau Attention (Additive Attention)\n",
  "\n",
  "Bahdanau Attention, also known as **Additive Attention**, was proposed by Bahdanau et al. (2014) and represents one of the earliest and most influential attention mechanisms for neural machine translation.\n",
  "\n",
  "In Bahdanau Attention, the alignment score between the decoder’s previous hidden state $s_{t-1}$ and an encoder hidden state $h_j$ is computed using a small feed-forward neural network:\n",
  "\n",
  "$$ \\text{score}(s_{t-1}, h_j) = v_a^T \\tanh(W_a s_{t-1} + U_a h_j) $$\n",
  "\n",
  "Here, $W_a$, $U_a$, and $v_a$ are learnable parameters. Unlike dot-product attention, this formulation allows the model to learn a nonlinear compatibility function between encoder and decoder states.\n",
  "\n",
  "Bahdanau Attention is particularly effective when encoder and decoder hidden states have different dimensionalities, making it well-suited for RNN-based Seq2Seq models.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### 2.4 Decoder with Attention Integration\n",
  "\n",
  "When attention is incorporated into the decoder, the decoder’s hidden state update depends not only on the previous target token and hidden state, but also on the dynamically computed context vector $c_t$.\n",
  "\n",
  "The decoder state update can be expressed as:\n",
  "\n",
  "$$ s_t = f(s_{t-1}, y_{t-1}, c_t) $$\n",
  "\n",
  "The output distribution over the target vocabulary is then computed using both the decoder state and the context vector. This allows the decoder to condition each prediction on the most relevant parts of the source sentence, effectively mitigating long-sequence degradation.\n",
  "\n",
  "From an architectural perspective, attention transforms the encoder from a static information source into a **memory bank** that the decoder can query at every timestep.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## 3️⃣ Attention and Model Interpretability\n",
  "\n",
  "A key advantage of attention mechanisms is improved **interpretability**. The attention weights $\\alpha_{tj}$ provide a soft alignment between source and target tokens, indicating which source words the model attends to when generating each target word.\n",
  "\n",
  "By visualizing attention weights as heatmaps, it is possible to analyze word-to-word correspondences, diagnose translation errors, and gain insight into the model’s decision-making process. Although attention does not guarantee perfect linguistic alignment, it offers a valuable window into otherwise opaque neural sequence models.\n",
  "\n",
  "---\n"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "## Chapter Summary\n",
  "\n",
  "This chapter extends the Seq2Seq framework by introducing attention mechanisms to address the fixed-context bottleneck of vanilla encoder–decoder models. By computing a dynamic context vector at each decoding step, attention allows the decoder to selectively focus on relevant source tokens and significantly improves performance on long sequences.\n",
  "\n",
  "Bahdanau Attention provides a flexible, learnable alignment function that integrates naturally with RNN-based Seq2Seq models. Beyond performance gains, attention also enhances interpretability by exposing soft alignments between source and target sequences.\n",
  "\n",
  "These ideas form the conceptual foundation for **Transformer models**, which replace recurrent architectures entirely with attention-based mechanisms, as explored in later chapters.\n"
 ]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Data Preparation\n",
    "\n",
    "We will reuse the English-German translation dataset preparation steps from Chapter 11. We load the data, clean it, vectorization it, and prepare the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- 1. Data Loading (Same as Chapter 11) ---\n",
    "url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"deu-eng.zip\", origin=url, extract=True)\n",
    "text_file = os.path.join(os.path.dirname(zip_path), \"deu.txt\")\n",
    "\n",
    "def load_data(path, num_samples=50000):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    data = []\n",
    "    for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            english = parts[0]\n",
    "            german = f\"sos {parts[1]} eos\" # Start/End tokens\n",
    "            data.append([english, german])\n",
    "    return np.array(data)\n",
    "\n",
    "raw_data = load_data(text_file)\n",
    "print(f\"Loaded {len(raw_data)} samples.\")\n",
    "\n",
    "# --- 2. Train/Val/Test Split ---\n",
    "indices = np.arange(len(raw_data))\n",
    "np.random.shuffle(indices)\n",
    "raw_data = raw_data[indices]\n",
    "\n",
    "num_val = int(0.1 * len(raw_data))\n",
    "num_test = int(0.1 * len(raw_data))\n",
    "train_pairs = raw_data[:-(num_val + num_test)]\n",
    "val_pairs = raw_data[-(num_val + num_test):-num_test]\n",
    "test_pairs = raw_data[-num_test:]\n",
    "\n",
    "# --- 3. Text Vectorization ---\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "en_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH,\n",
    "    standardize='lower_and_strip_punctuation')\n",
    "\n",
    "de_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1,\n",
    "    standardize='lower_and_strip_punctuation')\n",
    "\n",
    "en_vectorizer.adapt(train_pairs[:, 0])\n",
    "de_vectorizer.adapt(train_pairs[:, 1])\n",
    "\n",
    "# --- 4. Dataset Pipeline ---\n",
    "def format_dataset(eng, deu):\n",
    "    eng = en_vectorizer(eng)\n",
    "    deu = de_vectorizer(deu)\n",
    "    return ({'encoder_inputs': eng, 'decoder_inputs': deu[:, :-1]}, deu[:, 1:])\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    eng_texts, deu_texts = pairs[:, 0], pairs[:, 1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, deu_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Custom Attention Layer\n",
    "\n",
    "This is the core contribution of this chapter. Since Keras did not natively support a simple plug-and-play Bahdanau Attention wrapper for RNN cells at the time of writing, we implement it manually.\n",
    "\n",
    "We define `DecoderRNNAttentionWrapper`. This layer acts as a wrapper around a standard GRU cell. Inside its `call` method, it calculates the attention context and feeds the concatenated `[input, context]` to the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNAttentionWrapper(layers.Layer):\n",
    "    def __init__(self, cell_fn, units, **kwargs):\n",
    "        super(DecoderRNNAttentionWrapper, self).__init__(**kwargs)\n",
    "        self._cell_fn = cell_fn  # The basic GRU Cell\n",
    "        self.units = units       # Dimension of Attention internal layers\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape comes as [Encoder_Out_Shape, Decoder_Input_Shape]\n",
    "        # Encoder Out Shape: (batch, time, hidden)\n",
    "        \n",
    "        # W_a: Weight for Encoder Outputs (Values)\n",
    "        self.W_a = self.add_weight(\n",
    "            name='W_a', \n",
    "            shape=tf.TensorShape((input_shape[0][2], self.units)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        # U_a: Weight for Decoder State (Query)\n",
    "        self.U_a = self.add_weight(\n",
    "            name='U_a', \n",
    "            shape=tf.TensorShape((self._cell_fn.units, self.units)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        # V_a: Weight for calculating the final score\n",
    "        self.V_a = self.add_weight(\n",
    "            name='V_a', \n",
    "            shape=tf.TensorShape((self.units, 1)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        \n",
    "        super(DecoderRNNAttentionWrapper, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, initial_state, training=False):\n",
    "        # inputs is a list: [encoder_outputs, decoder_inputs]\n",
    "        encoder_outputs, decoder_inputs = inputs\n",
    "        \n",
    "        # Define the step function for K.rnn\n",
    "        # This function runs for every time step of the Decoder\n",
    "        def _step(inputs, states):\n",
    "            # inputs: Single decoder step input\n",
    "            # states: List containing [prev_decoder_state, encoder_outputs]\n",
    "            \n",
    "            prev_decoder_state = states[0]\n",
    "            full_encoder_outputs = states[-1] # Accessed via constants\n",
    "\n",
    "            # --- Attention Score Calculation ---\n",
    "            \n",
    "            # 1. Transform Encoder Outputs: H * W_a \n",
    "            # Result shape: (batch, en_seq_len, units)\n",
    "            W_a_dot_h = K.dot(full_encoder_outputs, self.W_a)\n",
    "            \n",
    "            # 2. Transform Previous Decoder State: S_{t-1} * U_a\n",
    "            # Result shape: (batch, 1, units)\n",
    "            U_a_dot_s = K.expand_dims(K.dot(prev_decoder_state, self.U_a), 1)\n",
    "            \n",
    "            # 3. Tanh Activation: tanh(W_a.H + U_a.S)\n",
    "            # Broadcasting happens here to add (batch, 1, units) to (batch, seq, units)\n",
    "            Wa_plus_Ua = K.tanh(W_a_dot_h + U_a_dot_s)\n",
    "            \n",
    "            # 4. Calculate Score: V_a^T * tanh(...)\n",
    "            # Shape: (batch, en_seq_len, 1)\n",
    "            e_i = K.dot(Wa_plus_Ua, self.V_a)\n",
    "            # Squeeze to remove last dim -> (batch, en_seq_len)\n",
    "            e_i = K.squeeze(e_i, axis=-1) \n",
    "            \n",
    "            # 5. Softmax to get Attention Weights (alpha)\n",
    "            a_i = K.softmax(e_i)\n",
    "            \n",
    "            # --- Context Vector Calculation ---\n",
    "            # Weighted sum of encoder outputs\n",
    "            # context = sum(alpha * H)\n",
    "            c_i = K.sum(full_encoder_outputs * K.expand_dims(a_i, -1), axis=1)\n",
    "            \n",
    "            # --- GRU Step ---\n",
    "            # Concatenate Context Vector + Current Decoder Input\n",
    "            gru_input = K.concatenate([inputs, c_i], axis=-1)\n",
    "            \n",
    "            # Call the basic GRU cell\n",
    "            output, new_states = self._cell_fn(gru_input, states)\n",
    "            \n",
    "            # Return: (Output, Attention_Weights), [New_State]\n",
    "            return (output, a_i), new_states\n",
    "\n",
    "        # K.rnn loops the _step function over the time dimension of decoder_inputs\n",
    "        # constants=[encoder_outputs] ensures encoder_outputs is available in _step\n",
    "        _, outputs, _ = K.rnn(\n",
    "            step_function=_step,\n",
    "            inputs=decoder_inputs,\n",
    "            initial_states=[initial_state],\n",
    "            constants=[encoder_outputs]\n",
    "        )\n",
    "        \n",
    "        # Unpack outputs (GRU outputs, Attention Weights)\n",
    "        decoder_outputs, attention_weights = outputs\n",
    "        return decoder_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Code Explanation\n",
    "\n",
    "1.  **Input:** The layer receives `encoder_outputs` (the full history of the source sentence) and `decoder_inputs` (the target sentence offset by one).\n",
    "2.  **`K.rnn` Loop:** Unlike standard layers that process matrices, we need to iterate step-by-step because the attention for time $t$ depends on the state from $t-1$. `K.rnn` handles this unrolling efficiently.\n",
    "3.  **`_step` Function:** Inside the loop:\n",
    "    * We project the encoder states (`W_a`) and the current decoder state (`U_a`) into a shared latent space `units`.\n",
    "    * We apply `tanh` nonlinearity.\n",
    "    * We project down to a score using `V_a`.\n",
    "    * `K.softmax` converts scores into probabilities ($\\{ \\alpha_1, ..., \\alpha_T \\}$).\n",
    "    * We compute the **Context Vector** as the weighted sum of encoder states.\n",
    "    * We feed `[Decoder_Input, Context_Vector]` into the actual GRU cell.\n",
    "4.  **Output:** The layer returns the GRU outputs (for prediction) AND the attention weights (for visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Building the Attention-Based Seq2Seq Model\n",
    "\n",
    "Now we integrate our custom layer into the functional Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "LATENT_DIM = 256\n",
    "\n",
    "# --- Encoder (Same as Chapter 11) ---\n",
    "encoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "enc_emb = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "encoder_gru = layers.Bidirectional(layers.GRU(LATENT_DIM // 2, return_sequences=True, return_state=True))\n",
    "encoder_out, fwd_state, bwd_state = encoder_gru(enc_emb)\n",
    "\n",
    "# Concatenate forward and backward states for the initial decoder state\n",
    "encoder_state = layers.Concatenate()([fwd_state, bwd_state])\n",
    "\n",
    "# --- Decoder with Attention ---\n",
    "decoder_inputs = layers.Input(shape=(SEQUENCE_LENGTH,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "dec_emb_layer = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Define the Basic GRU Cell\n",
    "decoder_cell = layers.GRUCell(LATENT_DIM)\n",
    "\n",
    "# Wrap it with Attention\n",
    "# Note: We pass the encoder outputs AND decoder embeddings to the call\n",
    "attention_wrapper = DecoderRNNAttentionWrapper(cell_fn=decoder_cell, units=512)\n",
    "decoder_outputs, attention_weights = attention_wrapper(\n",
    "    [encoder_out, dec_emb],\n",
    "    initial_state=encoder_state\n",
    ")\n",
    "\n",
    "# Final Output Layer\n",
    "decoder_dense = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "final_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- Model Compilation ---\n",
    "attn_model = models.Model([encoder_inputs, decoder_inputs], final_outputs)\n",
    "\n",
    "attn_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "attn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Training the Model\n",
    "\n",
    "We train using Teacher Forcing (feeding the correct previous word to generate the next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = attn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5, # Reduced for demonstration speed\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Visualization: Peeking into the \"Brain\"\n",
    "\n",
    "The beauty of Attention is interpretability. We can extract the attention weights to see which English words the model focused on when generating a specific German word.\n",
    "\n",
    "To do this, we need a separate **Inference Model** that outputs the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define Inference Model for Visualization ---\n",
    "# This model takes inputs and returns: Predictions, Attention Weights\n",
    "vis_model = models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs], \n",
    "    outputs=[final_outputs, attention_weights]\n",
    ")\n",
    "\n",
    "def plot_attention(text_en, text_de_input):\n",
    "    # Preprocess\n",
    "    en_seq = en_vectorizer([text_en])\n",
    "    de_seq = de_vectorizer([text_de_input])[:, :-1] # Remove end token for input\n",
    "    \n",
    "    # Predict\n",
    "    preds, attn_weights = vis_model.predict([en_seq, de_seq])\n",
    "    \n",
    "    # attn_weights shape: (1, dec_len, enc_len)\n",
    "    # Squeeze batch dimension\n",
    "    attn_weights = attn_weights[0]\n",
    "    \n",
    "    # Get words for plotting\n",
    "    en_words = text_en.split()\n",
    "    de_words = text_de_input.split()[1:] # Skip 'sos'\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    # Slice the attention matrix to match sentence lengths\n",
    "    # (The model pads to 20, but we only want to visualize real words)\n",
    "    ax.matshow(attn_weights[:len(de_words), :len(en_words)], cmap='viridis')\n",
    "    \n",
    "    ax.set_xticklabels([''] + en_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + de_words)\n",
    "    \n",
    "    plt.xlabel('Source (English)')\n",
    "    plt.ylabel('Target (German)')\n",
    "    plt.show()\n",
    "\n",
    "# Example Visualization\n",
    "# Ideally, we generate the translation first, but for visualization here\n",
    "# we will force feed a known pair to see alignment.\n",
    "sample_en = \"i am very happy\"\n",
    "sample_de = \"sos ich bin sehr glücklich\"\n",
    "\n",
    "plot_attention(sample_en, sample_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Plot\n",
    "\n",
    "If the model learned correctly, you should see a diagonal alignment:\n",
    "* \"ich\" (I) should attend to \"i\".\n",
    "* \"bin\" (am) should attend to \"am\".\n",
    "* \"sehr\" (very) should attend to \"very\".\n",
    "\n",
    "In complex translations where word order flips (e.g., German verbs moving to the end), the attention mechanism allows the model to jump to the correct encoder position effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Chapter Summary\n",
    "\n",
    "* **Limitations of Fixed Context:** Standard Seq2Seq models struggle with long sentences because they compress everything into one vector.\n",
    "* **Attention Mechanism:** Allows the Decoder to create a dynamic context vector at every time step by taking a weighted sum of Encoder states.\n",
    "* **Implementation:** We built a custom `DecoderRNNAttentionWrapper` because `K.rnn` allows us to define custom step logic involving alignment score calculation.\n",
    "* **Alignment:** The attention weights $\\alpha_{tj}$ represent the alignment between the target word at step $t$ and source word at step $j$.\n",
    "* **Result:** Attention not only improves translation accuracy (BLEU scores) but makes the model interpretable via heatmaps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
