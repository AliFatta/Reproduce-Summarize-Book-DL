{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Transformers\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "This chapter focuses on the Transformer architecture and its impact on modern Natural Language Processing (NLP). While earlier chapters introduced recurrent-based sequence models and attention mechanisms, this chapter highlights how Transformers replace recurrence entirely with self-attention.\n",
    "\n",
    "Rather than building Transformers from scratch, the emphasis here is on **leveraging pretrained Transformer models**, particularly **BERT (Bidirectional Encoder Representations from Transformers)** and **DistilBERT**, to solve downstream NLP tasks efficiently using transfer learning.\n",
    "\n",
    "Two major applications are explored:\n",
    "1. **Text Classification**, demonstrated through SMS spam detection using a pretrained BERT model from TensorFlow Hub.\n",
    "2. **Extractive Question Answering (QA)**, implemented using DistilBERT via the Hugging Face Transformers library and trained on the SQuAD dataset.\n",
    "\n",
    "The chapter demonstrates how pretrained Transformer encoders can be adapted to different NLP tasks with minimal task-specific layers while achieving strong generalization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 Transfer Learning in NLP\n",
    "\n",
    "Transfer learning in NLP is based on the observation that language understanding can be learned from large unlabeled corpora and reused across tasks. Transformer models are pretrained on massive datasets such as Wikipedia and BooksCorpus using self-supervised objectives.\n",
    "\n",
    "Formally, pretraining optimizes a general objective:\n",
    "\n",
    "$$ \\mathcal{L}_{pretrain} = \\mathbb{E}_{x \\sim D}[\\ell_{pretrain}(x)] $$\n",
    "\n",
    "For a downstream task, the pretrained parameters are fine-tuned by minimizing a task-specific loss:\n",
    "\n",
    "$$ \\mathcal{L}_{task} = \\mathbb{E}_{(x,y) \\sim D_{task}}[\\ell_{task}(f_\\theta(x), y)] $$\n",
    "\n",
    "This paradigm drastically reduces labeled data requirements and training time while maintaining strong performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transformer Encoder Architecture\n",
    "\n",
    "BERT is composed exclusively of the **encoder stack** of the original Transformer architecture. Unlike recurrent models, Transformers process the entire sequence in parallel, enabling efficient training and modeling of long-range dependencies.\n",
    "\n",
    "Each encoder layer consists of:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Position-wise Feedforward Networks\n",
    "3. Residual Connections and Layer Normalization\n",
    "\n",
    "The core operation is scaled dot-product attention:\n",
    "\n",
    "$$ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "This mechanism allows each token to attend to all other tokens, producing contextualized representations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 BERT Pretraining Objectives\n",
    "\n",
    "BERT introduces two self-supervised pretraining objectives that differ fundamentally from traditional left-to-right language modeling.\n",
    "\n",
    "**Masked Language Modeling (MLM):**\n",
    "A random subset (15%) of input tokens is masked, and the model predicts the original tokens using both left and right context:\n",
    "\n",
    "$$ \\mathcal{L}_{MLM} = - \\sum \\log P(x_i | X_{masked}) $$\n",
    "\n",
    "**Next Sentence Prediction (NSP):**\n",
    "Given two sentences A and B, the model predicts whether B follows A in the original corpus. This objective enables the model to learn sentence-level coherence.\n",
    "\n",
    "Together, MLM and NSP allow BERT to learn rich token-level and sentence-level representations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Input Representation and Tokenization\n",
    "\n",
    "BERT constructs input embeddings by summing three components:\n",
    "\n",
    "1. Token Embeddings derived from WordPiece tokenization\n",
    "2. Segment Embeddings distinguishing sentence A and sentence B\n",
    "3. Position Embeddings encoding token order\n",
    "\n",
    "Special tokens play a central role:\n",
    "* **[CLS]** represents the entire sequence and is used for classification tasks\n",
    "* **[SEP]** separates sentences\n",
    "\n",
    "The final embedding for token $i$ is:\n",
    "\n",
    "$$ E_i = E_{token} + E_{segment} + E_{position} $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 DistilBERT and Knowledge Distillation\n",
    "\n",
    "DistilBERT is a compressed version of BERT trained using **knowledge distillation**, where a smaller student model learns to approximate the behavior of a larger teacher model.\n",
    "\n",
    "The distillation objective combines multiple losses:\n",
    "\n",
    "$$ \\mathcal{L}_{distill} = \\alpha \\mathcal{L}_{MLM} + \\beta \\mathcal{L}_{KD} + \\gamma \\mathcal{L}_{cos} $$\n",
    "\n",
    "Despite using fewer layers, DistilBERT retains most of BERT’s performance while being significantly faster and more memory-efficient.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-based Applications\n",
    "\n",
    "### 3.1 Text Classification with BERT\n",
    "\n",
    "For text classification tasks such as spam detection, BERT is used as a feature extractor. The final hidden state of the [CLS] token is passed to a classification head:\n",
    "\n",
    "$$ \\hat{y} = \\text{softmax}(W h_{CLS} + b) $$\n",
    "\n",
    "Only a small number of task-specific parameters are trained, while the pretrained encoder provides rich linguistic representations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extractive Question Answering\n",
    "\n",
    "In extractive Question Answering, the model does not generate text. Instead, it predicts the start and end positions of the answer span within a given context.\n",
    "\n",
    "The model outputs two probability distributions:\n",
    "\n",
    "* Start logits: $P(start = i)$\n",
    "* End logits: $P(end = j)$\n",
    "\n",
    "The predicted answer span is:\n",
    "\n",
    "$$ (i^*, j^*) = \\arg\\max_{i \\le j} P(start=i) P(end=j) $$\n",
    "\n",
    "This formulation enables precise extraction of answers from unstructured text.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup\n",
    "\n",
    "This chapter requires specific libraries. We need `tensorflow-text` for BERT preprocessing and `transformers` + `datasets` for the Hugging Face section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-text tensorflow-hub transformers datasets imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Required for BERT preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Part 1: Spam Classification with BERT\n",
    "\n",
    "We will build a binary classifier to detect Spam SMS messages. We will use a pretrained BERT encoder from TensorFlow Hub and attach a simple classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download Data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"smsspamcollection.zip\", origin=url, extract=True)\n",
    "data_path = os.path.join(os.path.dirname(zip_path), \"SMSSpamCollection\")\n",
    "\n",
    "# 2. Load Data\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            label_str, text = parts\n",
    "            inputs.append(text)\n",
    "            labels.append(1 if label_str == 'spam' else 0)\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total samples: {len(inputs)}\")\n",
    "print(f\"Spam count: {sum(labels)}\")\n",
    "print(f\"Ham count: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Handling Class Imbalance\n",
    "Spam datasets are heavily imbalanced. We will use `RandomUnderSampler` to balance the dataset by reducing the number of Ham (non-spam) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Reshape inputs for sampling\n",
    "inputs_reshaped = inputs.reshape(-1, 1)\n",
    "\n",
    "# Undersample majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "inputs_res, labels_res = rus.fit_resample(inputs_reshaped, labels)\n",
    "\n",
    "inputs_res = inputs_res.flatten()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    inputs_res, labels_res, test_size=0.2, random_state=42, stratify=labels_res\n",
    ")\n",
    "\n",
    "print(f\"Balanced Train Size: {len(X_train)}\")\n",
    "print(f\"Balanced Test Size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the BERT Model\n",
    "We use TF Hub to load:\n",
    "1.  **BERT Preprocessor:** Handles tokenization and packing inputs (creating `input_word_ids`, `input_mask`, `input_type_ids`).\n",
    "2.  **BERT Encoder:** The actual model that outputs pooled and sequence representations.\n",
    "\n",
    "We select the \"Small BERT\" to keep training fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Hub URLs for BERT Preprocessor and Encoder\n",
    "# Using Small BERT for speed\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    # 1. Preprocessing Layer\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    # 2. Encoder Layer (BERT)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    # 3. Classification Head\n",
    "    # We use the 'pooled_output' which represents the [CLS] token embedding\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "We use `BinaryCrossentropy(from_logits=True)` because our final layer has no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "# Using AdamW optimizer is standard for BERT, but standard Adam works for this demo\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history = classifier_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Inference\n",
    "Let's test the model on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Reply to this message to win a free vacation!\",\n",
    "    \"Hey man, are we still meeting for lunch?\",\n",
    "    \"Urgent! Your bank account has been compromised. Click here.\"\n",
    "]\n",
    "\n",
    "results = classifier_model.predict(examples)\n",
    "results = tf.sigmoid(results).numpy()\n",
    "\n",
    "for text, score in zip(examples, results):\n",
    "    label = \"SPAM\" if score > 0.5 else \"HAM\"\n",
    "    print(f\"'{text}' -> {label} ({score[0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Part 2: Question Answering with Hugging Face\n",
    "\n",
    "We will now use the **Hugging Face** ecosystem to build a Question Answering system using **DistilBERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, TFDistilBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load SQuAD Dataset\n",
    "# SQuAD (Stanford Question Answering Dataset) contains Context-Question-Answer triplets\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample = dataset['train'][0]\n",
    "print(\"Context:\", sample['context'])\n",
    "print(\"Question:\", sample['question'])\n",
    "print(\"Answer:\", sample['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preprocessing (The Tricky Part)\n",
    "The dataset provides the **character start index** of the answer. However, the model works with **token indices**. We need to map character indices to token indices.\n",
    "\n",
    "We use `DistilBertTokenizerFast` because it provides methods to map between chars and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    # truncation=\"only_second\" truncates the context (second seq) if it's too long, not the question\n",
    "    encodings = tokenizer(examples[\"question\"], examples[\"context\"], truncation=\"only_second\", max_length=384, stride=128, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    \n",
    "    # Note: Complex mapping logic is usually required here to handle answers that span across split windows.\n",
    "    # For simplicity in this demo, we will use a simplified preprocessing strategy focusing on the first answer.\n",
    "    \n",
    "    # Simplified for educational clarity:\n",
    "    # We need to calculate start_positions and end_positions (token indices)\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Iterate over batch\n",
    "    for i, offsets in enumerate(encodings.pop(\"offset_mapping\")):\n",
    "        # Find the index of the original example this feature belongs to\n",
    "        sample_index = encodings[\"overflow_to_sample_mapping\"][i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        # If no answer, set indices to 0 (CLS token)\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Start/End character index\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Find sequence tokens (where context tokens are)\n",
    "            sequence_ids = encodings.sequence_ids(i)\n",
    "            \n",
    "            # Find the start and end of the context in tokens\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # Check if answer is fully inside the context span\n",
    "            if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Map char to token\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offsets[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offsets[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "    encodings[\"start_positions\"] = start_positions\n",
    "    encodings[\"end_positions\"] = end_positions\n",
    "    return encodings\n",
    "\n",
    "# Apply preprocessing (Subset for speed)\n",
    "tokenized_squad = dataset['train'].select(range(1000)).map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_val = dataset['validation'].select(range(100)).map(preprocess_function, batched=True, remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Convert to TensorFlow Dataset\n",
    "Hugging Face datasets can be easily converted to `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = tokenized_squad.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_val_set = tokenized_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Training\n",
    "We load `TFDistilBertForQuestionAnswering`. This model has the DistilBERT backbone plus a span classification head (two outputs: start logits and end logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(tf_train_set, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Asking BERT a Question\n",
    "Let's do inference manually to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, context):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the token index with the highest score for start and end\n",
    "    answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "    answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "    # Convert tokens back to string\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "context = \"\"\"\n",
    "Transformers are a type of deep learning model introduced in 2017. \n",
    "They are primarily used in natural language processing tasks. \n",
    "BERT and GPT are famous examples of Transformers.\n",
    "\"\"\"\n",
    "\n",
    "q1 = \"When were Transformers introduced?\"\n",
    "q2 = \"What fields are they used in?\"\n",
    "\n",
    "print(f\"Q: {q1}\\nA: {ask_question(q1, context)}\\n\")\n",
    "print(f\"Q: {q2}\\nA: {ask_question(q2, context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Chapter Summary\n",
    "\n",
    "* **BERT** revolutionized NLP by allowing Transfer Learning. We can take a generic pretrained model and fine-tune it on small datasets (like our 5,000 spam SMS dataset) to get high performance.\n",
    "* **Architecture:** BERT uses the Encoder stack. It requires specific inputs: Token IDs, Mask, and Segment IDs. It outputs a sequence of vectors and a pooled vector (used for classification).\n",
    "* **Hugging Face:** The `transformers` library abstracts away much of the complexity of loading models and tokenizers. The `datasets` library handles downloading and metrics.\n",
    "* **Question Answering:** This is a span prediction task. The model predicts the start and end tokens of the answer within the context text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
