{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Transformers\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In Chapter 5, we introduced the Transformer architecture. In this chapter, we dive deeper into the ecosystem that Transformers have created. We move beyond building them from scratch to leveraging **Pretrained Models**, specifically **BERT** (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "We will cover two major practical applications:\n",
    "1.  **Spam Classification:** Using a pretrained **BERT** model from TensorFlow Hub to classify SMS messages.\n",
    "2.  **Question Answering (QA):** Using **DistilBERT** (a lighter version of BERT) via the **Hugging Face Transformers** library to answer questions based on a context paragraph (SQuAD dataset).\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **Transfer Learning in NLP:** Using models pretrained on massive corpora (like Wikipedia) for downstream tasks.\n",
    "* **BERT Architecture:** Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
    "* **Tokenization:** WordPiece tokenization and special tokens (`[CLS]`, `[SEP]`).\n",
    "* **Span Prediction:** How QA models predict start and end indices of an answer.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Using **TensorFlow Hub** to load pretrained models.\n",
    "* Using **Hugging Face Transformers** and **Datasets** libraries.\n",
    "* Handling class imbalance with undersampling techniques.\n",
    "* Preprocessing data for QA tasks (aligning character indices to token indices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "BERT is essentially the **Encoder** stack of the original Transformer architecture, but deeper and trained on massive amounts of text. \n",
    "\n",
    "**Pretraining Tasks:**\n",
    "Instead of predicting the next word (like standard Language Modeling), BERT uses two tasks:\n",
    "1.  **Masked Language Modeling (MLM):** Randomly mask 15% of tokens in the input and ask the model to predict them based on the context from *both* left and right directions.\n",
    "2.  **Next Sentence Prediction (NSP):** Given two sentences A and B, predict if B naturally follows A.\n",
    "\n",
    "**Embeddings:**\n",
    "BERT combines three types of embeddings:\n",
    "1.  **Token Embeddings:** The ID of the word/sub-word.\n",
    "2.  **Segment Embeddings:** Distinguishes between Sentence A and Sentence B.\n",
    "3.  **Position Embeddings:** Learned vectors indicating the position of tokens.\n",
    "\n",
    "### 2.2 DistilBERT\n",
    "DistilBERT is a smaller, faster, cheaper, and lighter version of BERT. It is trained using **Knowledge Distillation**, where a small student model (DistilBERT) is trained to reproduce the behavior of a large teacher model (BERT). It retains 97% of BERT's performance but is 40% smaller and 60% faster.\n",
    "\n",
    "### 2.3 Question Answering (Span Prediction)\n",
    "In Extractive QA, the model does not generate text. Instead, given a Context (Paragraph) and a Question, it predicts:\n",
    "* **Start Logits:** The probability of each token being the *start* of the answer.\n",
    "* **End Logits:** The probability of each token being the *end* of the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup\n",
    "\n",
    "This chapter requires specific libraries. We need `tensorflow-text` for BERT preprocessing and `transformers` + `datasets` for the Hugging Face section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-text tensorflow-hub transformers datasets imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Required for BERT preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Part 1: Spam Classification with BERT\n",
    "\n",
    "We will build a binary classifier to detect Spam SMS messages. We will use a pretrained BERT encoder from TensorFlow Hub and attach a simple classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download Data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "zip_path = tf.keras.utils.get_file(\"smsspamcollection.zip\", origin=url, extract=True)\n",
    "data_path = os.path.join(os.path.dirname(zip_path), \"SMSSpamCollection\")\n",
    "\n",
    "# 2. Load Data\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            label_str, text = parts\n",
    "            inputs.append(text)\n",
    "            labels.append(1 if label_str == 'spam' else 0)\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total samples: {len(inputs)}\")\n",
    "print(f\"Spam count: {sum(labels)}\")\n",
    "print(f\"Ham count: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Handling Class Imbalance\n",
    "Spam datasets are heavily imbalanced. We will use `RandomUnderSampler` to balance the dataset by reducing the number of Ham (non-spam) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Reshape inputs for sampling\n",
    "inputs_reshaped = inputs.reshape(-1, 1)\n",
    "\n",
    "# Undersample majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "inputs_res, labels_res = rus.fit_resample(inputs_reshaped, labels)\n",
    "\n",
    "inputs_res = inputs_res.flatten()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    inputs_res, labels_res, test_size=0.2, random_state=42, stratify=labels_res\n",
    ")\n",
    "\n",
    "print(f\"Balanced Train Size: {len(X_train)}\")\n",
    "print(f\"Balanced Test Size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the BERT Model\n",
    "We use TF Hub to load:\n",
    "1.  **BERT Preprocessor:** Handles tokenization and packing inputs (creating `input_word_ids`, `input_mask`, `input_type_ids`).\n",
    "2.  **BERT Encoder:** The actual model that outputs pooled and sequence representations.\n",
    "\n",
    "We select the \"Small BERT\" to keep training fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Hub URLs for BERT Preprocessor and Encoder\n",
    "# Using Small BERT for speed\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    # 1. Preprocessing Layer\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    # 2. Encoder Layer (BERT)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    # 3. Classification Head\n",
    "    # We use the 'pooled_output' which represents the [CLS] token embedding\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "We use `BinaryCrossentropy(from_logits=True)` because our final layer has no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "# Using AdamW optimizer is standard for BERT, but standard Adam works for this demo\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history = classifier_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Inference\n",
    "Let's test the model on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Reply to this message to win a free vacation!\",\n",
    "    \"Hey man, are we still meeting for lunch?\",\n",
    "    \"Urgent! Your bank account has been compromised. Click here.\"\n",
    "]\n",
    "\n",
    "results = classifier_model.predict(examples)\n",
    "results = tf.sigmoid(results).numpy()\n",
    "\n",
    "for text, score in zip(examples, results):\n",
    "    label = \"SPAM\" if score > 0.5 else \"HAM\"\n",
    "    print(f\"'{text}' -> {label} ({score[0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Part 2: Question Answering with Hugging Face\n",
    "\n",
    "We will now use the **Hugging Face** ecosystem to build a Question Answering system using **DistilBERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, TFDistilBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load SQuAD Dataset\n",
    "# SQuAD (Stanford Question Answering Dataset) contains Context-Question-Answer triplets\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample = dataset['train'][0]\n",
    "print(\"Context:\", sample['context'])\n",
    "print(\"Question:\", sample['question'])\n",
    "print(\"Answer:\", sample['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preprocessing (The Tricky Part)\n",
    "The dataset provides the **character start index** of the answer. However, the model works with **token indices**. We need to map character indices to token indices.\n",
    "\n",
    "We use `DistilBertTokenizerFast` because it provides methods to map between chars and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    # truncation=\"only_second\" truncates the context (second seq) if it's too long, not the question\n",
    "    encodings = tokenizer(examples[\"question\"], examples[\"context\"], truncation=\"only_second\", max_length=384, stride=128, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    \n",
    "    # Note: Complex mapping logic is usually required here to handle answers that span across split windows.\n",
    "    # For simplicity in this demo, we will use a simplified preprocessing strategy focusing on the first answer.\n",
    "    \n",
    "    # Simplified for educational clarity:\n",
    "    # We need to calculate start_positions and end_positions (token indices)\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Iterate over batch\n",
    "    for i, offsets in enumerate(encodings.pop(\"offset_mapping\")):\n",
    "        # Find the index of the original example this feature belongs to\n",
    "        sample_index = encodings[\"overflow_to_sample_mapping\"][i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        # If no answer, set indices to 0 (CLS token)\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Start/End character index\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Find sequence tokens (where context tokens are)\n",
    "            sequence_ids = encodings.sequence_ids(i)\n",
    "            \n",
    "            # Find the start and end of the context in tokens\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # Check if answer is fully inside the context span\n",
    "            if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Map char to token\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offsets[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offsets[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "    encodings[\"start_positions\"] = start_positions\n",
    "    encodings[\"end_positions\"] = end_positions\n",
    "    return encodings\n",
    "\n",
    "# Apply preprocessing (Subset for speed)\n",
    "tokenized_squad = dataset['train'].select(range(1000)).map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_val = dataset['validation'].select(range(100)).map(preprocess_function, batched=True, remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Convert to TensorFlow Dataset\n",
    "Hugging Face datasets can be easily converted to `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = tokenized_squad.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_val_set = tokenized_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Training\n",
    "We load `TFDistilBertForQuestionAnswering`. This model has the DistilBERT backbone plus a span classification head (two outputs: start logits and end logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(tf_train_set, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Asking BERT a Question\n",
    "Let's do inference manually to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, context):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the token index with the highest score for start and end\n",
    "    answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "    answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "    # Convert tokens back to string\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "context = \"\"\"\n",
    "Transformers are a type of deep learning model introduced in 2017. \n",
    "They are primarily used in natural language processing tasks. \n",
    "BERT and GPT are famous examples of Transformers.\n",
    "\"\"\"\n",
    "\n",
    "q1 = \"When were Transformers introduced?\"\n",
    "q2 = \"What fields are they used in?\"\n",
    "\n",
    "print(f\"Q: {q1}\\nA: {ask_question(q1, context)}\\n\")\n",
    "print(f\"Q: {q2}\\nA: {ask_question(q2, context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Chapter Summary\n",
    "\n",
    "* **BERT** revolutionized NLP by allowing Transfer Learning. We can take a generic pretrained model and fine-tune it on small datasets (like our 5,000 spam SMS dataset) to get high performance.\n",
    "* **Architecture:** BERT uses the Encoder stack. It requires specific inputs: Token IDs, Mask, and Segment IDs. It outputs a sequence of vectors and a pooled vector (used for classification).\n",
    "* **Hugging Face:** The `transformers` library abstracts away much of the complexity of loading models and tokenizers. The `datasets` library handles downloading and metrics.\n",
    "* **Question Answering:** This is a span prediction task. The model predicts the start and end tokens of the answer within the context text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
