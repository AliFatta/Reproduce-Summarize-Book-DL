{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Computer Vision & Model Interpretability\n",
    "\n",
    "## 1️⃣ Overview\n",
    "\n",
    "In **Chapter 7**, we trained a powerful image classifier using Transfer Learning with **InceptionResNetV2**. While high accuracy is great, in real-world applications (like medical imaging or autonomous driving), we need to know *why* the model made a specific decision.\n",
    "\n",
    "This appendix provides the technical implementation for **Grad-CAM**, a technique that highlights the regions of an image that influenced the model's prediction the most.\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Nested Models:** Keras `Applications` often wrap models in a single layer. We'll learn how to \"unwrap\" them to access internal convolutional layers.\n",
    "* **Gradient Tape:** Using TensorFlow's automatic differentiation to compute gradients of the top class score with respect to the feature maps.\n",
    "* **Heatmap Visualization:** overlaying the activation map on the original image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Refresher: Grad-CAM\n",
    "\n",
    "**Grad-CAM** works by answering this question: *\"How much would the prediction score change if I changed the values in this specific feature map?\"*\n",
    "\n",
    "1.  **Forward Pass:** Run the image through the model to get the prediction.\n",
    "2.  **Backward Pass:** Calculate the gradient of the predicted class score $y^c$ with respect to the feature maps $A^k$ of the last convolutional layer.\n",
    "3.  **Global Average Pooling:** Average these gradients to get a weight $\\alpha_k$ for each feature map. This tells us \"how important\" that specific map is.\n",
    "4.  **Weighted Combination:** Combine the feature maps using these weights to get a single 2D heatmap.\n",
    "    $$ L_{Grad-CAM}^c = ReLU(\\sum_k \\alpha_k A^k) $$\n",
    "\n",
    "We use **ReLU** because we are only interested in features that have a *positive* influence on the class of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Model Loading\n",
    "\n",
    "We will recreate the **InceptionResNetV2** transfer learning model used in Chapter 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# 1. Recreate the Model from Chapter 7\n",
    "def get_model():\n",
    "    # Define the base model (Pretrained)\n",
    "    base_model = InceptionResNetV2(include_top=False, pooling='avg', weights='imagenet', input_shape=(299, 299, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Define the wrapper model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(200, activation='softmax') # Assuming 200 classes like TinyImageNet\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Problem: Nested Models\n",
    "\n",
    "Look at the summary above. The entire InceptionResNetV2 architecture is hidden inside a single layer named `inception_resnet_v2`. \n",
    "\n",
    "To perform Grad-CAM, we need access to the **last convolutional layer** (usually named `conv_7b` in InceptionResNetV2). We cannot access it directly because it is buried inside the nested model.\n",
    "\n",
    "**Solution:** We must \"unwrap\" the model by creating a new Functional API model that explicitly connects the internal layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_model(nested_model):\n",
    "    # 1. Access the inner base model\n",
    "    inception = nested_model.get_layer('inception_resnet_v2')\n",
    "    \n",
    "    # 2. Get the input of the inner model\n",
    "    inp = inception.input\n",
    "    \n",
    "    # 3. Re-connect the outputs\n",
    "    # We pass the inception output to the subsequent layers of the outer model\n",
    "    x = inception.output\n",
    "    x = nested_model.get_layer('dropout')(x)\n",
    "    out = nested_model.get_layer('dense')(x)\n",
    "    \n",
    "    # 4. Create a new flat model\n",
    "    return Model(inputs=inp, outputs=out)\n",
    "\n",
    "unwrapped_model = unwrap_model(model)\n",
    "\n",
    "# Now we can see all the internal layers!\n",
    "# Note: The summary will be huge, so we check just the last few layers\n",
    "# unwrapped_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Implementing Grad-CAM\n",
    "\n",
    "We need a special sub-model that outputs two things:\n",
    "1.  The activations of the last convolutional layer (`conv_7b`).\n",
    "2.  The final predictions of the model.\n",
    "\n",
    "We will compute the gradients of (2) with respect to (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # 1. Create a model that maps the input image to the activations\n",
    "    #    of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # 2. Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        \n",
    "        # If no specific class index is provided, use the predicted class\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # 3. Compute Gradients of the predicted class w.r.t the feature map\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # 4. Pool the gradients (Global Average Pooling)\n",
    "    # This gives us a vector of weights (one for each filter in the conv layer)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # 5. Multiply each channel in the feature map by its weight\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    \n",
    "    # Matrix multiplication: (H, W, Channels) @ (Channels, 1) -> (H, W, 1)\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # 6. Apply ReLU (we only care about features that have a positive influence)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    \n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Visualization\n",
    "\n",
    "We will generate a dummy image (since we don't have the dataset loaded) and visualize the heatmap. In a real scenario, you would pass a real photo of a cat or dog.\n",
    "\n",
    "The `conv_7b` layer in InceptionResNetV2 usually produces an $8 \\times 8$ output grid. The heatmap will be this size, so we must resize it to overlay it on the original $299 \\times 299$ image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy image\n",
    "# InceptionResNetV2 expects values in [-1, 1]\n",
    "img_size = (299, 299)\n",
    "dummy_img = np.random.rand(1, *img_size, 3).astype('float32') * 2 - 1 \n",
    "\n",
    "# Specify the layer name for InceptionResNetV2\n",
    "last_conv_layer_name = \"conv_7b\"\n",
    "\n",
    "# Generate Heatmap\n",
    "heatmap = make_gradcam_heatmap(dummy_img, unwrapped_model, last_conv_layer_name)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow((dummy_img[0] + 1) / 2) # Rescale back to [0, 1] for display\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.matshow(heatmap, fignum=0)\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Superimposing the Heatmap\n",
    "\n",
    "To make the result interpretable, we merge the heatmap with the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose_heatmap(img, heatmap, alpha=0.4):\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap\n",
    "    jet = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Resize heatmap to match image size\n",
    "    jet = cv2.resize(jet, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Superimpose\n",
    "    # Convert img back to [0, 255] if it was preprocessed\n",
    "    img_uint8 = np.uint8((img + 1) / 2 * 255)\n",
    "    \n",
    "    superimposed_img = jet * alpha + img_uint8\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n",
    "\n",
    "    return superimposed_img\n",
    "\n",
    "final_img = superimpose_heatmap(dummy_img[0], heatmap)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(final_img)\n",
    "plt.title(\"Superimposed Grad-CAM\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Summary\n",
    "\n",
    "* **Model Unwrapping:** When using Keras `Sequential` models containing other models (like `InceptionResNetV2`), we must explicitly reconstruct the graph to access internal layers.\n",
    "* **Feature Importance:** Grad-CAM uses the gradients flowing into the final convolutional layer to determine which filters are most active for a specific class.\n",
    "* **Localization:** By upsampling the coarse $8 \\times 8$ heatmap to the image size ($299 \\times 299$), we can roughly localize the object that triggered the classification, effectively turning a classifier into a weak object detector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
