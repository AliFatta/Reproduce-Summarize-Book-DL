{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Telling Things Apart - Image Segmentation\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we focused on **Classification** (assigning a single label to an entire image). In this chapter, we tackle a much more granular task: **Semantic Segmentation**.\n",
    "\n",
    "Semantic Segmentation involves classifying **every single pixel** in an image. Instead of saying \"This image contains a cat\", we say \"Pixels (x,y) to (x+n, y+m) belong to the cat, and the rest belong to the background.\"\n",
    "\n",
    "We will implement the legendary **U-Net** architecture, a model originally designed for biomedical image segmentation that has become the gold standard for general segmentation tasks due to its efficiency and precision.\n",
    "\n",
    "### Key Concepts:\n",
    "* **Semantic Segmentation:** Pixel-level classification.\n",
    "* **Upsampling:** How to increase the spatial resolution of feature maps (Transposed Convolutions).\n",
    "* **U-Net Architecture:** A symmetric Encoder-Decoder network with skip connections.\n",
    "* **IOU (Intersection over Union):** The standard metric for evaluating segmentation accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 Classification vs. Segmentation\n",
    "* **Classification:** Output is a vector of probabilities (e.g., `[0.1, 0.9, 0.0]`). Spatial information is lost.\n",
    "* **Segmentation:** Output is a **mask** of the same size as the input image (e.g., $128 \\times 128 \\times K$ classes).\n",
    "\n",
    "### 2.2 The Challenge: Resolution\n",
    "Standard CNNs (like VGG or ResNet) use Max Pooling to reduce image size and increase the receptive field. \n",
    "* Input: $256 \\times 256$\n",
    "* Bottleneck: $8 \\times 8$\n",
    "\n",
    "For segmentation, we need an output that is $256 \\times 256$. How do we get back up from $8 \\times 8$? \n",
    "We need **Upsampling**.\n",
    "\n",
    "### 2.3 Upsampling Techniques\n",
    "1.  **Nearest Neighbor / Bilinear Interpolation:** Simple mathematical resizing. No learning involved.\n",
    "2.  **Transposed Convolution (Conv2DTranspose):** Often called \"Deconvolution\". It uses learnable filters to expand the image, learning *how* to fill in the details.\n",
    "\n",
    "### 2.4 The U-Net Architecture\n",
    "U-Net consists of two paths:\n",
    "1.  **Contracting Path (Encoder):** A standard CNN that captures context but reduces resolution.\n",
    "2.  **Expanding Path (Decoder):** Uses Transposed Convolutions to restore resolution.\n",
    "\n",
    "**The Secret Sauce: Skip Connections.** \n",
    "Upsampling from a low-resolution bottleneck is hard; fine details (like the whiskers of a cat) are lost. U-Net concatenates the high-resolution feature maps from the Encoder directly to the corresponding layers in the Decoder. This gives the Decoder a \"template\" of fine details to paint over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Setup and Data Loading\n",
    "\n",
    "We will use the **Oxford-IIIT Pet Dataset**, a standard benchmark for segmentation. It contains images of pets and their pixel-wise masks (1: Pet, 2: Background, 3: Border).\n",
    "\n",
    "We will map these to 3 classes:\n",
    "* Class 0: Pet\n",
    "* Class 1: Background\n",
    "* Class 2: Border (Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 1. Download Dataset\n",
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "\n",
    "# 2. Image Processing Functions\n",
    "def normalize(input_image, input_mask):\n",
    "    # Normalize image to [0, 1]\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    # Masks are 1, 2, 3 in dataset. We shift to 0, 1, 2 for training\n",
    "    input_mask -= 1 \n",
    "    return input_image, input_mask\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    return input_image, input_mask\n",
    "\n",
    "# 3. Build Pipeline\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_batches = train_images.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_batches = test_images.batch(BATCH_SIZE)\n",
    "\n",
    "print(\"Data loaded. Input Image Shape: (128, 128, 3). Mask Shape: (128, 128, 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualizing the Data\n",
    "Let's look at an image and its corresponding ground truth mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        # Use standard coloring for image, grayscale for mask\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for image, mask in train_batches.take(1):\n",
    "    sample_image, sample_mask = image[0], mask[0]\n",
    "    display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Building the U-Net Architecture\n",
    "\n",
    "We will build the U-Net manually to understand the flow. \n",
    "* **Encoder:** `Conv2D` -> `MaxPool`.\n",
    "* **Decoder:** `Conv2DTranspose` -> `Concatenate` (Skip Connection) -> `Conv2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(output_channels: int):\n",
    "    inputs = Input(shape=[128, 128, 3])\n",
    "    \n",
    "    # --- Encoder (Downsampling) ---\n",
    "    # We save the output of each block to use in the skip connections later\n",
    "    \n",
    "    # Block 1\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    skip1 = x \n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    skip2 = x\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    skip3 = x\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    # --- Bottleneck ---\n",
    "    x = Conv2D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # --- Decoder (Upsampling) ---\n",
    "    \n",
    "    # Block 3 (Up)\n",
    "    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "    x = Concatenate()([x, skip3]) # SKIP CONNECTION\n",
    "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Block 2 (Up)\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "    x = Concatenate()([x, skip2]) # SKIP CONNECTION\n",
    "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Block 1 (Up)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "    x = Concatenate()([x, skip1]) # SKIP CONNECTION\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    # --- Output Layer ---\n",
    "    # Output has 'output_channels' filters (3 classes here)\n",
    "    outputs = Conv2D(output_channels, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = unet_model(output_channels=3)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Note: The summary is huge, so we just print the parameter count\n",
    "print(f\"Model created with {model.count_params():,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Training and Prediction\n",
    "\n",
    "We train the model. Since segmentation is computationally expensive, we will run for a few epochs to demonstrate convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to show predictions during training\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Clear output to keep notebook clean (optional, omitted here for safety)\n",
    "        # display([sample_image, sample_mask, create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "        pass\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    # pred_mask shape: (1, 128, 128, 3)\n",
    "    # We take argmax across the last axis (channels) to get the class index (0, 1, 2)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "EPOCHS = 5\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples // BATCH_SIZE // VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_batches, \n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Evaluation: Making Predictions\n",
    "\n",
    "Let's visualize the results. We will take images from the test set, run them through the U-Net, and compare the predicted mask with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        # Show prediction for the sample image loaded earlier\n",
    "        pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
    "        display([sample_image, sample_mask, create_mask(pred_mask)])\n",
    "\n",
    "# Show predictions on Test Data\n",
    "show_predictions(test_batches, num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Metric: Intersection over Union (IoU)\n",
    "\n",
    "Accuracy can be misleading in segmentation (if 90% of the image is background, predicting \"all background\" gives 90% accuracy but is useless). \n",
    "\n",
    "**IoU** measures the overlap between the predicted mask and the true mask.\n",
    "$$IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, name=None, dtype=None):\n",
    "        super(MeanIoU, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "# Recompile to include IoU\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', MeanIoU(num_classes=3)])\n",
    "\n",
    "# Evaluate on a small batch\n",
    "results = model.evaluate(test_batches, steps=5)\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "print(f\"Test Mean IoU: {results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Chapter Summary\n",
    "\n",
    "In this chapter, we performed **Semantic Segmentation** on the Oxford-IIIT Pet Dataset.\n",
    "\n",
    "* **Architecture:** We built **U-Net**, the industry standard for segmentation. We learned how it uses an Encoder to capture context and a Decoder with **Skip Connections** to recover precise spatial details.\n",
    "* **Upsampling:** We used `Conv2DTranspose` to learn how to resize feature maps intelligently, rather than just stretching them.\n",
    "* **Metrics:** We learned that pixel accuracy is insufficient and implemented **Mean IoU** for a robust evaluation of segmentation quality.\n",
    "\n",
    "This concludes Part 2 of the book (Computer Vision). In the next chapter, we enter the world of **Natural Language Processing (NLP)**, starting with Sentiment Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
