{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: The Amazing World of TensorFlow\n",
    "\n",
    "**Author:** Thushan Ganegedara (Book Author) / Adapted for Repository\n",
    "\n",
    "## 1Ô∏è‚É£ Chapter Overview\n",
    "\n",
    "Welcome to the first chapter of *TensorFlow in Action*. This chapter acts as a foundational pillar for the rest of the book. Unlike subsequent chapters that dive straight into building neural networks, this chapter focuses on the **\"Why\"** and **\"How\"** of the TensorFlow ecosystem.\n",
    "\n",
    "We will explore the architectural differences between processing units (CPUs, GPUs, and TPUs) and verify these differences through practical code experiments. We will also establish clear guidelines on when TensorFlow is the right tool for the job‚Äîand, equally importantly, when it is not.\n",
    "\n",
    "### Key Learning Goals:\n",
    "1.  **Understand the TensorFlow Ecosystem:** It is more than just a library; it is an end-to-end platform.\n",
    "2.  **Hardware Acceleration:** Grasp the theoretical and practical differences between CPU and GPU execution.\n",
    "3.  **Benchmarking:** Implement a rigorous benchmark to compare NumPy (CPU-bound) vs. TensorFlow (GPU-accelerated).\n",
    "4.  **Strategic Selection:** Learn to identify appropriate use-cases for TensorFlow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Theoretical Explanation\n",
    "\n",
    "### 2.1 What is TensorFlow?\n",
    "\n",
    "TensorFlow is an open-source end-to-end machine learning platform developed by Google. While it is most famous for Deep Learning, its capabilities extend far beyond just training neural networks. It supports:\n",
    "\n",
    "* **Data Pipelines (`tf.data`):** Efficiently loading and preprocessing massive datasets that do not fit in memory.\n",
    "* **Model Building (`tf.keras`):** High-level APIs for easy prototyping and low-level control for research.\n",
    "* **Deployment (TFX, TF Serving, TF Lite):** Tools to take models from a laptop to a production server or a mobile device.\n",
    "* **Visualization (TensorBoard):** Tools to debug, profile, and visualize model architecture and training progress.\n",
    "\n",
    "### 2.2 The Hardware: CPU vs. GPU vs. TPU\n",
    "\n",
    "One of the core reasons for TensorFlow's dominance is its ability to leverage hardware acceleration seamlessly. To understand why this matters, we use the analogy provided in the book.\n",
    "\n",
    "#### üèéÔ∏è CPU (Central Processing Unit) - The Race Car\n",
    "* **Analogy:** Imagine a Ferrari. It is incredibly fast and agile.\n",
    "* **Strength:** Low Latency. It can execute complex, sequential instructions (like `if-else` logic or OS tasks) very quickly.\n",
    "* **Weakness:** Low Throughput. It can only carry a few \"passengers\" (data points) at a time.\n",
    "* **Best For:** Serial processing, general-purpose computing, small datasets.\n",
    "\n",
    "#### üöå GPU (Graphics Processing Unit) - The City Bus\n",
    "* **Analogy:** Imagine a city bus. It is slower than a Ferrari per trip, but it can carry 50 people at once.\n",
    "* **Strength:** High Throughput. It has thousands of small cores designed to perform simple mathematical operations (like matrix addition/multiplication) in parallel.\n",
    "* **Weakness:** High Latency per single task compared to a CPU.\n",
    "* **Best For:** Massive parallel operations (Matrix Multiplications), Deep Learning, Graphics rendering.\n",
    "\n",
    "#### üöÄ TPU (Tensor Processing Unit) - The Specialized Shuttle\n",
    "* **Analogy:** A specialized shuttle designed for a specific route.\n",
    "* **Characteristics:** An ASIC (Application-Specific Integrated Circuit) custom-built by Google for Machine Learning.\n",
    "* **Precision:** Uses `bfloat16` (mixed precision) to speed up calculations drastically at the cost of slight precision loss, which is usually acceptable in ML.\n",
    "\n",
    "### 2.3 When to use TensorFlow?\n",
    "\n",
    "| **Use TensorFlow When...** | **Do NOT Use TensorFlow When...** |\n",
    "| :--- | :--- |\n",
    "| **Deep Learning:** You are building CNNs, RNNs, Transformers. | **Traditional ML:** You need Random Forests, SVMs, or k-Means. Use **Scikit-Learn** instead. |\n",
    "| **Big Data:** Your dataset is too large to fit in RAM. TF pipelines stream data efficiently. | **Small Data:** Your data fits in memory (e.g., 10k rows). Use **Pandas** and **NumPy** for speed and simplicity. |\n",
    "| **Production:** You need to deploy models to mobile (Android/iOS) or web servers. | **Complex NLP Rules:** You need heavy linguistic preprocessing (stemming, lemmatization). Use **spaCy** or **NLTK**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Code Reproduction & Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and verifying the hardware available to us. TensorFlow 2.x is designed to automatically detect GPUs, but it is good practice to verify this programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Scientific Computing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning Library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration for cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF logging info\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hardware Verification\n",
    "We will check if TensorFlow can see a GPU. If you are running this on Google Colab, make sure to change the Runtime type to GPU.\n",
    "\n",
    "The function `tf.config.list_physical_devices()` is the standard way to check available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hardware_availability():\n",
    "    \"\"\"\n",
    "    Checks for available computing devices (CPU and GPU).\n",
    "    Prints the list of devices visible to TensorFlow.\n",
    "    \"\"\"\n",
    "    # List all physical devices\n",
    "    physical_devices = tf.config.list_physical_devices()\n",
    "    \n",
    "    print(\"\\n--- Hardware Verification ---\")\n",
    "    print(f\"Total Physical Devices: {len(physical_devices)}\")\n",
    "    \n",
    "    # Check specifically for GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    cpus = tf.config.list_physical_devices('CPU')\n",
    "    \n",
    "    if cpus:\n",
    "        print(f\"‚úÖ CPU Available: {len(cpus)} (Standard processing)\")\n",
    "    \n",
    "    if gpus:\n",
    "        print(f\"‚úÖ GPU Available: {len(gpus)} (Accelerated processing)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   GPU #{i}: {gpu.name}\")\n",
    "            \n",
    "        # Optional: Print GPU details if possible\n",
    "        try:\n",
    "            gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            print(f\"   Details: {gpu_details.get('device_name', 'Unknown')}\")\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No GPU detected. Running in CPU-only mode.\")\n",
    "        print(\"   (Performance for large matrix ops will be slower)\")\n",
    "\n",
    "# Execute the check\n",
    "check_hardware_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Experiment: The Great Race (NumPy vs. TensorFlow)\n",
    "\n",
    "In this section, we reproduce the core experiment from Chapter 1. We will compare the performance of **Matrix Multiplication** ($C = A \\times B$) between:\n",
    "1.  **NumPy:** Runs on the **CPU**. Highly optimized for single-threaded or multi-core CPU execution.\n",
    "2.  **TensorFlow:** Runs on the **GPU** (if available) or **CPU**. Designed for massive parallelism.\n",
    "\n",
    "### Experimental Setup\n",
    "We will perform matrix multiplication for square matrices of increasing sizes $N \\times N$.\n",
    "The sizes will range from small ($100 \\times 100$) to very large ($5000 \\times 5000$ or more).\n",
    "\n",
    "**Hypothesis:**\n",
    "* For **small N**, NumPy might be faster due to TensorFlow's internal overhead (kernel launching, graph construction).\n",
    "* For **large N**, TensorFlow (especially on GPU) should be exponentially faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_multiplication(n, steps=5):\n",
    "    \"\"\"\n",
    "    Benchmarks matrix multiplication for matrices of size (n, n).\n",
    "    \n",
    "    Args:\n",
    "        n (int): The dimension of the square matrix.\n",
    "        steps (int): Number of times to repeat the operation for averaging.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_numpy_time, average_tf_time)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Prepare Data ---\n",
    "    # We create random matrices. \n",
    "    # Note: 'astype(np.float32)' is crucial. GPUs love float32. \n",
    "    # NumPy uses float64 by default, which is slower and heavier.\n",
    "    np_a = np.random.rand(n, n).astype(np.float32)\n",
    "    np_b = np.random.rand(n, n).astype(np.float32)\n",
    "    \n",
    "    # Convert to TensorFlow Tensors\n",
    "    # This allocates memory on the GPU (if available)\n",
    "    tf_a = tf.constant(np_a)\n",
    "    tf_b = tf.constant(np_b)\n",
    "    \n",
    "    # --- 2. Benchmark NumPy ---\n",
    "    np_times = []\n",
    "    for _ in range(steps):\n",
    "        start = time.time()\n",
    "        # np.dot is the standard matrix multiplication in NumPy\n",
    "        _ = np.dot(np_a, np_b)\n",
    "        end = time.time()\n",
    "        np_times.append(end - start)\n",
    "    \n",
    "    avg_np_time = np.mean(np_times)\n",
    "    \n",
    "    # --- 3. Benchmark TensorFlow ---\n",
    "    tf_times = []\n",
    "    \n",
    "    # Warm-up step! \n",
    "    # TF needs to initialize cuBLAS libraries and allocate buffers.\n",
    "    # We do not count this first run in our timing.\n",
    "    _ = tf.matmul(tf_a, tf_b)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        start = time.time()\n",
    "        # tf.matmul is the equivalent operation in TensorFlow\n",
    "        result = tf.matmul(tf_a, tf_b)\n",
    "        \n",
    "        # IMPORTANT: TensorFlow execution can be asynchronous (especially on GPU).\n",
    "        # We must verify the result is calculated to get accurate timing.\n",
    "        # .numpy() forces the synchronization (copying data back to CPU).\n",
    "        # However, for pure computation benchmarking, just ensuring the op is finished is enough.\n",
    "        # Here we accept the slight overhead of .numpy() to simulate real usage.\n",
    "        _ = result.numpy() \n",
    "        \n",
    "        end = time.time()\n",
    "        tf_times.append(end - start)\n",
    "        \n",
    "    avg_tf_time = np.mean(tf_times)\n",
    "    \n",
    "    return avg_np_time, avg_tf_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running the Benchmark\n",
    "We will now run the benchmark across a range of matrix sizes. \n",
    "\n",
    "**Note:** If you are running on a CPU-only environment, the `tf_times` might not be significantly better than `np_times` (and might even be slower due to overhead). The real power is visible with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes to test\n",
    "# We include small sizes to show overhead and large sizes to show throughput\n",
    "matrix_sizes = [100, 300, 500, 1000, 2000, 3000, 5000]\n",
    "\n",
    "# Storage for results\n",
    "results = {\n",
    "    \"Size\": [],\n",
    "    \"NumPy Time (s)\": [],\n",
    "    \"TensorFlow Time (s)\": []\n",
    "}\n",
    "\n",
    "print(f\"Starting Benchmark...\")\n",
    "print(f\"{'Size':<10} | {'NumPy (s)':<15} | {'TensorFlow (s)':<15} | {'Speedup':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for size in matrix_sizes:\n",
    "    try:\n",
    "        np_t, tf_t = benchmark_multiplication(size, steps=3)\n",
    "        \n",
    "        results[\"Size\"].append(size)\n",
    "        results[\"NumPy Time (s)\"].append(np_t)\n",
    "        results[\"TensorFlow Time (s)\"].append(tf_t)\n",
    "        \n",
    "        speedup = np_t / tf_t\n",
    "        print(f\"{size:<10} | {np_t:.5f}         | {tf_t:.5f}         | {speedup:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not run for size {size}: {e}\")\n",
    "        # Likely OOM (Out Of Memory) on GPU for very large matrices\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualization and Analysis\n",
    "\n",
    "Numbers are good, but charts tell the story better. We will plot the execution time vs. matrix size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easier plotting\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting NumPy Lines\n",
    "sns.lineplot(data=df_results, x=\"Size\", y=\"NumPy Time (s)\", label=\"NumPy (CPU)\", marker='o', linewidth=2)\n",
    "\n",
    "# Plotting TensorFlow Lines\n",
    "sns.lineplot(data=df_results, x=\"Size\", y=\"TensorFlow Time (s)\", label=\"TensorFlow (GPU/CPU)\", marker='s', linewidth=2)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title(\"Matrix Multiplication Performance: NumPy vs TensorFlow\", fontsize=16)\n",
    "plt.xlabel(\"Matrix Dimension (N x N)\", fontsize=12)\n",
    "plt.ylabel(\"Execution Time (seconds)\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.yscale('log') # Log scale helps visualize the order of magnitude difference\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Analyzing the Results\n",
    "\n",
    "**1. The Overhead Zone (Small Matrices):**\n",
    "For sizes like $100 \\times 100$, you might notice that TensorFlow is actually *slower* or equal to NumPy. \n",
    "* **Reason:** Moving data from RAM (CPU) to VRAM (GPU) takes time. Additionally, TensorFlow has to launch a \"kernel\" (a function on the GPU). For small tasks, the administrative time (overhead) exceeds the computation time.\n",
    "\n",
    "**2. The Crossover Point:**\n",
    "Somewhere between 500 and 1000, TensorFlow typically overtakes NumPy.\n",
    "\n",
    "**3. The Acceleration Zone (Large Matrices):**\n",
    "At $5000 \\times 5000$, the difference should be massive (often 10x-50x faster on GPU).\n",
    "* **Reason:** NumPy computation scales roughly as $O(N^3)$. While TF also scales similarly mathematically, the massive parallelism of the GPU (thousands of cores) allows it to chew through the cubic complexity much faster than the CPU's limited cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ When NOT to use TensorFlow: The Overhead Example\n",
    "\n",
    "Let's explicitly demonstrate a case where TensorFlow is the wrong choice. This usually happens with:\n",
    "1.  Scalar operations.\n",
    "2.  Tiny loops.\n",
    "3.  Heavy data manipulation that requires frequent CPU-GPU communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overhead_test():\n",
    "    print(\"\\n--- Overhead Test: Scalar Addition ---\")\n",
    "    \n",
    "    x_np = 10.0\n",
    "    y_np = 20.0\n",
    "    \n",
    "    x_tf = tf.constant(10.0)\n",
    "    y_tf = tf.constant(20.0)\n",
    "    \n",
    "    # Measure NumPy scalar add\n",
    "    start = time.time()\n",
    "    for _ in range(10000):\n",
    "        z = x_np + y_np\n",
    "    print(f\"NumPy (10k ops): {time.time() - start:.4f} seconds\")\n",
    "    \n",
    "    # Measure TF scalar add\n",
    "    start = time.time()\n",
    "    for _ in range(10000):\n",
    "        z = x_tf + y_tf\n",
    "    print(f\"TensorFlow (10k ops): {time.time() - start:.4f} seconds\")\n",
    "\n",
    "overhead_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Overhead\n",
    "You will likely see that **NumPy is significantly faster** for the loop above. \n",
    "\n",
    "**Why?** TensorFlow is optimized for *Tensors* (matrices/arrays), not scalars. Invoking the TensorFlow engine 10,000 times for a simple `10 + 20` operation incurs huge overhead. NumPy does this almost instantly in C memory.\n",
    "\n",
    "**Takeaway:** Don't use TensorFlow for simple loop logic or scalar math. Use it for the heavy lifting (matrix math)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Chapter Summary\n",
    "\n",
    "In this chapter, we laid the groundwork for our Deep Learning journey:\n",
    "\n",
    "1.  **TensorFlow is Broad:** It is not just for training; it handles the full ML lifecycle (Data -> Model -> Production).\n",
    "2.  **Hardware Matters:** \n",
    "    * **CPU:** Good for sequential, complex logic (The Race Car).\n",
    "    * **GPU:** Good for parallel, simple math (The City Bus).\n",
    "    * **TPU:** Good for specialized ML matrix math (The Shuttle).\n",
    "3.  **Performance Check:** We proved that TensorFlow scales linearly or sub-linearly with data size on GPUs, whereas NumPy hits performance walls quickly.\n",
    "4.  **Right Tool for the Job:** We learned that for small data or simple scalar operations, NumPy is superior. For deep learning and large matrices, TensorFlow is essential.\n",
    "\n",
    "In **Chapter 2**, we will dive deeper into the specific building blocks of TensorFlow: `tf.Variable`, `tf.Tensor`, and the computational graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
