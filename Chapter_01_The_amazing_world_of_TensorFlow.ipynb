{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: The Amazing World of TensorFlow\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "This chapter serves as a high-level introduction to the TensorFlow ecosystem. Unlike subsequent chapters which focus on building specific models, this chapter establishes the *context* for why TensorFlow exists, when to use it, and how it leverages hardware acceleration.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* The difference between CPU, GPU, and TPU in the context of ML.\n",
    "* The trade-offs between TensorFlow and other libraries (like NumPy or Scikit-Learn).\n",
    "* The stages of a Machine Learning pipeline (Data -> Model -> Serving).\n",
    "\n",
    "**Practical Skills:**\n",
    "* Understanding hardware acceleration performance gains.\n",
    "* Identifying the appropriate tool for a given ML problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### What is TensorFlow?\n",
    "TensorFlow is an end-to-end machine learning framework developed by Google. While primarily known for Deep Learning (DL), it is a holistic ecosystem that supports:\n",
    "* **Model Development:** Building neural networks (using Keras).\n",
    "* **Monitoring:** Visualizing training with TensorBoard.\n",
    "* **Deployment:** Serving models via TensorFlow Serving, TFX, or TensorFlow Lite.\n",
    "\n",
    "### Hardware: CPU vs. GPU vs. TPU\n",
    "Deep learning relies heavily on matrix multiplications. The choice of hardware significantly impacts performance.\n",
    "\n",
    "1.  **CPU (Central Processing Unit):** \n",
    "    * *Analogy:* A **Car**. Great for transporting a few people very quickly (low latency). \n",
    "    * *Role:* Handles complex, sequential instructions well. Not ideal for massive parallel matrix operations.\n",
    "\n",
    "2.  **GPU (Graphics Processing Unit):**\n",
    "    * *Analogy:* A **Bus**. Slower than a car per trip, but carries many more people at once (high throughput).\n",
    "    * *Role:* Massive parallelism. Has thousands of cores designed to perform simple tasks (like matrix math) simultaneously.\n",
    "\n",
    "3.  **TPU (Tensor Processing Unit):**\n",
    "    * *Analogy:* A **Specialized Shuttle**. An economical bus designed for a very specific route.\n",
    "    * *Role:* Application-Specific Integrated Circuit (ASIC) built specifically for AI. Uses lower precision (bfloat16) for extreme speed in matrix operations, but lacks general-purpose versatility.\n",
    "\n",
    "### When to use (and NOT use) TensorFlow\n",
    "\n",
    "| **Use TensorFlow When...** | **Do NOT Use TensorFlow When...** |\n",
    "| :--- | :--- |\n",
    "| You are building Deep Learning models (CNNs, RNNs, Transformers). | You need traditional ML algorithms (Random Forests, SVMs, k-Means). Use **Scikit-Learn** instead. |\n",
    "| You need heavy data pipelines processing images or text. | You are manipulating small structured datasets that fit in memory. Use **Pandas** instead. |\n",
    "| You need to deploy models to production or mobile devices. | You are building complex NLP preprocessing pipelines involving linguistics (stemming, lemmatization). Use **spaCy** instead. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Code Reproduction\n",
    "\n",
    "Although Chapter 1 is primarily theoretical, it references a performance comparison between **NumPy** and **TensorFlow** for matrix multiplication (Section 1.6). \n",
    "\n",
    "Below, we reproduce this experiment to demonstrate the scalability of TensorFlow on large datasets compared to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TensorFlow is using the CPU/GPU as expected for fair comparison\n",
    "# Note: If you have a GPU configured, TF will automatically use it.\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "def compare_matrix_multiplication(sizes):\n",
    "    numpy_times = []\n",
    "    tf_times = []\n",
    "\n",
    "    for n in sizes:\n",
    "        # --- NumPy ---\n",
    "        # Create random matrices\n",
    "        np_a = np.random.rand(n, n).astype(np.float32)\n",
    "        np_b = np.random.rand(n, n).astype(np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        np.dot(np_a, np_b)\n",
    "        numpy_times.append(time.time() - start_time)\n",
    "\n",
    "        # --- TensorFlow ---\n",
    "        # Create random tensors\n",
    "        tf_a = tf.random.uniform((n, n))\n",
    "        tf_b = tf.random.uniform((n, n))\n",
    "        \n",
    "        # Warmup (TensorFlow has a small overhead for the first operation)\n",
    "        _ = tf.matmul(tf_a, tf_b)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Run the operation\n",
    "        result = tf.matmul(tf_a, tf_b)\n",
    "        # Force execution (if eager execution wasn't instant for some reason)\n",
    "        _ = result.numpy() \n",
    "        tf_times.append(time.time() - start_time)\n",
    "        \n",
    "        print(f\"Size {n}x{n} - NumPy: {numpy_times[-1]:.4f}s, TF: {tf_times[-1]:.4f}s\")\n",
    "\n",
    "    return numpy_times, tf_times\n",
    "\n",
    "# Define matrix sizes to test (n x n)\n",
    "# We start small and go large to see the divergence\n",
    "sizes = [100, 500, 1000, 2000, 5000]\n",
    "\n",
    "# Run comparison\n",
    "np_times, tf_times = compare_matrix_multiplication(sizes)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, np_times, label='NumPy', marker='o')\n",
    "plt.plot(sizes, tf_times, label='TensorFlow', marker='x')\n",
    "plt.title('Matrix Multiplication Speed: NumPy vs TensorFlow')\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "**What this code does:**\n",
    "1.  We define a set of matrix sizes ($N \\times N$) ranging from 100 to 5000.\n",
    "2.  For each size, we generate random matrices using both NumPy and TensorFlow.\n",
    "3.  We perform matrix multiplication (`np.dot` vs `tf.matmul`) and measure the execution time.\n",
    "4.  We plot the results to visualize the performance difference.\n",
    "\n",
    "**Why it is important:**\n",
    "This experiment validates the claim that TensorFlow scales better with large data. While NumPy is highly optimized for CPUs, TensorFlow is designed to leverage hardware acceleration (AVX instructions on CPU, or CUDA cores on GPU). \n",
    "\n",
    "**Note:** On smaller matrix sizes (e.g., 100x100), NumPy might be faster due to TensorFlow's internal overhead. However, as $N$ increases (e.g., 5000+), TensorFlow's optimized kernels (especially if running on a GPU) will significantly outperform NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Step-by-Step Explanation\n",
    "\n",
    "### 1. Initialization\n",
    "* **Input:** We define a list `sizes = [100, 500, ..., 5000]`.\n",
    "* **Process:** The script loops through each size to run independent tests.\n",
    "\n",
    "### 2. NumPy Calculation\n",
    "* **Process:** `np.random.rand` creates arrays in system memory. `np.dot` executes the math on the CPU.\n",
    "* **Observation:** NumPy shows exponential time growth as matrix size increases ($O(N^3)$ complexity for naive matrix multiplication, though optimized BLAS libraries help).\n",
    "\n",
    "### 3. TensorFlow Calculation\n",
    "* **Process:** `tf.random.uniform` creates Tensors. If a GPU is available, these tensors are allocated on VRAM. `tf.matmul` executes the operation.\n",
    "* **Warmup:** We run a dummy operation first to handle any initialization overhead TensorFlow might have (allocating memory, initializing kernels).\n",
    "* **Output:** The time taken is recorded. On a GPU, this line remains relatively flat or grows linearly compared to NumPy's exponential growth.\n",
    "\n",
    "### 4. Visualization\n",
    "* The resulting plot demonstrates the **Performance Crossover Point** where TensorFlow becomes more efficient than NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Chapter Summary\n",
    "\n",
    "* **TensorFlow Ecosystem:** It is not just a library for training models, but a full suite for the entire ML lifecycle (preprocessing, training, deployment).\n",
    "* **Hardware Matters:** \n",
    "    * **CPU:** Low latency, general purpose.\n",
    "    * **GPU:** High throughput, parallel tasks (ideal for Deep Learning).\n",
    "    * **TPU:** Specialized, high speed, low precision.\n",
    "* **Selection Criteria:** \n",
    "    * Use **TensorFlow** for Deep Learning, massive datasets, and production pipelines.\n",
    "    * Use **Scikit-Learn/Pandas** for small, structured datasets and traditional ML.\n",
    "* **Performance:** TensorFlow outperforms standard numerical libraries like NumPy as data dimensionality grows, particularly when hardware acceleration is available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
