{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: State-of-the-Art in Deep Learning: Transformers\n",
    "\n",
    "## 1️⃣ Chapter Overview\n",
    "\n",
    "In the previous chapters, we explored Recurrent Neural Networks (RNNs) and LSTMs for sequence processing. While powerful, these models suffer from limitations like sequential processing (slow training) and difficulty in retaining long-range dependencies. \n",
    "\n",
    "This chapter introduces the **Transformer**, a groundbreaking architecture introduced in the paper *\"Attention Is All You Need\"* (Vaswani et al., 2017). Transformers discarded recurrence entirely in favor of a mechanism called **Self-Attention**, allowing the model to look at the entire input sequence simultaneously.\n",
    "\n",
    "**Key Machine Learning Concepts:**\n",
    "* **Encoder-Decoder Architecture:** The standard framework for sequence-to-sequence tasks.\n",
    "* **Self-Attention:** The ability of a model to weigh the importance of different words in a sentence relative to a specific word.\n",
    "* **Multi-Head Attention:** Running multiple self-attention mechanisms in parallel to capture different types of relationships.\n",
    "* **Positional Encoding:** Injecting information about the order of words, since Transformers process them in parallel.\n",
    "\n",
    "**Practical Skills:**\n",
    "* Implementing custom Keras layers using the `Subclassing API`.\n",
    "* Building complex mathematical operations (Scaled Dot-Product Attention) from scratch in TensorFlow.\n",
    "* Assembling a full Transformer architecture from custom components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Theoretical Explanation\n",
    "\n",
    "### 2.1 The Intuition: Why \"Attention\"?\n",
    "Imagine translating the sentence: *\"The animal did not cross the street because it was too tired.\"*\n",
    "\n",
    "When a human reads the word **\"it\"**, they immediately know it refers to **\"the animal\"**, not **\"the street\"**. \n",
    "\n",
    "* **RNNs** struggle with this if the distance between \"animal\" and \"it\" is large.\n",
    "* **Self-Attention** calculates a score relating \"it\" to every other word in the sentence. The score for \"animal\" would be high, effectively telling the model: *\"When processing 'it', pay close attention to 'animal'.\"*\n",
    "\n",
    "### 2.2 The Core Mechanism: Q, K, V\n",
    "Self-attention is mathematically described using three vectors for every input token: **Query (Q)**, **Key (K)**, and **Value (V)**.\n",
    "\n",
    "1.  **Query (Q):** What the token is looking for.\n",
    "2.  **Key (K):** What the token identifies as.\n",
    "3.  **Value (V):** The actual information content of the token.\n",
    "\n",
    "**Analogy:** \n",
    "Think of searching a library catalog.\n",
    "* **Query:** The search term you type in.\n",
    "* **Key:** The book titles/categories in the database.\n",
    "* **Value:** The content of the book you eventually get.\n",
    "\n",
    "We calculate the match between $Q$ and $K$ (using dot product) to get a score (attention weight). We then use that score to weight the $V$.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "### 2.3 Multi-Head Attention\n",
    "Instead of having one set of Q, K, V matrices, we have multiple sets (heads). This allows the model to focus on different aspects simultaneously. For example, one head might track grammatical relationships (subject-verb), while another tracks semantic references (pronoun resolution).\n",
    "\n",
    "### 2.4 Positional Encodings\n",
    "Since Transformers process the sentence \"Dog bites Man\" and \"Man bites Dog\" identically (as a bag of words) without recurrence, we must explicitly add information about the position of each word. We add a **Positional Encoding** vector to the input embeddings to give the model a sense of order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Code Reproduction: Building the Transformer Components\n",
    "\n",
    "We will build the Transformer from the bottom up using the **Keras Subclassing API**. This gives us fine-grained control over the forward pass calculations.\n",
    "\n",
    "### 3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Ensure reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Self-Attention Layer\n",
    "\n",
    "This is the heart of the Transformer. We calculate the $Q$, $K$, and $V$ vectors using dense layers (linear projections) and then apply the attention formula.\n",
    "\n",
    "**Note on Masking:** In the decoder, we must prevent positions from attending to subsequent positions (peeking into the future). We implement this via a `mask` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Check if d_model is divisible by n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        # The weight matrices for Q, K, V are implemented as Dense layers\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        # Linear layer for the output\n",
    "        self.dense = layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (n_heads, d_head).\n",
    "        Transpose the result to shape (batch_size, n_heads, seq_len, d_head)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.d_head))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # 1. Generate Q, K, V\n",
    "        qw = self.wq(q)\n",
    "        kw = self.wk(k)\n",
    "        vw = self.wv(v)\n",
    "\n",
    "        # 2. Split heads\n",
    "        # Shape: (batch, n_heads, seq_len, d_head)\n",
    "        q_heads = self.split_heads(qw, batch_size)\n",
    "        k_heads = self.split_heads(kw, batch_size)\n",
    "        v_heads = self.split_heads(vw, batch_size)\n",
    "\n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        # Matmul of Q and K\n",
    "        # Result Shape: (batch, n_heads, seq_len_q, seq_len_k)\n",
    "        matmul_qk = tf.matmul(q_heads, k_heads, transpose_b=True)\n",
    "\n",
    "        # Scale by sqrt(d_k)\n",
    "        dk = tf.cast(self.d_head, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply Mask (if provided)\n",
    "        # We add a large negative number to positions we want to mask\n",
    "        # so that after softmax they become 0.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # Softmax to get probabilities\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # Weighted sum of Values\n",
    "        output = tf.matmul(attention_weights, v_heads)\n",
    "\n",
    "        # 4. Concatenate heads and run through final dense layer\n",
    "        # Transpose back: (batch, seq_len, n_heads, d_head)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        # Reshape: (batch, seq_len, d_model)\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return self.dense(concat_attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The Feed-Forward Layer\n",
    "\n",
    "After self-attention, each token is processed independently by a fully connected network. This typically consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "Equation: $FFN(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        \"\"\"\n",
    "        d_model: Dimensionality of the model (embedding size)\n",
    "        dff: Dimensionality of the inner feed-forward layer (usually 4x d_model)\n",
    "        \"\"\"\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.dense1 = layers.Dense(dff, activation='relu')\n",
    "        self.dense2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Encoder Layer\n",
    "\n",
    "Now we compose the `SelfAttentionLayer` and `FCLayer` into a single Encoder Layer. \n",
    "Crucially, the Transformer uses **Residual Connections** (Add) and **Layer Normalization** (Norm) after each sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = SelfAttentionLayer(d_model, n_heads)\n",
    "        self.ffn = FCLayer(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # Sub-layer 1: Multi-Head Attention\n",
    "        attn_output = self.mha(x, x, x, mask)  # Q, K, V are all x (Self-Attention)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Add & Norm\n",
    "\n",
    "        # Sub-layer 2: Feed Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Add & Norm\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 The Decoder Layer\n",
    "\n",
    "The Decoder layer is slightly more complex. It has three sub-layers:\n",
    "1.  **Masked Self-Attention:** Can only attend to previous tokens.\n",
    "2.  **Encoder-Decoder Attention:** Queries come from the decoder, Keys and Values come from the encoder output.\n",
    "3.  **Feed Forward Network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = SelfAttentionLayer(d_model, n_heads) # Masked Self-Attention\n",
    "        self.mha2 = SelfAttentionLayer(d_model, n_heads) # Encoder-Decoder Attention\n",
    "\n",
    "        self.ffn = FCLayer(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # 1. Masked Self-Attention (on the target sequence)\n",
    "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # 2. Encoder-Decoder Attention\n",
    "        # Q = Decoder output so far (out1)\n",
    "        # K, V = Encoder output\n",
    "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        # 3. Feed Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Positional Encoding\n",
    "\n",
    "The book uses the standard sinusoidal positional encoding described in Vaswani et al. \n",
    "$$ PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(position, d_model):\n",
    "    # Provide a matrix of position indices\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 The Final Transformer Model (MinTransformer)\n",
    "\n",
    "We assemble everything into a Keras Model. This model takes both Encoder and Decoder inputs and outputs the probability distribution of the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinTransformer(models.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, dff, input_vocab_size, \n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(MinTransformer, self).__init__()\n",
    "\n",
    "        # --- ENCODER ---\n",
    "        self.encoder_embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.encoder_pos_encoding = get_positional_encoding(pe_input, d_model)\n",
    "        self.encoder_layers = [EncoderLayer(d_model, n_heads, dff, rate) \n",
    "                               for _ in range(n_layers)]\n",
    "        self.encoder_dropout = layers.Dropout(rate)\n",
    "\n",
    "        # --- DECODER ---\n",
    "        self.decoder_embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.decoder_pos_encoding = get_positional_encoding(pe_target, d_model)\n",
    "        self.decoder_layers = [DecoderLayer(d_model, n_heads, dff, rate) \n",
    "                               for _ in range(n_layers)]\n",
    "        self.decoder_dropout = layers.Dropout(rate)\n",
    "\n",
    "        # --- FINAL OUTPUT ---\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Inputs is a tuple: (encoder_input, decoder_input)\n",
    "        # We'll omit masks generation here for brevity, assuming they are passed or handled externally \n",
    "        # in a full training loop. For the chapter's scope, we define the architecture.\n",
    "        inp, tar = inputs\n",
    "        \n",
    "        # Dummy masks for demonstration (In real training, these depend on padding)\n",
    "        enc_padding_mask = None \n",
    "        look_ahead_mask = None\n",
    "        dec_padding_mask = None\n",
    "\n",
    "        # --- ENCODER PASS ---\n",
    "        enc_out = self.encoder_embedding(inp)\n",
    "        enc_out *= tf.math.sqrt(tf.cast(512, tf.float32)) # Scaling by sqrt(d_model)\n",
    "        enc_out += self.encoder_pos_encoding[:, :tf.shape(inp)[1], :]\n",
    "        enc_out = self.encoder_dropout(enc_out, training=training)\n",
    "\n",
    "        for i in range(len(self.encoder_layers)):\n",
    "            enc_out = self.encoder_layers[i](enc_out, training, enc_padding_mask)\n",
    "\n",
    "        # --- DECODER PASS ---\n",
    "        dec_out = self.decoder_embedding(tar)\n",
    "        dec_out *= tf.math.sqrt(tf.cast(512, tf.float32))\n",
    "        dec_out += self.decoder_pos_encoding[:, :tf.shape(tar)[1], :]\n",
    "        dec_out = self.decoder_dropout(dec_out, training=training)\n",
    "\n",
    "        for i in range(len(self.decoder_layers)):\n",
    "            dec_out = self.decoder_layers[i](dec_out, enc_out, training, \n",
    "                                             look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # Final prediction\n",
    "        final_output = self.final_layer(dec_out)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Testing the Model\n",
    "\n",
    "Let's instantiate the model with some sample hyperparameters to verify that the forward pass works and produces the correct output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "input_vocab_size = 8500\n",
    "target_vocab_size = 8000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = MinTransformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    input_vocab_size, \n",
    "    target_vocab_size, \n",
    "    pe_input=1000, \n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Create Dummy Input\n",
    "# Batch size: 64, Sequence Length: 38\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "# Run Forward Pass\n",
    "fn_out = transformer((temp_input, temp_target), training=False)\n",
    "\n",
    "print(f\"Encoder Input Shape: {temp_input.shape}\")\n",
    "print(f\"Decoder Input Shape: {temp_target.shape}\")\n",
    "print(f\"Transformer Output Shape: {fn_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "The output shape should be `(64, 36, 8000)`.\n",
    "* `64`: Batch size.\n",
    "* `36`: Target sequence length (the model outputs a prediction for every step in the decoder input).\n",
    "* `8000`: Target vocab size (logits for the next word probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Chapter Summary\n",
    "\n",
    "In this chapter, we dissected the Transformer model, the current state-of-the-art in NLP.\n",
    "\n",
    "* **No More Recurrence:** We replaced LSTMs with **Self-Attention**, allowing parallel processing of sequences.\n",
    "* **Attention Mechanism:** We implemented `Scaled Dot-Product Attention`, which allows the model to dynamically focus on different parts of the input.\n",
    "* **Multi-Head Attention:** We split the attention mechanism into multiple heads to capture different linguistic features simultaneously.\n",
    "* **Positional Encoding:** We injected sine/cosine waves into the embeddings so the model knows the order of words.\n",
    "* **Architecture:** We built the Encoder (extracts features) and Decoder (generates sequence) and connected them into a full `MinTransformer` model in Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
